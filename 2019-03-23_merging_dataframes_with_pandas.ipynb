{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas Configuration Options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 300)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Files Location**\n",
    "\n",
    "* Most data files for the exercises can be found on the [course site](https://www.datacamp.com/courses/merging-dataframes-with-pandas)\n",
    "    * [Baby Names](https://assets.datacamp.com/production/repositories/516/datasets/43c9b6bf4c283ab024b2d7d61fbf15a0baa1e44d/Baby%20names.zip)\n",
    "    * [Summer Olympic Medals](https://assets.datacamp.com/production/repositories/516/datasets/2d14df8d3c6a1773358fa000f203282c2e1107d6/Summer%20Olympic%20medals.zip)\n",
    "    * [Automobile Fuel Efficiency](https://assets.datacamp.com/production/repositories/516/datasets/2f3d8b2156d5669fb7e12137f1c2e979c3c9ce0b/automobiles.csv)\n",
    "    * [Exchange Rates](https://assets.datacamp.com/production/repositories/516/datasets/e91482db6a7bae394653278e4e908e63ed9ac833/exchange.csv)\n",
    "    * [GDP](https://assets.datacamp.com/production/repositories/516/datasets/a0858a700501f88721ca9e4bdfca99b9e10b937f/GDP.zip)\n",
    "    * [Oil Prices](https://assets.datacamp.com/production/repositories/516/datasets/707566cf46c4dd6290b9029f5e07a92baf3fe3f7/oil_price.csv)\n",
    "    * [Pittsburgh Weather](https://assets.datacamp.com/production/repositories/516/datasets/58c1ead59818b2451324e9e84239db7bda6b11d3/pittsburgh2013.csv)\n",
    "    * [Sales](https://assets.datacamp.com/production/repositories/516/datasets/2b89c1b00016e1ebcfd7f08a127d2c79589ce5c0/Sales.zip)\n",
    "    * [S&P 500](https://assets.datacamp.com/production/repositories/516/datasets/7a9b570a02ef589891d9576a86876a616ca5f3c8/sp500.csv)\n",
    "* Other data files may be found in my [DataCamp repository](https://github.com/trenton3983/DataCamp/tree/master/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data File Objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path.cwd() / 'data' / 'merging-dataframes-with-pandas'\n",
    "auto_fuel_file = data / 'auto_fuel_efficiency.csv'\n",
    "baby_1881_file = data / 'baby_names1881.csv'\n",
    "baby_1981_file = data / 'baby_names1981.csv'\n",
    "exch_rates_file = data / 'exchange_rates.csv'\n",
    "gdp_china_file = data / 'gdp_china.csv'\n",
    "gdp_usa_file = data / 'gdp_usa.csv'\n",
    "oil_price_file = data / 'oil_price.csv'\n",
    "pitts_file = data / 'pittsburgh_weather_2013.csv'\n",
    "sales_feb_hardware_file = data / 'sales-feb-Hardware.csv'\n",
    "sales_feb_service_file = data / 'sales-feb-Service.csv'\n",
    "sales_feb_software_file = data / 'sales-feb-Software.csv'\n",
    "sales_jan_2015_file = data / 'sales-jan-2015.csv'\n",
    "sales_feb_2015_file = data / 'sales-feb-2015.csv'\n",
    "sales_mar_2015_file = data / 'sales-mar-2015.csv'\n",
    "sp500_file = data / 'sp500.csv'\n",
    "so_bronze_file = data / 'summer_olympics_Bronze.csv'\n",
    "so_bronze5_file = data / 'summer_olympics_bronze_top5.csv'\n",
    "so_gold_file = data / 'summer_olympics_Gold.csv'\n",
    "so_gold5_file = data / 'summer_olympics_gold_top5.csv'\n",
    "so_silver_file = data / 'summer_olympics_Silver.csv'\n",
    "so_silver5_file = data / 'summer_olympics_silver_top5.csv'\n",
    "so_all_medalists_file = data / 'summer_olympics_medalists 1896 to 2008 - ALL MEDALISTS.tsv'\n",
    "so_editions_file = data / 'summer_olympics_medalists 1896 to 2008 - EDITIONS.tsv'\n",
    "so_ioc_codes_file = data / 'summer_olympics_medalists 1896 to 2008 - IOC COUNTRY CODES.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Merging DataFrames with pandas\n",
    "\n",
    "***Course Description***\n",
    "\n",
    "As a Data Scientist, you'll often find that the data you need is not in a single file. It may be spread across a number of text files, spreadsheets, or databases. You want to be able to import the data of interest as a collection of DataFrames and figure out how to combine them to answer your central questions. This course is all about the act of combining, or merging, DataFrames, an essential part of any working Data Scientist's toolbox. You'll hone your pandas skills by learning how to organize, reshape, and aggregate multiple data sets to answer your specific questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "In this chapter, you'll learn about different techniques you can use to import multiple files into DataFrames. Having imported your data into individual DataFrames, you'll then learn how to share information between DataFrames using their Indexes. Understanding how Indexes work is essential information that you'll need for merging DataFrames later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Reading multiple data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools for pandas data import\n",
    "\n",
    "* pd.read_csv() for CSV files\n",
    "    * dataframe = pd.read_csv(filepath)\n",
    "    * dozens of optional input parameters\n",
    "* Other data import tools:\n",
    "    * pd.read_excel()\n",
    "    * pd.read_html()\n",
    "    * pd.read_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading separate files\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "dataframe0 = pd.read_csv('sales-jan-2015.csv')\n",
    "dataframe1 = pd.read_csv('sales-feb-2015.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a loop\n",
    "\n",
    "```python\n",
    "filenames = ['sales-jan-2015.csv', 'sales-feb-2015.csv']\n",
    "dataframes = []\n",
    "for f in filenames:\n",
    "    dataframes.append(pd.read_csv(f))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a comprehension\n",
    "\n",
    "```python\n",
    "filenames = ['sales-jan-2015.csv', 'sales-feb-2015.csv']\n",
    "dataframes = [pd.read_csv(f) for f in filenames]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using glob\n",
    "\n",
    "```python\n",
    "from glob import glob\n",
    "filenames = glob('sales*.csv')\n",
    "dataframes = [pd.read_csv(f) for f in filenames]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading DataFrames from multiple files\n",
    "\n",
    "When data is spread among several files, you usually invoke pandas' <code>read_csv()</code> (or a similar data import function) multiple times to load the data into several DataFrames.\n",
    "\n",
    "The data files for this example have been derived from a [list of Olympic medals awarded between 1896 & 2008](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data) compiled by the Guardian.\n",
    "\n",
    "The column labels of each DataFrame are <code>NOC</code>, <code>Country</code>, & <code>Total</code> where <code>NOC</code> is a three-letter code for the name of the country and <code>Total</code> is the number of medals of that type won (bronze, silver, or gold).\n",
    "\n",
    "**Instructions**\n",
    "<ul>\n",
    "<li>Import <span style=\"background-color: #A733FF\">pandas</span> as <span style=\"background-color: #A733FF\">pd</span>.</li>\n",
    "<li>Read the file <span style=\"background-color: #A733FF\">'Bronze.csv'</span> into a DataFrame called <span style=\"background-color: #A733FF\">bronze</span>.</li>\n",
    "<li>Read the file <span style=\"background-color: #A733FF\">'Silver.csv'</span> into a DataFrame called <span style=\"background-color: #A733FF\">silver</span>.</li>\n",
    "<li>Read the file <span style=\"background-color: #A733FF\">'Gold.csv'</span> into a DataFrame called <span style=\"background-color: #A733FF\">gold</span>.</li>\n",
    "<li>Print the first 5 rows of the DataFrame <span style=\"background-color: #A733FF\">gold</span>. This has been done for you, so hit 'Submit Answer' to see the results.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'Bronze.csv' into a DataFrame: bronze\n",
    "bronze = pd.read_csv(so_bronze_file)\n",
    "\n",
    "# Read 'Silver.csv' into a DataFrame: silver\n",
    "silver = pd.read_csv(so_silver_file)\n",
    "\n",
    "# Read 'Gold.csv' into a DataFrame: gold\n",
    "gold = pd.read_csv(so_gold_file)\n",
    "\n",
    "# Print the first five rows of gold\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading DataFrames from multiple files in a loop\n",
    "\n",
    "As you saw in the video, loading data from multiple files into DataFrames is more efficient in a <em>loop</em> or a <em>list comprehension</em>.\n",
    "\n",
    "Notice that this approach is not restricted to working with CSV files. That is, even if your data comes in other formats, as long as pandas has a suitable data import function, you can apply a loop or comprehension to generate a list of DataFrames imported from the source files.\n",
    "\n",
    "Here, you'll continue working with [The Guardian's Olympic medal dataset](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data).\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a list of file names called <mark>filenames</mark> with three strings <mark>'Gold.csv'</mark>, <mark>'Silver.csv'</mark>, & <mark>'Bronze.csv'</mark>. This has been done for you.\n",
    "* Use a <mark>for</mark> loop to create another list called <mark>dataframes</mark> containing the three DataFrames loaded from <mark>filenames</mark>:\n",
    "    * Iterate over <mark>filenames</mark>.\n",
    "    * Read each CSV file in <mark>filenames</mark> into a DataFrame and append it to <mark>dataframes</mark> by using <mark>pd.read_csv()</mark> inside a call to <mark>.append()</mark>.\n",
    "* Print the first 5 rows of the first DataFrame of the list <mark>dataframes</mark>. This has been done for you, so hit 'Submit Answer' to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of file names: filenames\n",
    "filenames = [so_bronze_file, so_silver_file, so_gold_file]\n",
    "\n",
    "# Create the list of three DataFrames: dataframes\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "# Print top 5 rows of 1st DataFrame in dataframes\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining DataFrames from multiple data files\n",
    "\n",
    "In this exercise, you'll *combine* the three DataFrames from earlier exercises - ```gold```, ```silver```, & ```bronze``` - into a single DataFrame called ```medals```. The approach you'll use here is clumsy. Later on in the course, you'll see various powerful methods that are frequently used in practice for *concatenating* or *merging* DataFrames.\n",
    "\n",
    "Remember, the column labels of each DataFrame are ```NOC```, ```Country```, and ```Total```, where ```NOC``` is a three-letter code for the name of the country and ```Total``` is the number of medals of that type won.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Construct a copy of the DataFrame ```gold``` called ```medals``` using the ```.copy()``` method.\n",
    "* Create a list called ```new_labels``` with entries ```'NOC'```, ```'Country'```, & ```'Gold'```. This is the same as the column labels from ```gold``` with the column label ```'Total'``` replaced by ```'Gold'```.\n",
    "* Rename the columns of ```medals``` by assigning ```new_labels``` to ```medals.columns```.\n",
    "* Create new columns ```'Silver'``` and ```'Bronze'``` in medals using ```silver['Total']``` & ```bronze['Total']```.\n",
    "* Print the top 5 rows of the final DataFrame ```medals```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of gold: medals\n",
    "medals = gold.copy()\n",
    "\n",
    "# Create list of new column labels: new_labels\n",
    "new_labels = ['NOC', 'Country', 'Gold']\n",
    "\n",
    "# Rename the columns of medals using new_labels\n",
    "medals.columns = new_labels\n",
    "\n",
    "# Add columns 'Silver' & 'Bronze' to medals\n",
    "medals['Silver'] = silver['Total']\n",
    "medals['Bronze'] = bronze['Total']\n",
    "\n",
    "# Print the head of medals\n",
    "medals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bronze, silver, gold, dataframes, medals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Indexes\" vs. \"Indices\"\n",
    "\n",
    "* indices: many index labels within Index data structures\n",
    "* indexes: many pandas Index data structures\n",
    "\n",
    "![](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/merging_dataframes_in_python/indices_indexes.JPG \"Indices & Indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing weather data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "w_mean = pd.read_csv('quarterly_mean_temp.csv', index_col='Month')\n",
    "w_max = pd.read_csv('quarterly_max_temp.csv', index_col='Month')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the data\n",
    "\n",
    "```python\n",
    "print(w_mean)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Apr     61.956044\n",
    "Jan     32.133333\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "\n",
    "print(w_max)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68\n",
    "Apr     89\n",
    "Jul     91\n",
    "Oct     84\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DataFrame indexes\n",
    "\n",
    "```python\n",
    "print(w_mean.index)\n",
    "Index(['Apr', 'Jan', 'Jul', 'Oct'], dtype='object', name='Month')\n",
    "\n",
    "print(w_max.index)\n",
    "Index(['Jan', 'Apr', 'Jul', 'Oct'], dtype='object', name='Month')\n",
    "\n",
    "print(type(w_mean.index))\n",
    "<class 'pandas.indexes.base.Index'>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .reindex()\n",
    "\n",
    "```python\n",
    "ordered = ['Jan', 'Apr', 'Jul', 'Oct']\n",
    "w_mean2 = w_mean.reindex(ordered)\n",
    "print(w_mean2)\n",
    "\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .sort_index()\n",
    "\n",
    "```python\n",
    "w_mean2.sort_index()\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Apr     61.956044\n",
    "Jan     32.133333\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex from a DataFrame Index\n",
    "\n",
    "```python\n",
    "w_mean.reindex(w_max.index)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing with missing labels\n",
    "\n",
    "```python\n",
    "w_mean3 = w_mean.reindex(['Jan', 'Apr', 'Dec'])\n",
    "print(w_mean3)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Dec     NaN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex from a DataFrame Index\n",
    "\n",
    "```python\n",
    "w_max.reindex(w_mean3.index)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68.0\n",
    "Apr     89.0\n",
    "Dec     NaN\n",
    "\n",
    "w_max.reindex(w_mean3.index).dropna()\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68.0\n",
    "Apr     89.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order matters\n",
    "\n",
    "```python\n",
    "w_max.reindex(w_mean.index)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Apr     89\n",
    "Jan     68\n",
    "Jul     91\n",
    "Oct     84\n",
    "\n",
    "w_mean.reindex(w_max.index)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting DataFrame with the Index & columns\n",
    "\n",
    "It is often useful to rearrange the sequence of the rows of a DataFrame by *sorting*. You don't have to implement these yourself; the principal methods for doing this are ```.sort_index()``` and ```.sort_values()```.\n",
    "\n",
    "In this exercise, you'll use these methods with a DataFrame of temperature values indexed by month names. You'll sort the rows alphabetically using the Index and numerically using a column. Notice, for this data, the original ordering is probably most useful and intuitive: the purpose here is for you to understand what the sorting methods do.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read ```'monthly_max_temp.csv'``` into a DataFrame called ```weather1``` with ```'Month'``` as the index.\n",
    "* Sort the index of ```weather1``` in alphabetical order using the ```.sort_index()``` method and store the result in ```weather2```.\n",
    "* Sort the index of ```weather1``` in *reverse* alphabetical order by specifying the additional keyword argument ```ascending=False``` inside ```.sort_index()```.\n",
    "* Use the ```.sort_values()``` method to sort ```weather1``` in increasing numerical order according to the values of the column ```'Max TemperatureF'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_max_temp = {'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "                    'Max TemperatureF': [68, 60, 68, 84, 88, 89, 91, 86, 90, 84, 72, 68]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
    "# weather1 = pd.read_csv('monthly_max_temp.csv', index_col='Month')\n",
    "weather1 = pd.DataFrame.from_dict(monthly_max_temp)\n",
    "weather1.set_index('Month', inplace=True)\n",
    "\n",
    "# Print the head of weather1\n",
    "print(weather1.head())\n",
    "\n",
    "# Sort the index of weather1 in alphabetical order: weather2\n",
    "weather2 = weather1.sort_index()\n",
    "\n",
    "# Print the head of weather2\n",
    "print(weather2.head())\n",
    "\n",
    "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
    "weather3 = weather1.sort_index(ascending=False)\n",
    "\n",
    "# Print the head of weather3\n",
    "print(weather3.head())\n",
    "\n",
    "# Sort weather1 numerically using the values of 'Max TemperatureF': weather4\n",
    "weather4 = weather1.sort_values(by='Max TemperatureF')\n",
    "\n",
    "# Print the head of weather4\n",
    "print(weather4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing DataFrame from a list\n",
    "\n",
    "Sorting methods are not the only way to change DataFrame Indexes. There is also the ```.reindex()``` method.\n",
    "\n",
    "In this exercise, you'll reindex a DataFrame of quarterly-sampled mean temperature values to contain monthly samples (this is an example of *upsampling* or increasing the rate of samples, which you may recall from the [pandas Foundations](https://www.datacamp.com/courses/pandas-foundations) course).\n",
    "\n",
    "The original data has the first month's abbreviation of the quarter (three-month interval) on the Index, namely ```Apr```, ```Jan```, ```Jul```, and ```Oct```. This data has been loaded into a DataFrame called ```weather1``` and has been printed in its entirety in the IPython Shell. Notice it has only four rows (corresponding to the first month of each quarter) and that the rows are not sorted chronologically.\n",
    "\n",
    "You'll initially use a list of all twelve month abbreviations and subsequently apply the ```.ffill()``` method to *forward-fill* the null entries when upsampling. This list of month abbreviations has been pre-loaded as ```year```.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Reorder the rows of ```weather1``` using the ```.reindex()``` method with the list ```year``` as the argument, which contains the abbreviations for each month.\n",
    "* Reorder the rows of ```weather1``` just as you did above, this time chaining the ```.ffill()``` method to replace the null values with the last preceding non-null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_max_temp = {'Month': ['Jan', 'Apr', 'Jul', 'Oct'],\n",
    "                    'Max TemperatureF': [32.13333, 61.956044, 68.934783, 43.434783]}\n",
    "weather1 = pd.DataFrame.from_dict(monthly_max_temp)\n",
    "weather1.set_index('Month', inplace=True)\n",
    "weather1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Reindex weather1 using the list year: weather2\n",
    "weather2 = weather1.reindex(year)\n",
    "\n",
    "# Print weather2\n",
    "weather2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex weather1 using the list year with forward-fill: weather3\n",
    "weather3 = weather1.reindex(year).ffill()\n",
    "\n",
    "# Print weather3\n",
    "weather3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing DataFrame using another DataFrame Index\n",
    "\n",
    "Another common technique is to reindex a DataFrame using the Index of another DataFrame. The DataFrame ```.reindex()``` method can accept the Index of a DataFrame or Series as input. You can access the Index of a DataFrame with its ```.index``` attribute.\n",
    "\n",
    "The [Baby Names Dataset](https://www.data.gov/developers/baby-names-dataset/) from [data.gov](http://data.gov/) summarizes counts of names (with genders) from births registered in the US since 1881. In this exercise, you will start with two baby-names DataFrames ```names_1981``` and ```names_1881``` loaded for you.\n",
    "\n",
    "The DataFrames ```names_1981``` and ```names_1881``` both have a MultiIndex with levels ```name``` and ```gender``` giving unique labels to counts in each row. If you're interested in seeing how the MultiIndexes were set up, ```names_1981``` and ```names_1881``` were read in using the following commands:\n",
    "\n",
    "```python\n",
    "names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "```\n",
    "\n",
    "As you can see by looking at their shapes, which have been printed in the IPython Shell, the DataFrame corresponding to 1981 births is much larger, reflecting the greater diversity of names in 1981 as compared to 1881.\n",
    "\n",
    "Your job here is to use the DataFrame ```.reindex()``` and ```.dropna()``` methods to make a DataFrame ```common_names``` counting names from 1881 that were still popular in 1981.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a new DataFrame ```common_names``` by reindexing ```names_1981``` using the Index of the DataFrame ```names_1881``` of older names.\n",
    "* Print the shape of the new ```common_names``` DataFrame. This has been done for you. It should be the same as that of ```names_1881```.\n",
    "* Drop the rows of ```common_names``` that have null counts using the ```.dropna()``` method. These rows correspond to names that fell out of fashion between 1881 & 1981.\n",
    "* Print the shape of the reassigned ```common_names``` DataFrame. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_1981 = pd.read_csv(baby_1981_file, header=None, names=['name', 'gender', 'count'], index_col=(0,1))\n",
    "names_1981.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_1881 = pd.read_csv(baby_1881_file, header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "names_1881.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex names_1981 with index of names_1881: common_names\n",
    "common_names = names_1981.reindex(names_1881.index)\n",
    "\n",
    "# Print shape of common_names\n",
    "common_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null counts: common_names\n",
    "common_names = common_names.dropna()\n",
    "\n",
    "# Print shape of new common_names\n",
    "common_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_names.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weather1, weather2, weather3, weather4, common_names, names_1881, names_1981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic with Series & DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading weather data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "weather = pd.read_csv('pittsburgh2013.csv', index_col='Date', parse_dates=True)\n",
    "weather.loc['2013-7-1':'2013-7-7', 'PrecipitationIn']\n",
    "\n",
    "Date\n",
    "2013-07-01 0.18\n",
    "2013-07-02 0.14\n",
    "2013-07-03 0.00\n",
    "2013-07-04 0.25\n",
    "2013-07-05 0.02\n",
    "2013-07-06 0.06\n",
    "2013-07-07 0.10\n",
    "Name: PrecipitationIn, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar multiplication\n",
    "\n",
    "```python\n",
    "weather.loc['2013-07-01':'2013-07-07', 'PrecipitationIn'] * 2.54\n",
    "\n",
    "Date\n",
    "2013-07-01 0.4572\n",
    "2013-07-02 0.3556\n",
    "2013-07-03 0.0000\n",
    "2013-07-04 0.6350\n",
    "2013-07-05 0.0508\n",
    "2013-07-06 0.1524\n",
    "2013-07-07 0.2540\n",
    "Name: PrecipitationIn, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute temperature range\n",
    "\n",
    "```python\n",
    "week1_range = weather.loc['2013-07-01':'2013-07-07', ['Min TemperatureF', 'Max TemperatureF']]\n",
    "print(week1_range)\n",
    "Min TemperatureF Max TemperatureF\n",
    "Date\n",
    "2013-07-01 66 79\n",
    "2013-07-02 66 84\n",
    "2013-07-03 71 86\n",
    "2013-07-04 70 86\n",
    "2013-07-05 69 86\n",
    "2013-07-06 70 89\n",
    "2013-07-07 70 77\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average temperature\n",
    "\n",
    "```python\n",
    "week1_mean = weather.loc['2013-07-01':'2013-07-07', 'Mean TemperatureF']\n",
    "print(week1_mean)\n",
    "Date\n",
    "2013-07-01 72\n",
    "2013-07-02 74\n",
    "2013-07-03 78\n",
    "2013-07-04 77\n",
    "2013-07-05 76\n",
    "2013-07-06 78\n",
    "2013-07-07 72\n",
    "Name: Mean TemperatureF, dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative temperature range\n",
    "\n",
    "```python\n",
    "week1_range / week1_mean\n",
    "RuntimeWarning: Cannot compare type 'Timestamp' with type 'str', sort order is\n",
    "undefined for incomparable objects\n",
    "return this.join(other, how=how, return_indexers=return_indexers)\n",
    "\n",
    "2013-07-01 00:00:00 2013-07-02 00:00:00 2013-07-03 00:00:00 \\\n",
    "Date\n",
    "2013-07-01 NaN NaN NaN\n",
    "2013-07-02 NaN NaN NaN\n",
    "2013-07-03 NaN NaN NaN\n",
    "2013-07-04 NaN NaN NaN\n",
    "2013-07-05 NaN NaN NaN\n",
    "2013-07-06 NaN NaN NaN\n",
    "2013-07-07 NaN NaN NaN\n",
    "2013-07-04 00:00:00 2013-07-05 00:00:00 2013-07-06 00:00:00 \\\n",
    "Date\n",
    "2013-07-01 NaN NaN NaN\n",
    "... ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative temperature range\n",
    "\n",
    "```python\n",
    "week1_range.divide(week1_mean, axis='rows')\n",
    "\n",
    "Min TemperatureF Max TemperatureF\n",
    "Date\n",
    "2013-07-01 0.916667 1.097222\n",
    "2013-07-02 0.891892 1.135135\n",
    "2013-07-03 0.910256 1.102564\n",
    "2013-07-04 0.909091 1.116883\n",
    "2013-07-05 0.907895 1.131579\n",
    "2013-07-06 0.897436 1.141026\n",
    "2013-07-07 0.972222 1.069444\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage changes\n",
    "\n",
    "```python\n",
    "week1_mean.pct_change() * 100\n",
    "\n",
    "Date\n",
    "2013-07-01 NaN\n",
    "2013-07-02 2.777778\n",
    "2013-07-03 5.405405\n",
    "2013-07-04 -1.282051\n",
    "2013-07-05 -1.298701\n",
    "2013-07-06 2.631579\n",
    "2013-07-07 -7.692308\n",
    "Name: Mean TemperatureF, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bronze Olympic medals\n",
    "\n",
    "```python\n",
    "bronze = pd.read_csv('bronze_top5.csv', index_col=0)\n",
    "print(bronze)\n",
    "Total\n",
    "Country\n",
    "United States 1052.0\n",
    "Soviet Union 584.0\n",
    "United Kingdom 505.0\n",
    "France 475.0\n",
    "Germany 454.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silver Olympic medals\n",
    "\n",
    "```python\n",
    "silver = pd.read_csv('silver_top5.csv', index_col=0)\n",
    "print(silver)\n",
    "Total\n",
    "Country\n",
    "United States 1195.0\n",
    "Soviet Union 627.0\n",
    "United Kingdom 591.0\n",
    "France 461.0\n",
    "Italy 394.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold Olympic medals\n",
    "\n",
    "```python\n",
    "gold = pd.read_csv('gold_top5.csv', index_col=0)\n",
    "print(gold)\n",
    "Total\n",
    "Country\n",
    "United States 2088.0\n",
    "Soviet Union 838.0\n",
    "United Kingdom 498.0\n",
    "Italy 460.0\n",
    "Germany 407.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver\n",
    "\n",
    "```python\n",
    "bronze + silver\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver\n",
    "\n",
    "```python\n",
    "bronze + silver\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "In [22]: print(bronze['United States'])\n",
    "1052.0\n",
    "In [23]: print(silver['United States'])\n",
    "1195.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the .add() method\n",
    "\n",
    "```python\n",
    "bronze.add(silver)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a fill_value\n",
    "\n",
    "```python\n",
    "bronze.add(silver, fill_value=0)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany 454.0\n",
    "Italy 394.0\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver, gold\n",
    "\n",
    "```python\n",
    "bronze + silver + gold\n",
    "\n",
    "Country\n",
    "France NaN\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 2049.0\n",
    "United Kingdom 1594.0\n",
    "United States 4335.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chaining .add()\n",
    "\n",
    "```python\n",
    "bronze.add(silver, fill_value=0).add(gold, fill_value=0)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany 861.0\n",
    "Italy 854.0\n",
    "Soviet Union 2049.0\n",
    "United Kingdom 1594.0\n",
    "United States 4335.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding unaligned DataFrames\n",
    "\n",
    "The DataFrames ```january``` and ```february```, which have been printed in the IPython Shell, represent the sales a company made in the corresponding months.\n",
    "\n",
    "The Indexes in both DataFrames are called ```Company```, identifying which company bought that quantity of units. The column ```Units``` is the number of units sold.\n",
    "\n",
    "If you were to add these two ```DataFrames``` by executing the command ```total = january + february```, how many rows would the resulting DataFrame have? Try this in the IPython Shell and find out for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_dict = {'Company': ['Acme Corporation', 'Hooli', 'Initech', 'Mediacore', 'Streeplex'],\n",
    "            'Units': [19, 17, 20, 10, 13]}\n",
    "feb_dict = {'Company': ['Acme Corporation', 'Hooli', 'Mediacore', 'Vandelay Inc'],\n",
    "            'Units': [15, 3, 12, 25]}\n",
    "\n",
    "january = pd.DataFrame.from_dict(jan_dict)\n",
    "january.set_index('Company', inplace=True)\n",
    "print(january)\n",
    "\n",
    "february = pd.DataFrame.from_dict(feb_dict)\n",
    "february.set_index('Company', inplace=True)\n",
    "print('\\n', february, '\\n')\n",
    "\n",
    "print(january + february)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting in Arithmetic formulas\n",
    "\n",
    "In this exercise, you'll work with weather data pulled from [wunderground.com](https://www.wunderground.com/). The DataFrame <code>weather</code> has been pre-loaded along with <code>pandas as pd</code>. It has 365 rows (observed each day of the year 2013 in Pittsburgh, PA) and 22 columns reflecting different weather measurements each day.\n",
    "\n",
    "You'll subset a collection of columns related to temperature measurements in degrees Fahrenheit, convert them to degrees Celsius, and relabel the columns of the new DataFrame to reflect the change of units.\n",
    "\n",
    "Remember, ordinary arithmetic operators (like <code>+</code>, <code>-</code>, <code>*</code>, and <code>/</code>) broadcast scalar values to conforming DataFrames when combining scalars & DataFrames in arithmetic expressions. Broadcasting also works with pandas Series and NumPy arrays.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a new DataFrame ```temps_f``` by extracting the columns ```'Min TemperatureF'```, ```'Mean TemperatureF'```, & ```'Max TemperatureF'``` from ```weather``` as a new DataFrame ```temps_f```. To do this, pass the relevant columns as a list to ```weather[]```.\n",
    "* Create a new DataFrame ```temps_c``` from ```temps_f``` using the formula ```(temps_f - 32) * 5/9```.\n",
    "* Rename the columns of ```temps_c``` to replace ```'F'``` with ```'C'``` using the ```.str.replace('F', 'C')``` method on ```temps_c.columns```.\n",
    "* Print the first 5 rows of DataFrame ```temps_c```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(pitts_file)\n",
    "weather.set_index('Date', inplace=True)\n",
    "weather.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract selected columns from weather as new DataFrame: temps_f\n",
    "temps_f = weather[['Min TemperatureF', 'Mean TemperatureF', 'Max TemperatureF']]\n",
    "\n",
    "# Convert temps_f to celsius: temps_c\n",
    "temps_c = (temps_f - 32) * 5/9\n",
    "\n",
    "# Rename 'F' in column names with 'C': temps_c.columns\n",
    "temps_c.columns = temps_c.columns.str.replace('F', 'C')\n",
    "\n",
    "# Print first 5 rows of temps_c\n",
    "temps_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing percentage growth of GDP\n",
    "\n",
    "Your job in this exercise is to compute the yearly percent-change of US GDP ([Gross Domestic Product](https://en.wikipedia.org/wiki/Gross_domestic_product)) since 2008.\n",
    "\n",
    "The data has been obtained from the [Federal Reserve Bank of St. Louis](https://fred.stlouisfed.org/series/GDP/downloaddata) and is available in the file ```GDP.csv```, which contains quarterly data; you will resample it to annual sampling and then compute the annual growth of GDP. For a refresher on resampling, check out the relevant material from [pandas Foundations](https://campus.datacamp.com/courses/pandas-foundations/time-series-in-pandas?ex=7).\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read the file ```'GDP.csv'``` into a DataFrame called ```gdp```.\n",
    "* Use ```parse_dates=True``` and ```index_col='DATE'```.\n",
    "* Create a DataFrame ```post2008``` by slicing ```gdp``` such that it comprises all rows from 2008 onward.\n",
    "* Print the last 8 rows of the slice ```post2008```. This has been done for you. This data has quarterly frequency so the indices are separated by three-month intervals.\n",
    "* Create the DataFrame ```yearly``` by resampling the slice ```post2008``` by year. Remember, you need to chain ```.resample()``` (using the alias ```'A'``` for annual frequency) with some kind of aggregation; you will use the aggregation method ```.last()``` to select the last element when resampling.\n",
    "* Compute the percentage growth of the resampled DataFrame ```yearly``` with ```.pct_change() * 100```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'GDP.csv' into a DataFrame: gdp\n",
    "gdp = pd.read_csv(gdp_usa_file, parse_dates=True, index_col='DATE')\n",
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice all the gdp data from 2008 onward: post2008\n",
    "post2008 = gdp.loc['2008-01-01':]\n",
    "\n",
    "# Print the last 8 rows of post2008\n",
    "post2008.tail(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample post2008 by year, keeping last(): yearly\n",
    "yearly = post2008.resample('A').last()\n",
    "\n",
    "# Print yearly\n",
    "yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage growth of yearly: yearly['growth']\n",
    "yearly['growth'] = yearly.pct_change() * 100\n",
    "\n",
    "# Print yearly again\n",
    "yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting currency of stocks\n",
    "\n",
    "In this exercise, stock prices in US Dollars for the S&P 500 in 2015 have been obtained from [Yahoo Finance](https://finance.yahoo.com/). The files ```sp500.csv``` for sp500 and ```exchange.csv``` for the exchange rates are both provided to you.\n",
    "\n",
    "Using the daily exchange rate to Pounds Sterling, your task is to convert both the Open and Close column prices.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read the DataFrames ```sp500``` & ```exchange``` from the files ```'sp500.csv'``` & ```'exchange.csv'``` respectively..\n",
    "* Use ```parse_dates=True``` and ```index_col='Date'```.\n",
    "* Extract the columns ```'Open'``` & ```'Close'``` from the DataFrame ```sp500``` as a new DataFrame ```dollars``` and print the first 5 rows.\n",
    "* Construct a new DataFrame ```pounds``` by converting US dollars to British pounds. You'll use the ```.multiply()``` method of ```dollars``` with ```exchange['GBP/USD']``` and ```axis='rows'```\n",
    "* Print the first 5 rows of the new DataFrame ```pounds```. This has been done for you, so hit 'Submit Answer' to see the results!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'sp500.csv' into a DataFrame: sp500\n",
    "sp500 = pd.read_csv(sp500_file, parse_dates=True, index_col='Date')\n",
    "sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'exchange.csv' into a DataFrame: exchange\n",
    "exchange = pd.read_csv(exch_rates_file, parse_dates=True, index_col='Date')\n",
    "exchange.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
    "dollars = sp500[['Open', 'Close']]\n",
    "\n",
    "# Print the head of dollars\n",
    "dollars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dollars to pounds: pounds\n",
    "pounds = dollars.multiply(exchange['GBP/USD'], axis='rows')\n",
    "\n",
    "# Print the head of pounds\n",
    "pounds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del january, february, feb_dict, jan_dict, weather, temps_f, temps_c, gdp, post2008, yearly, sp500, exchange, dollars, pounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Data\n",
    "\n",
    "Having learned how to import multiple DataFrames and share information using Indexes, in this chapter you'll learn how to perform database-style operations to combine DataFrames. In particular, you'll learn about appending and concatenating DataFrames while working with a variety of real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending & concatenating Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append()\n",
    "\n",
    "* .append(): Series & DataFrame method\n",
    "* Invocation:\n",
    "* s1.append(s2)\n",
    "* Stacks rows of s2 below s1\n",
    "* Method for Series & DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat()\n",
    "\n",
    "* concat(): pandas module function\n",
    "* Invocation:\n",
    "* pd.concat([s1, s2, s3])\n",
    "* Can stack row-wise or column-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat() & .append()\n",
    "\n",
    "* Equivalence of concat() & .append():\n",
    "* result1 = pd.concat([s1, s2, s3])\n",
    "* result2 = s1.append(s2).append(s3)\n",
    "* result1 == result2 elementwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series of US states\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "northeast = pd.Series(['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'])\n",
    "south = pd.Series(['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'])\n",
    "midwest = pd.Series(['IL', 'IN', 'MN', 'MO', 'NE', 'ND', 'SD', 'IA', 'KS', 'MI', 'OH', 'WI'])\n",
    "west = pd.Series(['AZ', 'CO', 'ID', 'MT',\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .append()\n",
    "\n",
    "```python\n",
    "east = northeast.append(south)\n",
    "print(east)\n",
    "0 CT       7 DC\n",
    "1 ME       8 WV\n",
    "2 MA       9 AL\n",
    "3 NH       10 KY\n",
    "4 RI       11 MS\n",
    "5 VT       12 TN\n",
    "6 NJ       13 AR\n",
    "7 NY       14 LA\n",
    "8 PA       15 OK\n",
    "0 DE       16 TX\n",
    "1 FL       dtype: object\n",
    "2 GA\n",
    "3 MD\n",
    "4 NC\n",
    "5 SC\n",
    "6 VA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The appended Index\n",
    "\n",
    "```python\n",
    "print(east.index)\n",
    "Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], dtype='int64')\n",
    "\n",
    "print(east.loc[3])\n",
    "3 NH\n",
    "3 MD\n",
    "dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .reset_index()\n",
    "\n",
    "```python\n",
    "new_east = northeast.append(south).reset_index(drop=True)\n",
    "print(new_east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "9 DE\n",
    "10 FL\n",
    "dtype: object\n",
    "\n",
    "print(new_east.index)\n",
    "RangeIndex(start=0, stop=26, step=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using concat()\n",
    "\n",
    "```python\n",
    "east = pd.concat([northeast, south])\n",
    "print(east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "0 DE\n",
    "1 FL\n",
    "dtype: object\n",
    "print(east.index)\n",
    "Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], dtype='int64')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ignore_index\n",
    "\n",
    "```python\n",
    "new_east = pd.concat([northeast, south], ignore_index=True)\n",
    "print(new_east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "9 DE\n",
    "10 FL\n",
    "dtype: object\n",
    "print(new_east.index)\n",
    "RangeIndex(start=0, stop=26, step=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending Series with nonunique Indices\n",
    "\n",
    "The Series ```bronze``` and ```silver```, which have been printed in the IPython Shell, represent the 5 countries that won the most bronze and silver Olympic medals respectively between 1896 & 2008. The Indexes of both Series are called ```Country``` and the values are the corresponding number of medals won.\n",
    "\n",
    "If you were to run the command ```combined = bronze.append(silver)```, how many rows would ```combined``` have? And how many rows would ```combined.loc['United States']``` return? Find out for yourself by running these commands in the IPython Shell.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Possible Answers\n",
    "* combined has 5 rows and combined.loc['United States'] is empty (0 rows).\n",
    "* <mark>combined has 10 rows and combined.loc['United States'] has 2 rows.</mark>\n",
    "* combined has 6 rows and combined.loc['United States'] has 1 row.\n",
    "* combined has 5 rows and combined.loc['United States'] has 2 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze5_file, index_col=0)\n",
    "bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver = pd.read_csv(so_silver5_file, index_col=0)\n",
    "silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = bronze.append(silver)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.loc['United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending pandas Series\n",
    "\n",
    "In this exercise, you'll load sales data from the months January, February, and March into DataFrames. Then, you'll extract Series with the ```'Units'``` column from each and append them together with method chaining using ```.append()```.\n",
    "\n",
    "To check that the stacking worked, you'll print slices from these Series, and finally, you'll add the result to figure out the total units sold in the first quarter.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read the files ```'sales-jan-2015.csv'```, ```'sales-feb-2015.csv'``` and ```'sales-mar-2015.csv'``` into the DataFrames ```jan```, ```feb```, and ```mar``` respectively.\n",
    "* Use ```parse_dates=True``` and ```index_col='Date'```.\n",
    "* Extract the ```'Units'``` column of ```jan```, ```feb```, and ```mar``` to create the Series ```jan_units```, ```feb_units```, and ```mar_units``` respectively.\n",
    "* Construct the Series ```quarter1``` by appending ```feb_units``` to ```jan_units``` and then appending ```mar_units``` to the result. Use chained calls to the ```.append()``` method to do this.\n",
    "* Verify that ```quarter1``` has the individual Series stacked vertically. To do this:\n",
    "* Print the slice containing rows from ```jan 27, 2015``` to ```feb 2, 2015```.\n",
    "* Print the slice containing rows from ```feb 26, 2015``` to ```mar 7, 2015```.\n",
    "* Compute and print the total number of units sold from the Series ```quarter1```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv(sales_jan_2015_file, parse_dates=True, index_col='Date')\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv(sales_feb_2015_file, parse_dates=True, index_col='Date')\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv(sales_mar_2015_file, parse_dates=True, index_col='Date')\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating pandas Series along row axis\n",
    "\n",
    "Having learned how to append Series, you'll now learn how to achieve the same result by concatenating Series instead. You'll continue to work with the sales data you've seen previously. This time, the DataFrames ```jan```, ```feb```, and``` mar``` have been pre-loaded.\n",
    "\n",
    "Your job is to use ```pd.concat()``` with a list of Series to achieve the same result that you would get by chaining calls to ```.append()```.\n",
    "\n",
    "You may be wondering about the difference between ```pd.concat()``` and pandas' ```.append()``` method. One way to think of the difference is that ```.append()``` is a specific case of a concatenation, while ```pd.concat()``` gives you more flexibility, as you'll see in later exercises.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create an empty list called ```units```. This has been done for you.\n",
    "    * Use a ```for``` loop to iterate over ```[jan, feb, mar]```:\n",
    "* In each iteration of the loop, append the ```'Units'``` column of each DataFrame to ```units```.\n",
    "    * Concatenate the Series contained in the list ```units``` into a longer Series called ```quarter1``` using ```pd.concat()```.\n",
    "* Specify the keyword argument ```axis='rows'``` to stack the Series vertically.\n",
    "* Verify that ```quarter1``` has the individual Series stacked vertically by printing slices. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month.Units)\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units, axis='rows')\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bronze, silver, combined, jan, feb, mar, jan_units, feb_units, mar_units, quarter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending & concatenating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading population data\n",
    "\n",
    "```python\n",
    "In [1]: import pandas as pd\n",
    "In [2]: pop1 = pd.read_csv('population_01.csv', index_col=0)\n",
    "In [3]: pop2 = pd.read_csv('population_02.csv', index_col=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1_data = {'Zip Code ZCTA': [66407, 72732, 50579, 46421], '2010 Census Population': [479, 4716, 2405, 30670]}\n",
    "pop2_data = {'Zip Code ZCTA': [12776, 76092, 98360, 49464], '2010 Census Population': [2180, 26669, 12221, 27481]}\n",
    "\n",
    "pop1 = pd.DataFrame.from_dict(pop1_data)\n",
    "pop1.set_index('Zip Code ZCTA', drop=True, inplace=True)\n",
    "pop2 = pd.DataFrame.from_dict(pop2_data)\n",
    "pop2.set_index('Zip Code ZCTA', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(pop1), pop1.shape)\n",
    "print(type(pop2), pop2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending population DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1.append(pop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pop1.index.name, pop1.columns)\n",
    "print(pop2.index.name, pop2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data\n",
    "\n",
    "```python\n",
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "unemployment = pd.read_csv('unemployment_00.csv', index_col=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data = {'Zip Code ZCTA': [57538, 59916, 37660, 2860], '2010 Census Population': [322, 130, 40038, 45199]}\n",
    "emp_data = {'Zip': [2860, 46167, 1097, 80808], 'unemployment': [0.11, 0.02, 0.33, 0.07], 'participants': [34447, 4800, 42, 4310]}\n",
    "\n",
    "population = pd.DataFrame.from_dict(pop_data)\n",
    "population.set_index('Zip Code ZCTA', drop=True, inplace=True)\n",
    "unemployment = pd.DataFrame.from_dict(emp_data)\n",
    "unemployment.set_index('Zip', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending population & unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.append(unemployment, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated index labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.append(unemployment, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating rows\n",
    "\n",
    "* with ```axis=0```, ```pd.concat``` is the same as ```population.append(unemployment, sort=True)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([population, unemployment], axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating column\n",
    "\n",
    "* outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([population, unemployment], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pop1_data, pop2_data, pop1, pop2, pop_data, emp_data, population, unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending DataFrames with ignore_index\n",
    "\n",
    "In this exercise, you'll use the [Baby Names Dataset](https://www.data.gov/developers/baby-names-dataset/) (from [data.gov](http://data.gov/)) again. This time, both DataFrames ```names_1981``` and ```names_1881``` are loaded without specifying an Index column (so the default Indexes for both are RangeIndexes).\n",
    "\n",
    "You'll use the DataFrame ```.append()``` method to make a DataFrame ```combined_names```. To distinguish rows from the original two DataFrames, you'll add a ```'year'``` column to each with the year (1881 or 1981 in this case). In addition, you'll specify ```ignore_index=True``` so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled ```0, 1, ..., n-1```, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a ```'year'``` column in the DataFrames ```names_1881``` and ```names_1981```, with values of ```1881``` and ```1981``` respectively. Recall that assigning a scalar value to a DataFrame column broadcasts that value throughout.\n",
    "* Create a new DataFrame called ```combined_names``` by appending the rows of ```names_1981``` underneath the rows of ```names_1881```. Specify the keyword argument ```ignore_index=True``` to make a new RangeIndex of unique integers for each row.\n",
    "* Print the shapes of all three DataFrames. This has been done for you.\n",
    "* Extract all rows from ```combined_names``` that have the name ```'Morgan'```. To do this, use the ```.loc[]``` accessor with an appropriate filter. The relevant column of ```combined_names``` here is ```'name'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_1881 = pd.read_csv(baby_1881_file, header=None, names=['name', 'gender', 'count'])\n",
    "names_1981 = pd.read_csv(baby_1981_file, header=None, names=['name', 'gender', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_1981.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index=True, sort=False)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "combined_names[combined_names.name  == 'Morgan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating pandas DataFrames along column axis\n",
    "\n",
    "The function ```pd.concat()``` can concatenate DataFrames *horizontally* as well as *vertically* (vertical is the default). To make the DataFrames stack horizontally, you have to specify the keyword argument ```axis=1``` or``` axis='columns'```.\n",
    "\n",
    "In this exercise, you'll use weather data with maximum and mean daily temperatures sampled at different rates (quarterly versus monthly). You'll concatenate the rows of both and see that, where rows are missing in the coarser DataFrame, null values are inserted in the concatenated DataFrame. This corresponds to an outer join (which you will explore in more detail in later exercises).\n",
    "\n",
    "The files ```'quarterly_max_temp.csv'``` and ```'monthly_mean_temp.csv'``` have been pre-loaded into the DataFrames ```weather_max``` and ```weather_mean``` respectively, and ```pandas``` has been imported as ```pd```.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a new DataFrame called ```weather``` by concatenating the DataFrames ```weather_max``` and ```weather_mean``` *horizontally*.\n",
    "    * Pass the DataFrames to ```pd.concat()``` as a list and specify the keyword argument ```axis=1``` to stack them horizontally.\n",
    "* Print the new DataFrame ```weather```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_mean_data = {'Mean TemperatureF': [53.1, 70., 34.93548387, 28.71428571, 32.35483871, 72.87096774, 70.13333333, 35., 62.61290323, 39.8, 55.4516129 , 63.76666667],\n",
    "                     'Month': ['Apr', 'Aug', 'Dec', 'Feb', 'Jan', 'Jul', 'Jun', 'Mar', 'May', 'Nov', 'Oct', 'Sep']}\n",
    "weather_max_data = {'Max TemperatureF': [68, 89, 91, 84], 'Month': ['Jan', 'Apr', 'Jul', 'Oct']}\n",
    "\n",
    "weather_mean = pd.DataFrame.from_dict(weather_mean_data)\n",
    "weather_mean.set_index('Month', inplace=True, drop=True)\n",
    "weather_max = pd.DataFrame.from_dict(weather_max_data)\n",
    "weather_max.set_index('Month', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate weather_max and weather_mean horizontally: weather\n",
    "weather = pd.concat([weather_max, weather_mean], axis=1, sort=True)\n",
    "\n",
    "# Print weather\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading multiple files to build a DataFrame\n",
    "\n",
    "It is often convenient to build a large DataFrame by parsing many files as DataFrames and concatenating them all at once. You'll do this here with three files, but, in principle, this approach can be used to combine data from dozens or hundreds of files.\n",
    "\n",
    "Here, you'll work with DataFrames compiled from [The Guardian's Olympic medal dataset](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data).\n",
    "\n",
    "```pandas``` has been imported as ```pd``` and two lists have been pre-loaded: An empty list called ```medals```, and ```medal_types```, which contains the strings ```'bronze'```, ```'silver'```, and ```'gold'```.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Iterate over ```medal_types``` in the for ```loop```.\n",
    "* Inside the ```for``` loop:\n",
    "    * Create ```file_name``` using string interpolation with the loop variable ```medal```. This has been done for you. The expression ```\"%s_top5.csv\" % medal``` evaluates as a string with the value of ```medal``` replacing ```%s``` in the format string.\n",
    "    * Create the list of column names called ```columns```. This has been done for you.\n",
    "    * Read ```file_name``` into a DataFrame called ```medal_df```. Specify the keyword arguments ```header=0```, ```index_col='Country'```, and ```names=columns``` to get the correct row and column Indexes.\n",
    "    * Append ```medal_df``` to ```medals``` using the list ```.append()``` method.\n",
    "* Concatenate the list of DataFrames ```medals``` horizontally (using ```axis='columns'```) to create a single DataFrame called ```medals```. Print it in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five = data.glob('*_top5.csv')\n",
    "for file in top_five:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "medal_list = list()\n",
    "\n",
    "for medal in medal_types:\n",
    "\n",
    "    # Create the file name: file_name\n",
    "    file_name = data / f'summer_olympics_{medal}_top5.csv'\n",
    "    \n",
    "    # Create list of column names: columns\n",
    "    columns = ['Country', medal]\n",
    "    \n",
    "    # Read file_name into a DataFrame: df\n",
    "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
    "\n",
    "    # Append medal_df to medals\n",
    "    medal_list.append(medal_df)\n",
    "\n",
    "# Concatenate medals horizontally: medals\n",
    "medals = pd.concat(medal_list, axis='columns', sort=True)\n",
    "\n",
    "# Print medals\n",
    "medals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del names_1881, names_1981, combined_names, weather_mean_data, weather_max_data, weather_mean, weather_max, weather, top_five, medals, medal_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation, keys & MultiIndexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading rainfall data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "file1 = 'q1_rainfall_2013.csv'\n",
    "rain2013 = pd.read_csv(file1, index_col='Month', parse_dates=True)\n",
    "file2 = 'q1_rainfall_2014.csv'\n",
    "rain2014 = pd.read_csv(file2, index_col='Month', parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_2013_data = {'Month': ['Jan', 'Feb', 'Mar'], 'Precipitation': [0.096129, 0.067143, 0.061613]}\n",
    "rain_2014_data = {'Month': ['Jan', 'Feb', 'Mar'], 'Precipitation': [0.050323, 0.082143, 0.070968]}\n",
    "\n",
    "rain2013 = pd.DataFrame.from_dict(rain_2013_data)\n",
    "rain2013.set_index('Month', inplace=True)\n",
    "rain2014 = pd.DataFrame.from_dict(rain_2014_data)\n",
    "rain2014.set_index('Month', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining rainfall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([rain2013, rain2014], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using multi-index on rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis=0)\n",
    "rain1314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain1314.loc[2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain1314 = pd.concat([rain2013, rain2014], axis='columns')\n",
    "rain1314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a multi-index on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis='columns')\n",
    "rain1314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain1314[2013]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.concat() with dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_dict = {2013: rain2013, 2014: rain2014}\n",
    "rain1314 = pd.concat(rain_dict, axis='columns')\n",
    "rain1314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rain_2013_data, rain_2014_data, rain2013, rain2014, rain1314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating vertically to get MultiIndexed rows\n",
    "\n",
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. This can be done by specifying the ```keys``` parameter in the call to ```pd.concat()```, which generates a hierarchical index with the labels from ```keys``` as the outermost index label. So you don't have to rename the columns of each DataFrame as you load it. Instead, only the Index column needs to be specified.\n",
    "\n",
    "Here, you'll continue working with DataFrames compiled from [The Guardian's Olympic medal dataset](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data). Once again, ```pandas``` has been imported as ```pd``` and two lists have been pre-loaded: An empty list called ```medals```, and ```medal_types```, which contains the strings ```'bronze'```, ```'silver'```, and ```'gold'```.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Within the ```for``` loop:\n",
    "    * Read ```file_name``` into a DataFrame called ```medal_df```. Specify the index to be ```'Country'```.\n",
    "    * Append ```medal_df``` to ```medals```.\n",
    "* Concatenate the list of DataFrames ```medals``` into a single DataFrame called ```medals```. Be sure to use the keyword argument ```keys=['bronze', 'silver', 'gold']``` to create a vertically stacked DataFrame with a MultiIndex.\n",
    "* Print the new DataFrame ```medals```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "medal_list = list()\n",
    "\n",
    "for medal in medal_types:\n",
    "\n",
    "    # Create the file name: file_name\n",
    "    file_name = data / f'summer_olympics_{medal}_top5.csv'\n",
    "    \n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, index_col='Country')\n",
    "    \n",
    "    # Append medal_df to medals\n",
    "    medal_list.append(medal_df)\n",
    "    \n",
    "# Concatenate medals: medals\n",
    "medals = pd.concat(medal_list, keys=['bronze', 'silver', 'gold'])\n",
    "\n",
    "# Print medals in entirety\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing MultiIndexed DataFrames\n",
    "\n",
    "This exercise picks up where the last ended (again using [The Guardian's Olympic medal dataset](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data)).\n",
    "\n",
    "You are provided with the MultiIndexed DataFrame as produced at the end of the preceding exercise. Your task is to sort the DataFrame and to use the ```pd.IndexSlice``` to extract specific slices. Check out [this exercise](https://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/advanced-indexing?ex=10) from Manipulating DataFrames with pandas to refresh your memory on how to deal with MultiIndexed DataFrames.\n",
    "\n",
    "```pandas``` has been imported for you as ```pd``` and the DataFrame ```medals``` is already in your namespace.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a new DataFrame ```medals_sorted``` with the entries of ```medals``` sorted. Use ```.sort_index(level=0)``` to ensure the Index is sorted suitably.\n",
    "* Print the number of bronze medals won by Germany and all of the silver medal data. This has been done for you.\n",
    "* Create an alias for ```pd.IndexSlice``` called ```idx```. A slicer ```pd.IndexSlice``` is required when slicing on the *inner* level of a MultiIndex.\n",
    "* Slice all the data on medals won by the United Kingdom. To do this, use the ```.loc[]``` accessor with ```idx[:,'United Kingdom'], :```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the entries of medals: medals_sorted\n",
    "medals_sorted = medals.sort_index(level=0)\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "medals_sorted.loc[idx[:, 'United Kingdom'], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating horizontally to get MultiIndexed columns\n",
    "\n",
    "It is also possible to construct a DataFrame with hierarchically indexed columns. For this exercise, you'll start with pandas imported and a list of three DataFrames called ```dataframes```. All three DataFrames contain ```'Company'```, ```'Product'```, and ```'Units'``` columns with a ```'Date'``` column as the index pertaining to sales transactions during the month of February, 2015. The first DataFrame describes ```Hardware``` transactions, the second describes ```Software``` transactions, and the third, ```Service``` transactions.\n",
    "\n",
    "Your task is to concatenate the DataFrames horizontally and to create a MultiIndex on the columns. From there, you can summarize the resulting DataFrame and slice some information from it.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Construct a new DataFrame ```february``` with MultiIndexed columns by concatenating the list ```dataframes```.\n",
    "* Use ```axis=1``` to stack the DataFrames horizontally and the keyword argument ```keys=['Hardware', 'Software', 'Service']``` to construct a hierarchical Index from each DataFrame.\n",
    "* Print summary information from the new DataFrame ```february``` using the ```.info()``` method. This has been done for you.\n",
    "* Create an alias called ```idx``` for ```pd.IndexSlice```.\n",
    "* Extract a slice called ```slice_2_8``` from ```february``` (using ```.loc[]``` & ```idx```) that comprises rows between Feb. 2, 2015 to Feb. 8, 2015 from columns under ```'Company'```.\n",
    "* Print the ```slice_2_8```. This has been done for you, so hit 'Submit Answer' to see the sliced data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw = pd.read_csv(sales_feb_hardware_file, index_col='Date')\n",
    "sw = pd.read_csv(sales_feb_software_file, index_col='Date')\n",
    "sv = pd.read_csv(sales_feb_service_file, index_col='Date')\n",
    "\n",
    "dataframes = [hw, sw, sv]\n",
    "dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes: february\n",
    "february = pd.concat(dataframes, axis=1, keys=['Hardware', 'Software', 'Service'], sort=True)\n",
    "\n",
    "# Print february.info()\n",
    "february.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "february"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['2015-02-02':'2015-02-08', idx[:, 'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "slice_2_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating DataFrames from a dict\n",
    "\n",
    "You're now going to revisit the sales data you worked with earlier in the chapter. Three DataFrames ```jan```, ```feb```, and ```mar``` have been pre-loaded for you. Your task is to aggregate the sum of all sales over the ```'Company'``` column into a single DataFrame. You'll do this by constructing a dictionary of these DataFrames and then concatenating them.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a list called ```month_list``` consisting of the tuples ```('january', jan)```, ```('february', feb)```, and ```('march', mar)```.\n",
    "* Create an empty dictionary called ```month_dict```.\n",
    "* Inside the ```for``` loop:\n",
    "    * Group ```month_data``` by ```'Company'``` and use ```.sum()``` to aggregate.\n",
    "* Construct a new DataFrame called ```sales``` by concatenating the DataFrames stored in ```month_dict```.\n",
    "* Create an alias for ```pd.IndexSlice``` and print all sales by ```'Mediacore'```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan = pd.read_csv(sales_jan_2015_file)\n",
    "feb = pd.read_csv(sales_feb_2015_file)\n",
    "mar = pd.read_csv(sales_mar_2015_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = dict()\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby(['Company']).sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)\n",
    "\n",
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del medal_types, medal_list, medal_df, medals, medals_sorted, idx, hw, sw, sv, dataframes, february, slice_2_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer & inner joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using with arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(8).reshape(2, 4) + 0.1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.arange(6).reshape(2,3) + 0.2\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.arange(12).reshape(3,4) + 0.3\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking arrays horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([B, A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([B, A], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking arrays vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack([A, C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, C], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incompatible array dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, B], axis=0) # incompatible columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, C], axis=1) # incompatible rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data\n",
    "\n",
    "```python\n",
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "\n",
    "unemployment = pd.read_csv('unemployment_00.csv', index_col=0)\n",
    "print(population)\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "57538 322\n",
    "59916 130\n",
    "37660 40038\n",
    "2860 45199\n",
    "\n",
    "print(unemployment)\n",
    "unemployment participants\n",
    "Zip\n",
    "2860 0.11 34447\n",
    "46167 0.02 4800\n",
    "1097 0.33 42\n",
    "80808 0.07 4310\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to arrays\n",
    "\n",
    "```python\n",
    "population_array = np.array(population)\n",
    "print(population_array) # Index info is lost\n",
    "[[ 322]\n",
    "[ 130]\n",
    "[40038]\n",
    "[45199]]\n",
    "\n",
    "unemployment_array = np.array(unemployment)\n",
    "print(population_array)\n",
    "[[ 1.10000000e-01 3.44470000e+04]\n",
    "[ 2.00000000e-02 4.80000000e+03]\n",
    "[ 3.30000000e-01 4.20000000e+01]\n",
    "[ 7.00000000e-02 4.31000000e+03]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating data as arrays\n",
    "\n",
    "```python\n",
    "print(np.concatenate([population_array, unemployment_array], axis=1))\n",
    "[[ 3.22000000e+02 1.10000000e-01 3.44470000e+04]\n",
    "[ 1.30000000e+02 2.00000000e-02 4.80000000e+03]\n",
    "[ 4.00380000e+04 3.30000000e-01 4.20000000e+01]\n",
    "[ 4.51990000e+04 7.00000000e-02 4.31000000e+03]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins\n",
    "\n",
    "* Joining tables: Combining rows of multiple tables\n",
    "* Outer join\n",
    "    * Union of index sets (all labels, no repetition)\n",
    "    * Missing fields filled with NaN\n",
    "    * Preserves the indices in the original tables, filling null values for missing rows\n",
    "    * Has all the indices of the original tables without repetiton (like a set union)\n",
    "* Inner join\n",
    "    * Intersection of index sets (only common labels)\n",
    "    * Has only labels common to both tables (like a set intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation & inner join\n",
    "\n",
    "* only the row label present in both DataFrames is preserved\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=1, join='inner')\n",
    "\n",
    "2010 Census Population unemployment participants\n",
    "2860 45199 0.11 34447\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation & outer join\n",
    "\n",
    "* All row indiecs from the original two indexes exist in the joind DataFrame index.\n",
    "* When a row occurs in one DataFrame, but not in the other, the missing column entries are filled with null values\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=1, join='outer')\n",
    "\n",
    "2010 Census Population unemployment participants\n",
    "1097 NaN 0.33 42.0\n",
    "2860 45199.0 0.11 34447.0\n",
    "37660 40038.0 NaN NaN\n",
    "46167 NaN 0.02 4800.0\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "80808 NaN 0.07 4310.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join on other axis\n",
    "\n",
    "* The resulting DataFrame is empty becasue no column index label appears in both population and unemployment\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], join='inner', axis=0)\n",
    "\n",
    "Empty DataFrame\n",
    "Columns: []\n",
    "Index: [2860, 46167, 1097, 80808, 57538, 59916, 37660, 2860]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating DataFrames with inner join\n",
    "\n",
    "Here, you'll continue working with DataFrames compiled from [The Guardian's Olympic medal dataset](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data).\n",
    "\n",
    "The DataFrames ```bronze```, ```silver```, and ```gold``` have been pre-loaded for you.\n",
    "\n",
    "Your task is to compute an inner join.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Construct a list of DataFrames called ```medal_list``` with entries ```bronze```, ```silver```, and ```gold```.\n",
    "* Concatenate ```medal_list``` horizontally with an inner join to create ```medals```.\n",
    "    * Use the keyword argument ```keys=['bronze', 'silver', 'gold']``` to yield suitable hierarchical indexing.\n",
    "    * Use ```axis=1``` to get horizontal concatenation.\n",
    "    * Use ```join='inner'``` to keep only rows that share common index labels.\n",
    "* Print the new DataFrame ```medals```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze5_file)\n",
    "silver = pd.read_csv(so_silver_file)\n",
    "gold = pd.read_csv(so_gold_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, keys=['bronze', 'silver', 'gold'], axis=1, join='inner')\n",
    "\n",
    "# Print medals\n",
    "medals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling & concatenating DataFrames with inner join\n",
    "\n",
    "In this exercise, you'll compare the historical 10-year GDP (Gross Domestic Product) growth in the US and in China. The data for the US starts in 1947 and is recorded quarterly; by contrast, the data for China starts in 1961 and is recorded annually.\n",
    "\n",
    "You'll need to use a combination of resampling and an inner join to align the index labels. You'll need an appropriate [offset alias](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases) for resampling, and the method ```.resample()``` must be chained with some kind of aggregation method (```.pct_change()``` and ```.last()``` in this case).\n",
    "\n",
    "```pandas``` has been imported as ```pd```, and the DataFrames ```china``` and ```us``` have been pre-loaded, with the output of ```china.head()``` and ```us.head()``` printed in the IPython Shell.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Make a new DataFrame ```china_annual``` by resampling the DataFrame ```china``` with ```.resample('A').last()``` (i.e., with *annual* frequency) and chaining two method calls:\n",
    "* Chain ```.pct_change(10)``` as an aggregation method to compute the percentage change with an offset of ten years.\n",
    "* Chain ```.dropna()``` to eliminate rows containing null values.\n",
    "* Make a new DataFrame ```us_annual``` by resampling the DataFrame ```us``` exactly as you resampled ```china```.\n",
    "* Concatenate ```china_annual``` and ```us_annual``` to construct a DataFrame called ```gdp```. Use ```join='inner'``` to perform an *inner* join and use ```axis=1``` to concatenate *horizontally*.\n",
    "* Print the result of resampling ```gdp``` every decade (i.e., using ```.resample('10A')```) and aggregating with the method ```.last()```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china = pd.read_csv(gdp_china_file, parse_dates=['Year'])\n",
    "china.rename(columns={'GDP': 'China'}, inplace=True)\n",
    "china.set_index('Year', inplace=True)\n",
    "\n",
    "us = pd.read_csv(gdp_usa_file, parse_dates=['DATE'])\n",
    "us.rename(columns={'DATE': 'Year', 'VALUE': 'US'}, inplace=True)\n",
    "us.set_index('Year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample and tidy china: china_annual\n",
    "china_annual = china.resample('A').last().pct_change(10).dropna()\n",
    "china_annual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample and tidy us: us_annual\n",
    "us_annual = us.resample('A').last().pct_change(10).dropna()\n",
    "us_annual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate china_annual and us_annual: gdp\n",
    "gdp = pd.concat([china_annual, us_annual], join='inner', axis=1)\n",
    "\n",
    "# Resample gdp and print\n",
    "gdp.resample('10A').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bronze, silver, gold, medal_list, medals, china, us, china_annual, us_annual, gdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Merging Data\n",
    "\n",
    "Here, you'll learn all about merging pandas DataFrames. You'll explore different techniques for merging, and learn about left joins, right joins, inner joins, and outer joins, as well as when to use which. You'll also learn about ordered merging, which is useful when you want to merge DataFrames whose columns have natural orderings, like date-time columns.\n",
    "\n",
    "* ```merge()``` extends ```concat()``` with the ability to align rows using multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Merging DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_zipcode_population = {'Zipcode': [16855, 15681, 18657, 17307, 15635],\n",
    "                         '2010 Census Population': [282, 5241, 11985, 5899, 220]}\n",
    "pa_zipcode_city = {'Zipcode': [17545,18455, 17307, 15705, 16833, 16220, 18618, 16855, 16623, 15635, 15681, 18657, 15279, 17231, 18821],\n",
    "                   'City': ['MANHEIM', 'PRESTON PARK', 'BIGLERVILLE', 'INDIANA', 'CURWENSVILLE', 'CROWN', 'HARVEYS LAKE', 'MINERAL SPRINGS',\n",
    "                            'CASSVILLE', 'HANNASTOWN', 'SALTSBURG', 'TUNKHANNOCK', 'PITTSBURG', 'LEMASTERS', 'GREAT BEND'],\n",
    "                   'State': ['PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA', 'PA']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.DataFrame.from_dict(pa_zipcode_population)\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cities DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame.from_dict(pa_zipcode_city)\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging\n",
    "\n",
    "* ```pd.merge()``` computes a merge on ALL columns that occur in both DataFrames\n",
    "    * in the following case, the common column is **Zipcode**\n",
    "    * for any row in which the Zipcode entry in cities matches a row in population, a new row is made in the merfed DataFrame.\n",
    "    * by default, this is an inner join\n",
    "        * it's an inner join because it glues together only rows that match in the joining columns of **BOTH** DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(population, cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medal DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze_file)\n",
    "bronze.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(so_gold_file)\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging all columns\n",
    "\n",
    "* by default, ```pd.merge()``` uses all columns common to both DataFrames to merge\n",
    "* the rows of the merged DataFrame consist of all rows where the **NOC**, **Country**, and **Totals** columns are identical in both DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold)\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(so_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on='NOC')\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(so_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on multiple columns\n",
    "\n",
    "* this is where merging extend concatenation in allowing matching on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'])\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'])\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counties DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_counties = {'CITY NAME': ['SALTSBURG', 'MINERAL SPRINGS', 'BIGLERVILLE', 'HANNASTOWN', 'TUNKHANNOCK'],\n",
    "               'COUNTY NAME': ['INDIANA', 'CLEARFIELD', 'ADAMS', 'WESTMORELAND', 'WYOMING']}\n",
    "counties = pd.DataFrame.from_dict(pa_counties)\n",
    "counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying columns to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(counties, cities, left_on='CITY NAME', right_on='City')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switching left/right DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(cities, counties, left_on='City', right_on='CITY NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pa_zipcode_population, pa_zipcode_city, population, cities, bronze, gold, so_merge, pa_counties, counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging company DataFrames\n",
    "\n",
    "Suppose your company has operations in several different cities under several different managers. The DataFrames **revenue** and **managers** contain partial information related to the company. That is, the rows of the **city** columns don't quite match in **revenue** and **managers** (the Mendocino branch has no revenue yet since it just opened and the manager of Springfield branch recently left the company).\n",
    "\n",
    "The DataFrames have been printed in the IPython Shell. If you were to run the command ```combined = pd.merge(revenue, managers, on='city')```, how many rows would **combined** have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = {'city': ['Austin', 'Denver', 'Springfield'], 'revenue': [100, 83, 4]}\n",
    "man = {'city': ['Austin', 'Denver', 'Mendocino'], 'manager': ['Charles', 'Joel', 'Brett']}\n",
    "\n",
    "revenue = pd.DataFrame.from_dict(rev)\n",
    "managers = pd.DataFrame.from_dict(man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(revenue, managers, on='city')\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on a specific column\n",
    "\n",
    "This exercise follows on the last one with the DataFrames ```revenue``` and ```managers``` for your company. You expect your company to grow and, eventually, to operate in cities with the same name on different states. As such, you decide that every branch should have a numerical branch identifier. Thus, you add a ```branch_id``` column to both DataFrames. Moreover, new cities have been added to both the ```revenue``` and ```managers``` DataFrames as well. ```pandas``` has been imported as pd and both DataFrames are available in your namespace.\n",
    "\n",
    "At present, there should be a 1-to-1 relationship between the ```city``` and ```branch_id``` fields. In that case, the result of a merge on the ```city``` columns ought to give you the same output as a merge on the ```branch_id``` columns. Do they? Can you spot an ambiguity in one of the DataFrames?\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Using ```pd.merge()```, merge the DataFrames ```revenue``` and ```managers``` on the ```'city'``` column of each. Store the result as ```merge_by_city```.\n",
    "* Print the DataFrame ```merge_by_city```. This has been done for you.\n",
    "* Merge the DataFrames ```revenue``` and ```managers``` on the ```'branch_id'``` column of each. Store the result as ```merge_by_id```.\n",
    "* Print the DataFrame ```merge_by_id```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = {'city': ['Austin', 'Denver', 'Springfield', 'Mendocino'], 'revenue': [100, 83, 4, 200], 'branch_id': [10, 20, 30, 47]}\n",
    "man = {'city': ['Austin', 'Denver', 'Mendocino', 'Springfield'], 'manager': ['Charles', 'Joel', 'Brett', 'Sally'], 'branch_id': [10, 20, 47, 31]}\n",
    "\n",
    "revenue = pd.DataFrame.from_dict(rev)\n",
    "managers = pd.DataFrame.from_dict(man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge revenue with managers on 'city': merge_by_city\n",
    "merge_by_city = pd.merge(revenue, managers, on='city')\n",
    "\n",
    "# Print merge_by_city\n",
    "merge_by_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge revenue with managers on 'branch_id': merge_by_id\n",
    "merge_by_id = pd.merge(revenue, managers, on='branch_id')\n",
    "\n",
    "# Print merge_by_id\n",
    "merge_by_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when you merge on ```'city'```, the resulting DataFrame has a peculiar result: In row 2, the city Springfield has two different branch IDs. This is because there are actually two different cities named Springfield - one in the State of Illinois, and the other in Missouri. The ```revenue``` DataFrame has the one from Illinois, and the ```managers``` DataFrame has the one from Missouri. Consequently, when you merge on ```'branch_id'```, both of these get dropped from the merged DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on columns with non-matching labels\n",
    "\n",
    "You continue working with the ```revenue``` & ```managers``` DataFrames from before. This time, someone has changed the field name ```'city'``` to ```'branch'``` in the ```managers``` table. Now, when you attempt to merge DataFrames, an exception is thrown:\n",
    "\n",
    "```python\n",
    ">>> pd.merge(revenue, managers, on='city')\n",
    "Traceback (most recent call last):\n",
    "    ... <text deleted> ...\n",
    "    pd.merge(revenue, managers, on='city')\n",
    "    ... <text deleted> ...\n",
    "KeyError: 'city'\n",
    "```\n",
    "    \n",
    "Given this, it will take a bit more work for you to join or merge on the city/branch name. You have to specify the ```left_on``` and ```right_on``` parameters in the call to ```pd.merge()```.\n",
    "\n",
    "As before, ```pandas``` has been pre-imported as ```pd``` and the ```revenue``` and ```managers``` DataFrames are in your namespace. They have been printed in the IPython Shell so you can examine the columns prior to merging.\n",
    "\n",
    "Are you able to merge better than in the last exercise? How should the rows with ```Springfield``` be handled?\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Merge the DataFrames ```revenue``` and ```managers``` into a single DataFrame called ```combined``` using the ```'city'``` and ```'branch'``` columns from the appropriate DataFrames.\n",
    "    * In your call to ```pd.merge()```, you will have to specify the parameters ```left_on``` and ```right_on``` appropriately.\n",
    "* Print the new DataFrame ```combined```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_rev = {'Austin': 'TX', 'Denver': 'CO', 'Springfield': 'IL', 'Mendocino': 'CA'}\n",
    "state_man = {'Austin': 'TX', 'Denver': 'CO', 'Mendocino': 'CA', 'Springfield': 'MO'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue['state'] = revenue['city'].map(state_rev)\n",
    "managers['state'] = managers['city'].map(state_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers.rename(columns={'city': 'branch'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(revenue, managers, left_on='city', right_on='branch')\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on multiple columns\n",
    "\n",
    "Another strategy to disambiguate cities with identical names is to add information on the states in which the cities are located. To this end, you add a column called ```state``` to both DataFrames from the preceding exercises. Again, ```pandas``` has been pre-imported as ```pd``` and the ```revenue``` and ```managers``` DataFrames are in your namespace.\n",
    "\n",
    "Your goal in this exercise is to use ```pd.merge()``` to merge DataFrames using multiple columns (using ```'branch_id'```, ```'city'```, and ```'state'``` in this case).\n",
    "\n",
    "Are you able to match all your company's branches correctly?\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a column called ```'state'``` in the DataFrame ```revenue```, consisting of the list ```['TX','CO','IL','CA']```.\n",
    "* Create a column called ```'state'``` in the DataFrame ```managers```, consisting of the list ```['TX','CO','CA','MO']```.\n",
    "* Merge the DataFrames ```revenue``` and ```managers``` using three columns :```'branch_id'```, ```'city'```, and ```'state'```. Pass them in as a list to the ```on``` paramater of ```pd.merge()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers.rename(columns={'branch': 'city'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'state' column to revenue: revenue['state']\n",
    "revenue['state'] = ['TX','CO','IL','CA']\n",
    "\n",
    "# Add 'state' column to managers: managers['state']\n",
    "managers['state'] = ['TX','CO','CA','MO']\n",
    "\n",
    "# Merge revenue & managers on 'branch_id', 'city', & 'state': combined\n",
    "combined = pd.merge(revenue, managers, on=['branch_id', 'city', 'state'])\n",
    "\n",
    "# Print combined\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rev, man, revenue, managers, merge_by_city, merge_by_id, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames\n",
    "\n",
    "* Pandas has to search through DataFrame rows for matches when computing joins and merges\n",
    "    * It's useful to have different kinds of joins to mitigate costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medal DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze_file)\n",
    "bronze.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(so_gold_file)\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with inner join\n",
    "\n",
    "* ```merge()``` does an inner join by default\n",
    "    * it extracts the rows that match in joining columns from both DataFrames and it glues them together in the joined DataFrame\n",
    "    * the property ```how=innner``` is the default behavior   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='inner')\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with left join\n",
    "\n",
    "* using ```how=left``` keeps all rows of the left DataFrame in the merged DataFrame\n",
    "* Keeps all rows of the left DF in the merged DF\n",
    "* For rows in the left DF with matches in the right DF:\n",
    "    * Non-joining columns of right DF are appended to left DF\n",
    "* For rows in the left DF with no matches in the right DF:\n",
    "    * Non-joining columns are filled with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze5_file)\n",
    "gold = pd.read_csv(so_gold5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_noc = ['USA', 'URS', 'GBR', 'ITA', 'GER']\n",
    "b_noc = ['USA', 'URS', 'GBR', 'FRA', 'GER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold['NOC'] = g_noc\n",
    "bronze['NOC'] = b_noc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with right join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.DataFrame.from_dict({'Zip Code ZCTA': [57538, 59916, 37660, 2860],\n",
    "                                     '2010 Census Population': [322, 130, 40038, 45199]})\n",
    "population.set_index('Zip Code ZCTA', inplace=True)\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = pd.DataFrame.from_dict({'Zip': [2860, 46167, 1097],\n",
    "                                       'unemployment': [0.11, 0.02, 0.33],\n",
    "                                       'participants': [ 34447, 4800, 32]})\n",
    "unemployment.set_index('Zip', inplace=True)\n",
    "unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='left')\n",
    "\n",
    "* computes a left join using the Index by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.join(unemployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.join(unemployment, how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.join(unemployment, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.join(unemployment, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bronze, gold, so_merge, g_noc, b_noc, population, unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which should you use?\n",
    "\n",
    "* df1.append(df2): stacking vertically\n",
    "* pd.concat([df1, df2]):\n",
    "    * stacking many horizontally or vertically\n",
    "    * simple inner/outer joins on Indexes\n",
    "* df1.join(df2): inner/outer/left/right joins on Indexes\n",
    "* pd.merge([df1, df2]): many joins on multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = {'city': ['Austin', 'Denver', 'Springfield', 'Mendocino'],\n",
    "       'state': ['TX','CO','IL','CA'],\n",
    "       'revenue': [100, 83, 4, 200],\n",
    "       'branch_id': [10, 20, 30, 47]}\n",
    "\n",
    "man = {'city': ['Austin', 'Denver', 'Mendocino', 'Springfield'],\n",
    "       'state': ['TX','CO','CA','MO'],\n",
    "       'manager': ['Charles', 'Joel', 'Brett', 'Sally'],\n",
    "       'branch_id': [10, 20, 47, 31]}\n",
    "\n",
    "revenue = pd.DataFrame.from_dict(rev)\n",
    "revenue.set_index('branch_id', inplace=True)\n",
    "managers = pd.DataFrame.from_dict(man)\n",
    "managers.set_index('branch_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining by Index\n",
    "\n",
    "The DataFrames ```revenue``` and ```managers``` are displayed in the IPython Shell. Here, they are indexed by ```'branch_id'```.\n",
    "\n",
    "Choose the function call below that will join the DataFrames on their indexes and return 5 rows with index labels ```[10, 20, 30, 31, 47]```. Explore each of them in the IPython Shell to get a better understanding of their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue.join(managers, lsuffix='_rev', rsuffix='_mng', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a joining strategy\n",
    "\n",
    "Suppose you have two DataFrames: ```students``` (with columns ```'StudentID'```, ```'LastName'```, ```'FirstName'```, and ```'Major'```) and ```midterm_results``` (with columns ```'StudentID'```, ```'Q1'```, ```'Q2'```, and ```'Q3'``` for their scores on midterm questions).\n",
    "\n",
    "You want to combine the DataFrames into a single DataFrame ```grades```, and be able to easily spot which students wrote the midterm and which didn't (their midterm question scores ```'Q1'```, ```'Q2'```, & ```'Q3'``` should be filled with ```NaN``` values).\n",
    "\n",
    "You also want to drop rows from ```midterm_results``` in which the ```StudentID``` is not found in ```students```.\n",
    "\n",
    "Which of the following strategies gives the desired result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.DataFrame.from_dict({'StudentID': [], 'LastName': [], 'FirstName': [], 'Major': []})\n",
    "midterm_results = pd.DataFrame.from_dict({'StudentID': [], 'Q1': [], 'Q2': [], 'Q3': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midterm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.merge(students, midterm_results, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left & right merging on multiple columns\n",
    "\n",
    "You now have, in addition to the ```revenue``` and ```managers``` DataFrames from prior exercises, a DataFrame ```sales``` that summarizes units sold from specific branches (identified by ```city``` and ```state``` but not ```branch_id```).\n",
    "\n",
    "Once again, the ```managers``` DataFrame uses the label ```branch``` in place of ```city``` as in the other two DataFrames. Your task here is to employ *left* and *right* merges to preserve data and identify where data is missing.\n",
    "\n",
    "By merging ```revenue``` and ```sales``` with a *right* merge, you can identify the missing ```revenue``` values. Here, you don't need to specify ```left_on``` or ```right_on``` because the columns to merge on have matching labels.\n",
    "\n",
    "By merging ```sales``` and ```managers``` with a *left* merge, you can identify the missing ```manager```. Here, the columns to merge on have conflicting labels, so you must specify ```left_on``` and ```right_on```. In both cases, you're looking to figure out how to connect the fields in rows containing ```Springfield```.\n",
    "\n",
    "```pandas``` has been imported as ```pd``` and the three DataFrames ```revenue```, ```managers```, and ```sales``` have been pre-loaded. They have been printed for you to explore in the IPython Shell.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Execute a right merge using ```pd.merge()``` with ```revenue``` and ```sales``` to yield a new DataFrame ```revenue_and_sales```.\n",
    "    * Use ```how='right'``` and ```on=['city', 'state']```.\n",
    "* Print the new DataFrame ```revenue_and_sales```. This has been done for you.\n",
    "* Execute a left merge with ```sales``` and ```managers``` to yield a new DataFrame ```sales_and_managers```.\n",
    "    * Use ```how='left'```, ```left_on=['city', 'state']```, and ```right_on=['branch', 'state']```.\n",
    "* Print the new DataFrame ```sales_and_managers```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = {'city': ['Austin', 'Denver', 'Springfield', 'Mendocino'],\n",
    "       'branch_id': [10, 20, 30, 47],\n",
    "       'state': ['TX','CO','IL','CA'],\n",
    "       'revenue': [100, 83, 4, 200]}\n",
    "\n",
    "man = {'branch': ['Austin', 'Denver', 'Mendocino', 'Springfield'],\n",
    "       'branch_id': [10, 20, 47, 31],\n",
    "       'state': ['TX','CO','CA','MO'],\n",
    "       'manager': ['Charles', 'Joel', 'Brett', 'Sally']}\n",
    "\n",
    "sale = {'city': ['Mendocino', 'Denver', 'Austin', 'Springfield', 'Springfield'],\n",
    "        'state': ['CA', 'CO', 'TX', 'MO', 'IL'],\n",
    "        'units': [1, 4, 2, 5, 1]}\n",
    "\n",
    "revenue = pd.DataFrame.from_dict(rev)\n",
    "managers = pd.DataFrame.from_dict(man)\n",
    "sales = pd.DataFrame.from_dict(sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge revenue and sales: revenue_and_sales\n",
    "revenue_and_sales = pd.merge(revenue, sales, how='right', on=['city', 'state'])\n",
    "\n",
    "# Print revenue_and_sales\n",
    "revenue_and_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_and_managers = pd.merge(sales, managers, how='left', left_on=['city', 'state'], right_on=['branch', 'state'])\n",
    "\n",
    "# Print sales_and_managers\n",
    "sales_and_managers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging DataFrames with outer join\n",
    "\n",
    "This exercise picks up where the previous one left off. The DataFrames ```revenue```, ```managers```, and ```sales``` are pre-loaded into your namespace (and, of course, ```pandas``` is imported as ```pd```). Moreover, the merged DataFrames ```revenue_and_sales``` and ```sales_and_managers``` have been pre-computed exactly as you did in the previous exercise.\n",
    "\n",
    "The merged DataFrames contain enough information to construct a DataFrame with 5 rows with all known information correctly aligned and each branch listed only once. You will try to merge the merged DataFrames on all matching keys (which computes an inner join by default). You can compare the result to an outer join and also to an outer join with restricted subset of columns as keys.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Merge ```sales_and_managers``` with ```revenue_and_sales```. Store the result as ```merge_default```.\n",
    "* Print ```merge_default```. This has been done for you.\n",
    "* Merge ```sales_and_managers``` with ```revenue_and_sales``` using ```how='outer'```. Store the result as ```merge_outer```.\n",
    "* Print ```merge_outer```. This has been done for you.\n",
    "* Merge ```sales_and_managers``` with ```revenue_and_sales``` only on ```['city','state']``` using an outer join. Store the result as ```merge_outer_on``` and hit 'Submit Answer' to see what the merged DataFrames look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the first merge: merge_default\n",
    "merge_default = pd.merge(sales_and_managers, revenue_and_sales)\n",
    "\n",
    "# Print merge_default\n",
    "merge_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the second merge: merge_outer\n",
    "merge_outer = pd.merge(sales_and_managers, revenue_and_sales, how='outer')\n",
    "\n",
    "# Print merge_outer\n",
    "merge_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the third merge: merge_outer_on\n",
    "merge_outer_on = pd.merge(sales_and_managers, revenue_and_sales, on=['city', 'state'], how='outer')\n",
    "\n",
    "# Print merge_outer_on\n",
    "merge_outer_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rev, man, revenue, managers, students, midterm_results, grades, sale, sales, revenue_and_sales, sales_and_managers, merge_default, merge_outer, merger_outer_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software & hardware sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software = pd.read_csv(sales_feb_software_file, parse_dates=['Date']).sort_values('Date')\n",
    "software.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware = pd.read_csv(sales_feb_hardware_file, parse_dates=['Date']).sort_values('Date')\n",
    "hardware.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge()\n",
    "\n",
    "* attempting to merge yields an empty DataFrame because it's doing an INNER join on all columns with matching names by defaults\n",
    "    * 'Units' and 'Date' columns have no overlapping values, so the result is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software)\n",
    "sales_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge(how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software, how='outer')\n",
    "sales_merge.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting merge(how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software, how='outer').sort_values('Date')\n",
    "sales_merge.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_ordered()\n",
    "\n",
    "* the default is an OUTER join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merged = pd.merge_ordered(hardware, software)\n",
    "sales_merged.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using on & suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merged = pd.merge_ordered(hardware, software, on=['Date', 'Company'], suffixes=['_hardware', '_software'])\n",
    "sales_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_dir = Path.cwd() / 'data' / 'stocks'\n",
    "sp500_stocks = stocks_dir / 'SP500.csv'\n",
    "aapl = stocks_dir/ 'AAPL.csv'\n",
    "csco = stocks_dir/ 'CSCO.csv'\n",
    "amzn = stocks_dir/ 'AMZN.csv'\n",
    "msft = stocks_dir/ 'MSFT.csv'\n",
    "ibm = stocks_dir/ 'IBM.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df = pd.read_csv(sp500_stocks, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "aapl_df = pd.read_csv(aapl, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "csco_df = pd.read_csv(csco, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "amzn_df = pd.read_csv(amzn, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "msft_df = pd.read_csv(msft, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "ibm_df = pd.read_csv(ibm, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df.rename(columns={'Close': 'S&P'}, inplace=True)\n",
    "aapl_df.rename(columns={'Close': 'AAPL'}, inplace=True)\n",
    "csco_df.rename(columns={'Close': 'CSCO'}, inplace=True)\n",
    "amzn_df.rename(columns={'Close': 'AMZN'}, inplace=True)\n",
    "msft_df.rename(columns={'Close': 'MSFT'}, inplace=True)\n",
    "ibm_df.rename(columns={'Close': 'IBM'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.concat([sp500_df, aapl_df, csco_df, amzn_df, msft_df, ibm_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.to_csv(stocks_dir / 'stocks.csv', index=True, index_label='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = pd.read_csv(gdp_usa_file, parse_dates=['DATE'])\n",
    "gdp.sort_values(by=['DATE'], ascending=False, inplace=True)\n",
    "gdp.reset_index(inplace=True, drop=True)\n",
    "gdp.rename(columns={'VALUE': 'GDP', 'DATE': 'Date'}, inplace=True)\n",
    "gdp.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordered merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_2000_2015 = gdp[(gdp['Date'].dt.year >= 2000) & (gdp['Date'].dt.year <= 2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.reset_index(inplace=True)\n",
    "stocks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_2000_2015 = stocks[(stocks['Date'].dt.year >= 2000) & (stocks['Date'].dt.year <= 2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df = pd.merge_ordered(stocks_2000_2015, gdp_2000_2015, on='Date')\n",
    "ordered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordered merge with ffill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df = pd.merge_ordered(stocks_2000_2015, gdp_2000_2015, on='Date', fill_method='ffill')\n",
    "ordered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del software, hardware, sales_merge, sales_merged, stocks_dir, sp500_stocks, aapl\n",
    "del csco, amzn, msft, ibm, sp500_df, aapl_df, csco_df, amzn_df, msft_df, ibm_df, stocks\n",
    "del gdp, gdp_2000_2015, stocks_2000_2015, ordered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_ordered()\n",
    "\n",
    "This exercise uses pre-loaded DataFrames ```austin``` and ```houston``` that contain weather data from the cities Austin and Houston respectively. They have been printed in the IPython Shell for you to examine.\n",
    "\n",
    "Weather conditions were recorded on separate days and you need to merge these two DataFrames together such that the dates are ordered. To do this, you'll use ```pd.merge_ordered()```. After you're done, note the order of the rows before and after merging.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Perform an ordered merge on ```austin``` and ```houston``` using ```pd.merge_ordered()```. Store the result as ```tx_weather```.\n",
    "* Print ```tx_weather```. You should notice that the rows are sorted by the date but it is not possible to tell which observation came from which city.\n",
    "* Perform another ordered merge on ```austin``` and ```houston```.\n",
    "    * This time, specify the keyword arguments ```on='date'``` and ```suffixes=['_aus','_hus']``` so that the rows can be distinguished. Store the result as ```tx_weather_suff```.\n",
    "* Print ```tx_weather_suff``` to examine its contents. This has been done for you.\n",
    "* Perform a third ordered merge on ```austin``` and ```houston```.\n",
    "    * This time, in addition to the ```on``` and ```suffixes``` parameters, specify the keyword argument ```fill_method='ffill'``` to use *forward-filling* to replace ```NaN``` entries with the most recent non-null entry, and hit 'Submit Answer' to examine the contents of the merged DataFrames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin = pd.DataFrame.from_dict({'date': ['2016-01-01', '2016-02-08', '2016-01-17'], 'ratings': ['Cloudy', 'Cloudy', 'Sunny']})\n",
    "houston = pd.DataFrame.from_dict({'date': ['2016-01-04', '2016-01-01', '2016-03-01'], 'ratings': ['Rainy', 'Cloudy', 'Sunny']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the first ordered merge: tx_weather\n",
    "tx_weather = pd.merge_ordered(austin, houston)\n",
    "\n",
    "# Print tx_weather\n",
    "tx_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the second ordered merge: tx_weather_suff\n",
    "tx_weather_suff = pd.merge_ordered(austin, houston, on='date', suffixes=['_aus','_hus'])\n",
    "\n",
    "# Print tx_weather_suff\n",
    "tx_weather_suff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the third ordered merge: tx_weather_ffill\n",
    "tx_weather_ffill = pd.merge_ordered(austin, houston, on='date', suffixes=['_aus','_hus'], fill_method='ffill')\n",
    "\n",
    "# Print tx_weather_ffill\n",
    "tx_weather_ffill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del austin, houston, tx_weather, tx_weather_stuff, tx_weather_ffill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_asof()\n",
    "\n",
    "Similar to ```pd.merge_ordered()```, the ```pd.merge_asof()``` function will also merge values in order using the ```on``` column, but for each row in the left DataFrame, only rows from the right DataFrame whose ```'on'``` column values are less than the left value will be kept.\n",
    "\n",
    "This function can be used to align disparate datetime frequencies without having to first resample.\n",
    "\n",
    "Here, you'll merge monthly oil prices (US dollars) into a full automobile fuel efficiency dataset. The oil and automobile DataFrames have been pre-loaded as ```oil``` and ```auto```. The first 5 rows of each have been printed in the IPython Shell for you to explore.\n",
    "\n",
    "These datasets will align such that the first price of the year will be broadcast into the rows of the automobiles DataFrame. This is considered correct since by the start of any given year, most automobiles for that year will have already been manufactured.\n",
    "\n",
    "You'll then inspect the merged DataFrame, resample by year and compute the mean ```'Price'``` and ```'mpg'```. You should be able to see a trend in these two columns, that you can confirm by computing the Pearson correlation between resampled ```'Price'``` and ```'mpg'```.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Merge ```auto``` and ```oil``` using ```pd.merge_asof()``` with ```left_on='yr'``` and ```ight_on='Date'```. Store the result as merged.\n",
    "* Print the tail of ```merged```. This has been done for you.\n",
    "* Resample ```merged``` using ```'A'``` (annual frequency), and ```on='Date'```. Select ```[['mpg','Price']]``` and aggregate the mean. Store the result as ```yearly```.\n",
    "* Hit Submit Answer to examine the contents of ```yearly``` and ```yearly.corr()```, which shows the Pearson correlation between the resampled ```'Price'``` and ```'mpg'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil = pd.read_csv(oil_price_file, parse_dates=['Date'])\n",
    "auto = pd.read_csv(auto_fuel_file, parse_dates=['yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge auto and oil: merged\n",
    "merged = pd.merge_asof(auto, oil, left_on='yr', right_on='Date')\n",
    "\n",
    "# Print the tail of merged\n",
    "merged.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample merged: yearly\n",
    "yearly = merged.resample('A', on='Date')[['mpg','Price']].mean()\n",
    "\n",
    "# Print yearly\n",
    "yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print yearly.corr()\n",
    "yearly.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study - Summer Olympics\n",
    "\n",
    "To cement your new skills, you'll apply them by working on an in-depth study involving Olympic medal data. The analysis involves integrating your multi-DataFrame skills from this course and also skills you've gained in previous pandas courses. This is a rich dataset that will allow you to fully leverage your pandas data manipulation skills. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medals in the Summer Olympics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summer Olympic medalists 1896 to 2008 - IOC COUNTRY CODES.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_ioc_codes_file).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summer Olympic medalists 1896 to 2008 - EDITIONS.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_editions_file, sep='\\t').head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summer_1896.csv, summer_1900.csv, , summer_2008.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_all_medalists_file, sep='\\t', header=4).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: loading & merging files\n",
    "\n",
    "* pd.read_csv() (& its many options)\n",
    "* Looping over files, e.g.,\n",
    "    * [pd.read_csv(f) for f in glob('*.csv')]\n",
    "* Concatenating & appending, e.g.,\n",
    "    * pd.concat([df1, df2], axis=0)\n",
    "    * df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Olympic edition DataFrame\n",
    "\n",
    "In this chapter, you'll be using [The Guardian's Olympic medal dataset](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data).\n",
    "\n",
    "Your first task here is to prepare a DataFrame ```editions``` from a *tab-separated values* (TSV) file.\n",
    "\n",
    "Initially, ```editions``` has 26 rows (one for each Olympic edition, i.e., a year in which the Olympics was held) and 7 columns: ```'Edition'```, ```'Bronze'```, ```'Gold'```, ```'Silver'```, ```'Grand Total'```, ```'City'```, and ```'Country'```.\n",
    "\n",
    "For the analysis that follows, you won't need the overall medal counts, so you want to keep only the useful columns from ```editions```: ```'Edition'```, ```'Grand Total'```, ```City```, and ```Country```.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read ```file_path``` into a DataFrame called ```editions```. The identifier ```file_path``` has been pre-defined with the filename ```'Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv'```. You'll have to use the option ```sep='\\t'``` because the file uses tabs to delimit fields (```pd.read_csv()``` expects commas by default).\n",
    "* Select only the columns ```'Edition'```, ```'Grand Total'```, ```'City'```, and ```'Country'``` from ```editions```.\n",
    "* Print the final DataFrame ```editions``` in entirety (there are only 26 rows). This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editions = pd.read_csv(so_editions_file, sep='\\t')\n",
    "editions = editions[['Edition', 'Grand Total', 'City', 'Country']]\n",
    "editions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading IOC codes DataFrames\n",
    "\n",
    "Your task here is to prepare a DataFrame ```ioc_codes``` from a comma-separated values (CSV) file.\n",
    "\n",
    "Initially, ```ioc_codes``` has 200 rows (one for each country) and 3 columns: ```'Country'```, ```'NOC'```, & ```'ISO code'```.\n",
    "\n",
    "For the analysis that follows, you want to keep only the useful columns from ioc_codes: ```'Country'``` and ```'NOC'``` (the column ```'NOC'``` contains three-letter codes representing each country).\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read ```file_path``` into a DataFrame called ```ioc_codes```. The identifier ```file_path``` has been pre-defined with the filename ```'Summer Olympic medallists 1896 to 2008 - IOC COUNTRY CODES.csv'```.\n",
    "* Select only the columns ```'Country'``` and ```'NOC'``` from ```ioc_codes```.\n",
    "* Print the leading 5 and trailing 5 rows of the DataFrame ```ioc_codes``` (there are 200 rows in total). This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioc_codes = pd.read_csv(so_ioc_codes_file)\n",
    "ioc_codes = ioc_codes[['Country', 'NOC']]\n",
    "ioc_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building medals DataFrame\n",
    "\n",
    "Here, you'll start with the DataFrame editions from the previous exercise.\n",
    "\n",
    "You have a sequence of files summer_1896.csv, summer_1900.csv, ..., summer_2008.csv, one for each Olympic edition (year).\n",
    "\n",
    "You will build up a dictionary medals_dict with the Olympic editions (years) as keys and DataFrames as values.\n",
    "\n",
    "The dictionary is built up inside a loop over the year of each Olympic edition (from the Index of editions).\n",
    "\n",
    "Once the dictionary of DataFrames is built up, you will combine the DataFrames using pd.concat().\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Within the for loop:\n",
    "    * Create the file path. This has been done for you.\n",
    "    * Read file_path into a DataFrame. Assign the result to the year key of medals_dict.\n",
    "    * Select only the columns 'Athlete', 'NOC', and 'Medal' from medals_dict[year].\n",
    "    * Create a new column called 'Edition' in the DataFrame medals_dict[year] whose entries are all year.\n",
    "* Concatenate the dictionary of DataFrames medals_dict into a DataFame called medals. Specify the keyword argument ignore_index=True to prevent repeated integer indices.\n",
    "* Print the first and last 5 rows of medals. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Following is the code used to combine all of the editions by year\n",
    "    * the individual files are not available\n",
    "    * the combined dataset is provided\n",
    "\n",
    "```python\n",
    "for year in editions['Edition']:\n",
    "\n",
    "    # Create the file path: file_path\n",
    "    file_path = 'summer_{:d}.csv'.format(year)\n",
    "    \n",
    "    # Load file_path into a DataFrame: medals_dict[year]\n",
    "    medals_dict[year] = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns: medals_dict[year]\n",
    "    medals_dict[year] = medals_dict[year][['Athlete', 'NOC', 'Medal']]\n",
    "    \n",
    "    # Assign year to column 'Edition' of medals_dict\n",
    "    medals_dict[year]['Edition'] = year\n",
    "    \n",
    "# Concatenate medals_dict: medals\n",
    "medals = pd.concat(medals_dict, ignore_index=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medals = pd.read_csv(so_all_medalists_file, sep='\\t', header=4)\n",
    "medals = medals[['Athlete', 'NOC', 'Medal', 'Edition']]\n",
    "medals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a pivot table\n",
    "\n",
    "* Apply DataFrame pivot_table() method\n",
    "    * index: column to use as index of pivot table\n",
    "    * values: column(s) to aggregate\n",
    "    * aggfunc: function to apply for aggregation\n",
    "    * columns: categories as columns of pivot table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting medals by country/edition in a pivot table\n",
    "\n",
    "Here, you'll start with the concatenated DataFrame ```medals``` from the previous exercise.\n",
    "\n",
    "You can construct a pivot table to see the number of medals each country won in each year. The result is a new DataFrame with the Olympic edition on the Index and with 138 country ```NOC``` codes as columns. If you want a refresher on pivot tables, it may be useful to refer back to the relevant exercises in [Manipulating DataFrames with pandas](https://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/rearranging-and-reshaping-data?ex=14).\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Construct a pivot table from the DataFrame ```medals```, aggregating by ```count``` (by specifying the ```aggfunc``` parameter). Use ```'Edition'``` as the ```index```, ```'Athlete'``` for the ```values```, and ```'NOC'``` for the ```columns```.\n",
    "* Print the first & last 5 rows of ```medal_counts```. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the pivot_table: medal_counts\n",
    "medal_counts = medals.pivot_table(index='Edition', columns='NOC', values='Athlete', aggfunc='count')\n",
    "\n",
    "# Print the first & last 5 rows of medal_counts\n",
    "medal_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medal_counts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing fraction of medals per Olympic edition\n",
    "\n",
    "In this exercise, you'll start with the DataFrames editions, medals, & medal_counts from prior exercises.\n",
    "\n",
    "You can extract a Series with the total number of medals awarded in each Olympic edition.\n",
    "\n",
    "The DataFrame medal_counts can be divided row-wise by the total number of medals awarded each edition; the method .divide() performs the broadcast as you require.\n",
    "\n",
    "This gives you a normalized indication of each country's performance in each edition.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Set the index of the DataFrame editions to be 'Edition' (using the method .set_index()). Save the result as totals.\n",
    "* Extract the 'Grand Total' column from totals and assign the result back to totals.\n",
    "* Divide the DataFrame medal_counts by totals along each row. You will have to use the .divide() method with the option axis='rows'. Assign the result to fractions.\n",
    "* Print first & last 5 rows of the DataFrame fractions. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Index of editions: totals\n",
    "totals = ____\n",
    "\n",
    "# Reassign totals['Grand Total']: totals\n",
    "totals = ____\n",
    "\n",
    "# Divide medal_counts by totals: fractions\n",
    "fractions = ____\n",
    "\n",
    "# Print first & last 5 rows of fractions\n",
    "print(fractions.head())\n",
    "print(fractions.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing percentage change in fraction of medals won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building hosts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging to compute influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting influence of host country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
