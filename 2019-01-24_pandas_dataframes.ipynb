{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notebook Author: [Trenton McKinney](https://trenton3983.github.io/)\n",
    "- Course: **[DataCamp: pandas Foundations](https://learn.datacamp.com/courses/pandas-foundations)**\n",
    "- This [notebook](https://github.com/trenton3983/DataCamp/blob/master/2019-01-24_pandas_dataframes.ipynb) was created as a reproducible reference.\n",
    "- The material is from the course\n",
    "- I completed the exercises\n",
    "- If you find the content beneficial, consider a [DataCamp Subscription](https://www.datacamp.com/pricing?period=yearly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import NaN\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## pandas DataFrames\n",
    "\n",
    "### Course Description\n",
    "\n",
    "Pandas DataFrames are the most widely used in-memory representation of complex data collections within Python. Whether in finance, scientific fields, or data science, a familiarity with Pandas is essential. This course teaches you to work with real-world data sets containing both string and numeric data, often structured around time series. You will learn powerful analysis, selection, and visualization techniques in this course.\n",
    "\n",
    "### Synopsis\n",
    "\n",
    "#### Data Ingestion & Inspection\n",
    "- Data loaded into a DataFrame from a CSV file.\n",
    "- First few rows of the DataFrame displayed using `head()`.\n",
    "- Columns renamed to be more descriptive.\n",
    "- Data types of each column inspected using `dtypes`.\n",
    "- Rows with missing data identified using `isna().any()`.\n",
    "- Summary statistics generated with `describe()`.\n",
    "\n",
    "#### Exploratory Data Analysis\n",
    "- Histograms created to visualize distributions of numerical columns.\n",
    "- Box plots generated to identify outliers.\n",
    "- Scatter plots used to examine relationships between variables.\n",
    "- Grouping operations performed using `groupby()`.\n",
    "- Pivot tables created for summary statistics.\n",
    "\n",
    "#### Time Series in pandas\n",
    "- Date and time information set as the index of the DataFrame.\n",
    "- Data resampled to different frequencies (e.g., daily, monthly) using `resample()`.\n",
    "- Rolling averages calculated to smooth time series data.\n",
    "- Time series plotted to visualize trends over time.\n",
    "\n",
    "#### Case Study - Sunlight in Austin\n",
    "- Data filtered to specific time periods.\n",
    "- Specific conditions (e.g., overcast days) identified using string operations.\n",
    "- Daily maximum temperatures calculated for filtered data.\n",
    "- Cumulative distribution function (CDF) constructed for specific subsets of data.\n",
    "- Findings summarized and visualized with appropriate plots.\n",
    "\n",
    "\n",
    "### Data Files\n",
    "\n",
    "* Most data files for the exercises can be found [here](https://www.datacamp.com/courses/pandas-foundations)\n",
    "    * [1981-2010 NOAA Austin Climate Normals](https://assets.datacamp.com/production/course_1639/datasets/NOAA_QCLCD_2011_hourly_13904.txt)\n",
    "    * [July 2015 Austin airport departures (Southwest Airlines)](https://assets.datacamp.com/production/course_1639/datasets/austin_airport_departure_data_2015_july.csv)\n",
    "    * [Automobile miles per gallon](https://assets.datacamp.com/production/course_1639/datasets/auto-mpg.csv)  \n",
    "    * [Life expectancy at birth (Gapminder)](https://assets.datacamp.com/production/course_1639/datasets/life_expectancy_at_birth.csv)\n",
    "    * [Stock data (messy)](https://assets.datacamp.com/production/course_1639/datasets/messy_stock_data.tsv)\n",
    "    * [Percentage of bachelor's degrees awarded to women in the USA](https://assets.datacamp.com/production/course_1639/datasets/percent-bachelors-degrees-women-usa.csv)\n",
    "    * [Tips](https://assets.datacamp.com/production/course_1639/datasets/tips.csv)\n",
    "    * [Titanic](https://assets.datacamp.com/production/course_1639/datasets/titanic.csv)\n",
    "    * [2010 Austin weather](https://assets.datacamp.com/production/course_1639/datasets/weather_data_austin_2010.csv)\n",
    "    * [World Bank World Development Indicators](https://assets.datacamp.com/production/course_1639/datasets/world_ind_pop_data.csv)\n",
    "    * [World population](https://assets.datacamp.com/production/course_1639/datasets/world_population.csv)\n",
    "* Other data files may be found in my [DataCamp repository](https://github.com/trenton3983/DataCamp/tree/master/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Data ingestion & inspection\n",
    "\n",
    "In this chapter, you will be introduced to Panda's DataFrames. You will use Pandas to import and inspect a variety of datasets, ranging from population data obtained from The World Bank to monthly stock data obtained via Yahoo! Finance. You will also practice building DataFrames from scratch, and become familiar with Pandas' intrinsic data visualization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review pandas DataFrames\n",
    "\n",
    "* Example: DataFrame of Apple Stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL = pd.read_csv(r'DataCamp-master/11-pandas-foundations/_datasets/AAPL.csv',\n",
    "                   index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The rows are labeled by a special data structure called an Index.\n",
    "    * Indexes in Pandas are tailored lists of labels that permit fast look-up and some powerful relational operations.\n",
    "* The index labels in the AAPL DataFrame are dates in reverse chronological order.\n",
    "* Labeled rows & columns improves the clarity and intuition of many data analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(AAPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(AAPL.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(AAPL.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrames can be sliced like NumPy arrays or Python lists using colons to specify the start, end and stride of a slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of the DataFrame to the 5th row, inclusive of all columns\n",
    "AAPL.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at the 5th last row to the end of the DataFrame using a negative index\n",
    "AAPL.iloc[-5:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.Close.plot(kind='line')\n",
    "\n",
    "# Add first subplot\n",
    "plt.subplot(2, 1, 1)\n",
    "AAPL.Close.plot(kind='line')\n",
    "\n",
    "# Add title and specify axis labels\n",
    "plt.title('Close')\n",
    "plt.ylabel('Value - $')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "# Add second subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "AAPL.Volume.plot(kind='line')\n",
    "\n",
    "# Add title and specify axis labels\n",
    "plt.title('Volume')\n",
    "plt.ylabel('Number of Shares')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "\n",
    "* Assigning scalar value to column slice broadcasts value to each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.iloc[::3, -1] = np.nan  # every 3rd row of Volume is now NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note Volume now has few non-null numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = AAPL.Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lows = low.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lows[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Pandas Series, then, is a 1D labeled NumPy array and a DataFrame is a 2D labeled array whose columns as Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting your data\n",
    "\n",
    "You can use the DataFrame methods `.head()` and `.tail()` to view the first few and last few rows of a DataFrame. In this exercise, we have imported pandas as `pd` and loaded population data from 1960 to 2014 as a DataFrame `df`. This dataset was obtained from the World Bank.\n",
    "\n",
    "Your job is to use `df.head()` and `df.tail()` to verify that the first and last rows match a file on disk. In later exercises, you will see how to extract values from DataFrames with indexing, but for now, manually copy/paste or type values into assignment statements where needed. Select the correct answer for the first and last values in the `'Year'` and `'Total Population'` columns.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "Possible Answers\n",
    "* First: 1980, 26183676.0; Last: 2000, 35.\n",
    "* First: 1960, 92495902.0; Last: 2014, 15245855.0.\n",
    "* First: 40.472, 2001; Last: 44.5, 1880.\n",
    "* First: CSS, 104170.0; Last: USA, 95.203."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_df = pd.read_csv(r'DataCamp-master/11-pandas-foundations/_datasets/world_ind_pop_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame data types\n",
    "\n",
    "Pandas is aware of the data types in the columns of your DataFrame. It is also aware of null and `NaN` ('Not-a-Number') types which often indicate missing data. In this exercise, we have imported pandas as `pd` and read in the world population data which contains some `NaN` values, a value often used as a place-holder for missing or otherwise invalid data entries. Your job is to use `df.info()` to determine information about the total count of `non-null` entries and infer the total count of `'null'` entries, which likely indicates missing data. Select the best description of this data set from the following:\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "Possible Answers\n",
    "* The data is all of type float64 and none of it is missing.\n",
    "* The data is of mixed type, and 9914 of it is missing.\n",
    "* The data is of mixed type, and 3460 float64s are missing.\n",
    "* The data is all of type float64, and 3460 float64s are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 13374 entries, 0 to 13373\n",
    "Data columns (total 5 columns):\n",
    "CountryName                      13374 non-null object\n",
    "CountryCode                      13374 non-null object\n",
    "Year                             13374 non-null int64\n",
    "Total Population                 9914 non-null float64\n",
    "Urban population (% of total)    13374 non-null float64\n",
    "dtypes: float64(2), int64(1), object(2)\n",
    "memory usage: 522.5+ KB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy and pandas working together\n",
    "Pandas depends upon and interoperates with NumPy, the Python library for fast numeric array computations. For example, you can use the DataFrame attribute `.values` to represent a DataFrame `df` as a NumPy array. You can also pass pandas data structures to NumPy methods. In this exercise, we have imported pandas as `pd` and loaded world population data every 10 years since 1960 into the DataFrame `df`. This dataset was derived from the one used in the previous exercise.\n",
    "\n",
    "Your job is to extract the values and store them in an array using the attribute `.values`. You'll then use those values as input into the NumPy `np.log10()` method to compute the base 10 logarithm of the population values. Finally, you will pass the entire pandas DataFrame into the same NumPy `np.log10()` method and compare the results.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Import `numpy` using the standard alias `np`.\n",
    "* Assign the numerical values in the DataFrame `df` to an array `np_vals` using the attribute `values`.\n",
    "* Pass `np_vals` into the NumPy method `log10()` and store the results in `np_vals_log10`.\n",
    "* Pass the entire `df` DataFrame into the NumPy method `log10()` and store the results in `df_log10`.\n",
    "* Inspect the output of the `print()` code to see the `type()` of the variables that you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pd.read_csv(r'DataCamp-master/11-pandas-foundations/_datasets/world_population.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of DataFrame values: np_vals\n",
    "np_vals = pop_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new array of base 10 logarithm values: np_vals_log10\n",
    "np_vals_log10 = np.log10(np_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_vals_log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of new DataFrame by passing df to np.log10(): df_log10\n",
    "pop_df_log10 = np.log10(pop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df_log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print original and new data containers\n",
    "[print(x, 'has type', type(eval(x))) for x in ['np_vals', 'np_vals_log10', 'pop_df', 'pop_df_log10']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***As a data scientist, you'll frequently interact with NumPy arrays, pandas Series, and pandas DataFrames, and you'll leverage a variety of NumPy and pandas methods to perform your desired computations. Understanding how NumPy and pandas work together will prove to be very useful.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building DataFrames from Scratch\n",
    "\n",
    "* DataFrames read in from CSV\n",
    "```python\n",
    "pd.read_csv()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrames from dict (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'weekday': ['Sun', 'Sun', 'Mon', 'Mon'],\n",
    "        'city': ['Austin', 'Dallas', 'Austin', 'Dallas'],\n",
    "        'visitors': [139, 237, 326, 456],\n",
    "        'signups': [7, 12, 3, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrames from dict (2)\n",
    "    * lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Austin', 'Dallas', 'Austin', 'Dallas']\n",
    "signups = [7, 12, 3, 5]\n",
    "weekdays = ['Sun', 'Sun', 'Mon', 'Mon']\n",
    "visitors = [139, 237, 326, 456]\n",
    "\n",
    "list_labels = ['city', 'signups', 'visitors', 'weekday']\n",
    "list_cols = [cities, signups, visitors, weekdays]  # list of lists\n",
    "\n",
    "zipped = list(zip(list_labels, list_cols))  # tuples\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrames from dict (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = dict(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "\n",
    "* Saves time by generating long lists, arrays or columns without loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['fees'] = 0  # Broadcasts value to entire column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting with a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = [59.0, 65.2, 62.9, 65.4, 63.7, 65.7, 64.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'height': heights, 'sex': 'M'}  # M is broadcast to the entire column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index and columns\n",
    "\n",
    "* We can assign list of strings to the attributes columns and index as long as they are of suitable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns = ['height (in)', 'sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.index = ['A', 'B', 'C', 'D', 'E', 'F', 'G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip lists to build a DataFrame\n",
    "\n",
    "In this exercise, you're going to make a pandas DataFrame of the top three countries to win gold medals since 1896 by first building a dictionary. `list_keys` contains the column names `'Country'` and `'Total'`. `list_values` contains the full names of each country and the number of gold medals awarded. The values have been taken from [Wikipedia](https://en.wikipedia.org/wiki/All-time_Olympic_Games_medal_table).\n",
    "\n",
    "Your job is to use these lists to construct a list of tuples, use the list of tuples to construct a dictionary, and then use that dictionary to construct a DataFrame. In doing so, you'll make use of the `list()`, `zip()`, `dict()` and `pd.DataFrame()` functions. Pandas has already been imported as pd.\n",
    "\n",
    "Note: The [zip()](https://docs.python.org/3/library/functions.html#zip) function in Python 3 and above returns a special zip object, which is essentially a generator. To convert this `zip` object into a list, you'll need to use `list()`. You can learn more about the `zip()` function as well as generators in [Python Data Science Toolbox (Part 2)](https://www.datacamp.com/courses/python-data-science-toolbox-part-2).\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Zip the 2 lists `list_keys` and `list_values` together into one list of (key, value) tuples. Be sure to convert the `zip` object into a list, and store the result in `zipped`.\n",
    "* Inspect the contents of `zipped` using `print()`. This has been done for you.\n",
    "* Construct a dictionary using `zipped`. Store the result as `data`.\n",
    "* Construct a DataFrame using the dictionary. Store the result as `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keys = ['Country', 'Total']\n",
    "list_values = [['United States', 'Soviet Union', 'United Kingdom'], [1118, 473, 273]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = list(zip(list_keys, list_values))  # tuples\n",
    "zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labeling your data\n",
    "\n",
    "You can use the DataFrame attribute `df.columns` to view and assign new string labels to columns in a pandas DataFrame.\n",
    "\n",
    "In this exercise, we have imported pandas as `pd` and defined a DataFrame `df` containing top Billboard hits from the 1980s (from [Wikipedia](https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_number-one_singles_of_the_1980s#1980)). Each row has the year, artist, song name and the number of weeks at the top. However, this DataFrame has the column labels `a, b, c, d`. Your job is to use the `df.columns` attribute to re-assign descriptive column labels.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Create a list of new column labels with `'year'`, `'artist'`, `'song'`, `'chart weeks'`, and assign it to `list_labels`.\n",
    "* Assign your list of labels to `df.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard_values = np.array([['1980', 'Blondie', 'Call Me', '6'],\n",
    "                             ['1981', 'Chistorpher Cross', 'Arthurs Theme', '3'],\n",
    "                             ['1982', 'Joan Jett', 'I Love Rock and Roll', '7']]).transpose()\n",
    "billboard_keys = ['a', 'b', 'c', 'd']\n",
    "\n",
    "billboard_zipped = list(zip(billboard_keys, billboard_values))\n",
    "billboard_zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard_dict = dict(billboard_zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard = pd.DataFrame.from_dict(billboard_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of labels: list_labels\n",
    "list_labels = ['year', 'artist', 'song', 'chart weeks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the list of labels to the columns attribute: df.columns\n",
    "billboard.columns = list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building DataFrames with broadcasting\n",
    "\n",
    "You can implicitly use 'broadcasting', a feature of NumPy, when creating pandas DataFrames. In this exercise, you're going to create a DataFrame of cities in Pennsylvania that contains the city name in one column and the state name in the second. We have imported the names of 15 cities as the list `cities`.\n",
    "\n",
    "Your job is to construct a DataFrame from the list of cities and the string `'PA'`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Make a string object with the value 'PA' and assign it to state.\n",
    "* Construct a dictionary with 2 key:value pairs: 'state':state and 'city':cities.\n",
    "* Construct a pandas DataFrame from the dictionary you created and assign it to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Manheim', 'Preston park', 'Biglerville',\n",
    "          'Indiana', 'Curwensville', 'Crown',\n",
    "          'Harveys lake', 'Mineral springs', 'Cassville',\\\n",
    "          'Hannastown', 'Saltsburg', 'Tunkhannock',\n",
    "          'Pittsburgh', 'Lemasters', 'Great bend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a string with the value 'PA': state\n",
    "state = 'PA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dictionary: data\n",
    "data = {'state': state, 'city': cities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a DataFrame from dictionary data: df\n",
    "pa_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame\n",
    "print(pa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing & Exporting Data\n",
    "\n",
    "* Dataset: Sunspot observations collected from SILSO\n",
    "\n",
    "```python\n",
    "Format: Comma Separated values (adapted for import in spreadsheets)\n",
    "The separator is the semicolon ';'.\n",
    "\n",
    "Contents:\n",
    "Column 1-3: Gregorian calendar date\n",
    "- Year\n",
    "- Month\n",
    "- Day\n",
    "Column 4: Date in fraction of year.\n",
    "Column 5: Daily total sunspot number. A value of -1 indicates that no number is available for that day (missing value).\n",
    "Column 6: Daily standard deviation of the input sunspot numbers from individual stations.\n",
    "Column 7: Number of observations used to compute the daily value.\n",
    "Column 8: Definitive/provisional indicator. '1' indicates that the value is definitive. '0' indicates that the value is still provisional.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'data/silso_sunspot_data_1818-2019.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots = pd.read_csv(filepath, sep=';')\n",
    "sunspots.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems\n",
    "\n",
    "* CSV file has no column headers\n",
    "    * Columns 0-2: Gregorian date (year, month, day)\n",
    "    * Column 3: Date as fraction as year\n",
    "    * Column 4: Daily total sunspot number\n",
    "    * Column 5: Definitive / provisional indicator (1 OR 0)\n",
    "* Missing values in column 4: indicated by -1\n",
    "* Date representation inconvenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots = pd.read_csv(filepath, sep=';', header=None)\n",
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using names keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['year', 'month', 'day', 'dec_date',\n",
    "             'tot_sunspots', 'daily_std', 'observations', 'definite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots = pd.read_csv(filepath, sep=';', header=None, names=col_names)\n",
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using na_values keyword (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots = pd.read_csv(filepath, sep=';',\n",
    "                       header=None,\n",
    "                       names=col_names,\n",
    "                       na_values='-1')\n",
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using na_values keyword (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots = pd.read_csv(filepath, sep=';',\n",
    "                       header=None,\n",
    "                       names=col_names,\n",
    "                       na_values='  -1')\n",
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using na_values keyword (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots = pd.read_csv(filepath, sep=';',\n",
    "                       header=None,\n",
    "                       names=col_names,\n",
    "                       na_values={'tot_sunspots':['  -1'],\n",
    "                                  'daily_std':['-1']})\n",
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using parse_dates keyword\n",
    "\n",
    "- FutureWarning: Support for nested sequences for `parse_dates` in `pd.read_csv` is deprecated. Combine the desired columns with `pd.to_datetime` after parsing instead.\n",
    "\n",
    "```python\n",
    "sunspots = pd.read_csv(filepath, sep=';',\n",
    "                       header=None,\n",
    "                       names=col_names,\n",
    "                       na_values={'tot_sunspots':['  -1'],\n",
    "                                  'daily_std':['-1']},\n",
    "                       parse_dates=[[0, 1, 2]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots = pd.read_csv(filepath, sep=';',\n",
    "                       header=None,\n",
    "                       names=col_names,\n",
    "                       na_values={'tot_sunspots':['  -1'],\n",
    "                                  'daily_std':['-1']})\n",
    "sunspots['year_month_day'] = pd.to_datetime(sunspots['year'].astype(str) + '-' + sunspots['month'].astype(str) + '-' + sunspots['day'].astype(str))\n",
    "sunspots.drop(['year', 'month', 'day'], axis=1, inplace=True)\n",
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using dates as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots.index = sunspots['year_month_day']\n",
    "sunspots.index.name = 'date'\n",
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspots.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trimming redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['tot_sunspots', 'daily_std', 'observations', 'definite']\n",
    "sunspots = sunspots[cols]\n",
    "sunspots.iloc[10:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing files\n",
    "\n",
    "```python\n",
    "out_csv = 'sunspots.csv'\n",
    "sunspots.to_csv(out_csv)\n",
    "out_tsv = 'sunspots.tsv'\n",
    "sunspots.to_csv(out_tsv, sep='\\t')\n",
    "out_xlsx = 'sunspots.xlsx'\n",
    "sunspots.to_excel(out_xlsx)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a flat file\n",
    "\n",
    "In previous exercises, we have preloaded the data for you using the pandas function `read_csv()`. Now, it's your turn! Your job is to read the World Bank population data you saw earlier into a DataFrame using `read_csv()`. The file is available in the variable `data_file`.\n",
    "\n",
    "The next step is to reread the same file, but simultaneously rename the columns using the `names` keyword input parameter, set equal to a list of new column labels. You will also need to set `header=0` to rename the column labels.\n",
    "\n",
    "Finish up by inspecting the result with `df.head()` and `df.info()` in the IPython Shell (changing `df` to the name of your DataFrame variable).\n",
    "\n",
    "`pandas` has already been imported and is available in the workspace as `pd`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Use ***pd.read_csv()*** with the string ***data_file*** to read the CSV file into a DataFrame and assign it to ***df1***.\n",
    "* Create a list of new column labels - ***'year'***, ***'population'*** - and assign it to the variable ***new_labels***.\n",
    "* Reread the same file, again using ***pd.read_csv()***, but this time, add the keyword arguments ***header=0*** and ***names=new_labels***. Assign the resulting DataFrame to ***df2***.\n",
    "* Print both the ***df1*** and ***df2*** DataFrames to see the change in column names. This has already been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/world_population.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file: df1\n",
    "df1 = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the new column labels: new_labels\n",
    "new_labels = ['year', 'population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file, specifying the header and names parameters: df2\n",
    "df2 = pd.read_csv(data_file, header=0, names=new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print both the DataFrames\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delimiters, headers, and extensions\n",
    "\n",
    "Not all data files are clean and tidy. Pandas provides methods for reading those not-so-perfect data files that you encounter far too often.\n",
    "\n",
    "In this exercise, you have monthly stock data for four companies downloaded from [Yahoo Finance](https://finance.yahoo.com/). The data is stored as one row for each company and each column is the end-of-month closing price. The file name is given to you in the variable `file_messy`.\n",
    "\n",
    "In addition, this file has three aspects that may cause trouble for lesser tools: multiple header lines, comment records (rows) interleaved throughout the data rows, and space delimiters instead of commas.\n",
    "\n",
    "Your job is to use pandas to read the data from this problematic `file_messy` using non-default input options with `read_csv()` so as to tidy up the mess at read time. Then, write the cleaned up data to a CSV file with the variable `file_clean` that has been prepared for you, as you might do in a real data workflow.\n",
    "\n",
    "You can learn about the option input parameters needed by using `help()` on the pandas function `pd.read_csv()`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Use ***pd.read_csv()*** without using any keyword arguments to read ***file_messy*** into a pandas DataFrame ***df1***.\n",
    "* Use ***.head()*** to print the first 5 rows of ***df1*** and see how messy it is. Do this in the IPython Shell first so you can see how modifying ***read_csv()*** can clean up this mess.\n",
    "* Using the keyword arguments ***delimiter=' '***, ***header=3*** and ***comment='#'***, use ***pd.read_csv()*** again to read ***file_messy*** into a new DataFrame ***df2***.\n",
    "* Print the output of ***df2.head(***) to verify the file was read correctly.\n",
    "* Use the DataFrame method ***.to_csv()*** to save the DataFrame ***df2*** to the variable ***file_clean***. Be sure to specify ***index=False***.\n",
    "* Use the DataFrame method ***.to_excel()*** to save the DataFrame ***df2*** to the file ***'file_clean.xlsx'***. Again, remember to specify ***index=False***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw file as-is: df1\n",
    "file_messy = 'DataCamp-master/11-pandas-foundations/_datasets/messy_stock_data.tsv'\n",
    "df1 = pd.read_csv(file_messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output of df1.head()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file with the correct parameters: df2\n",
    "df2 = pd.read_csv(file_messy, delimiter=' ', header=3, comment='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output of df2.head()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save files\n",
    "\n",
    "```python\n",
    "# Save the cleaned up DataFrame to a CSV file without the index\n",
    "df2.to_csv(file_clean, index=False)\n",
    "# Save the cleaned up DataFrame to an excel file without the index\n",
    "df2.to_excel('file_clean.xlsx', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "aapl = pd.read_csv(r'DataCamp-master/11-pandas-foundations/_datasets/AAPL.csv',\n",
    "                   names=cols,\n",
    "                   index_col='date',\n",
    "                   parse_dates=True,\n",
    "                   header=0,\n",
    "                   na_values='null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting arrays (matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_arr = aapl['close'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(close_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(close_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Series (matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_series = aapl['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(close_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(close_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Series (pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting DataFrames (pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting DataFrames (matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aapl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.plot()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl['open'].plot(color='b', style='.-', legend=True)\n",
    "aapl['close'].plot(color='r', style='.', legend=True)\n",
    "plt.axis(('2000', '2001', 0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.loc['2001':'2004', ['open', 'close', 'high', 'low']].plot()\n",
    "\n",
    "plt.savefig('aapl.png')\n",
    "plt.savefig('aapl.jpg')\n",
    "plt.savefig('aapl.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting series using pandas\n",
    "\n",
    "Data visualization is often a very effective first step in gaining a rough understanding of a data set to be analyzed. Pandas provides data visualization by both depending upon and interoperating with the matplotlib library. You will now explore some of the basic plotting mechanics with pandas as well as related matplotlib options. We have pre-loaded a pandas DataFrame `df` which contains the data you need. Your job is to use the DataFrame method `df.plot()` to visualize the data, and then explore the optional matplotlib input parameters that this `.plot()` method accepts.\n",
    "\n",
    "The pandas `.plot()` method makes calls to matplotlib to construct the plots. This means that you can use the skills you've learned in previous visualization courses to customize the plot. In this exercise, you'll add a custom title and axis labels to the figure.\n",
    "\n",
    "Before plotting, inspect the DataFrame in the IPython Shell using `df.head()`. Also, use `type(df)` and note that it is a single column DataFrame.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Create the plot with the DataFrame method ***df.plot()***. Specify a ***color*** of ***'red'***.\n",
    "    * Note: ***c*** and ***color*** are interchangeable as parameters here, but we ask you to be explicit and specify ***color***.\n",
    "* Use ***plt.title()*** to give the plot a title of ***'Temperature in Austin'***.\n",
    "* Use ***plt.xlabel()*** to give the plot an x-axis label of ***'Hours since midnight August 1, 2010'***.\n",
    "* Use ***plt.ylabel()*** to give the plot a y-axis label of ***'Temperature (degrees F)'***.\n",
    "* Finally, display the plot using ***plt.show()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/weather_data_austin_2010.csv'\n",
    "df = pd.read_csv(data_file, usecols=['Temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot with color='red'\n",
    "df.plot(color='r')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Temperature in Austin')\n",
    "\n",
    "# Specify the x-axis label\n",
    "plt.xlabel('Hours since midnight August 1, 2010')\n",
    "\n",
    "# Specify the y-axis label\n",
    "plt.ylabel('Temperature (degrees F)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting DataFrames\n",
    "\n",
    "Comparing data from several columns can be very illuminating. Pandas makes doing so easy with multi-column DataFrames. By default, calling `df.plot()` will cause pandas to over-plot all column data, with each column as a single line. In this exercise, we have pre-loaded three columns of data from a weather data set - temperature, dew point, and pressure - but the problem is that pressure has different units of measure. The pressure data, measured in Atmospheres, has a different vertical scaling than that of the other two data columns, which are both measured in degrees Fahrenheit.\n",
    "\n",
    "Your job is to plot all columns as a multi-line plot, to see the nature of vertical scaling problem. Then, use a list of column names passed into the DataFrame `df[column_list]` to limit plotting to just one column, and then just 2 columns of data. When you are finished, you will have created 4 plots. You can cycle through them by clicking on the 'Previous Plot' and 'Next Plot' buttons.\n",
    "\n",
    "As in the previous exercise, inspect the DataFrame `df` in the IPython Shell using the `.head()` and `.info()` methods.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Plot all columns together on one figure by calling ***df.plot()***, and noting the vertical scaling problem.\n",
    "* Plot all columns as subplots. To do so, you need to specify ***subplots=True*** inside ***.plot()***.\n",
    "* Plot a single column of dew point data. To do this, define a column list containing a single column name ***'Dew Point (deg F)'***, and call ***df[column_list1].plot()***.\n",
    "* Plot two columns of data, ***'Temperature (deg F)'*** and ***'Dew Point (deg F)'***. To do this, define a list containing those column names and pass it into ***df[]***, as ***df[column_list2].plot()***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/weather_data_austin_2010.csv'\n",
    "df = pd.read_csv(data_file, parse_dates=[3], index_col='Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all columns (default)\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all columns as subplots\n",
    "df.plot(subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot just the Dew Point data\n",
    "column_list1 = ['DewPoint']\n",
    "df[column_list1].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Dew Point and Temperature data, but not the Pressure data\n",
    "column_list2 = ['Temperature','DewPoint']\n",
    "df[column_list2].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Having learned how to ingest and inspect your data, you will next explore it visually as well as quantitatively. This process, known as exploratory data analysis (EDA), is a crucial component of any data science project. Pandas has powerful methods that help with statistical and visual EDA. In this chapter, you will learn how and when to apply these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Iris Dataset\n",
    "\n",
    "* Famous dataset in pattern recognition\n",
    "* 150 observations, 4 features each\n",
    "    * Sepal length\n",
    "    * Sepal width\n",
    "    * Petal length\n",
    "    * Petal width\n",
    "* 3 species:\n",
    "    * setosa\n",
    "    * versicolor\n",
    "    * virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/iris.csv'\n",
    "iris = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(x='sepal length (cm)', y='sepal width (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(x='sepal length (cm)', y='sepal width (cm)',\n",
    "          kind='scatter')\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(y='sepal length (cm)',\n",
    "          kind='box')\n",
    "plt.ylabel('sepal length (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(y='sepal length (cm)',\n",
    "          kind='hist')\n",
    "plt.xlabel('sepal length (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Options\n",
    "\n",
    "* **bins** (integer): number of intervals or bins\n",
    "* **range** (tuple): extrema of bins (minimum, maximum)\n",
    "* **density** (boolean): whether to normalized to one - formerly this was **normed**\n",
    "* **cumulative** (boolean): computer Cumulative Distributions Function (CDF)\n",
    "* ... more matplotlib customizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(y='sepal length (cm)',\n",
    "          kind='hist',\n",
    "          bins=30,\n",
    "          range=(4, 8),\n",
    "          density=True)\n",
    "plt.xlabel('sepal length (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(y='sepal length (cm)',\n",
    "          kind='hist',\n",
    "          bins=30,\n",
    "          range=(4, 8),\n",
    "          density=True,\n",
    "          cumulative=True)\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.title('Cumulative Distribution Function (CDF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word of Warning\n",
    "\n",
    "* Three different DataFrame plot idioms\n",
    "    * iris.plot(kind='hist')\n",
    "    * iris.plt.hist()\n",
    "    * iris.hist()\n",
    "* Syntax / Results differ!\n",
    "* Pandas API still evolving: chech the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas line plots\n",
    "\n",
    "In the previous chapter, you saw that the `.plot()` method will place the Index values on the x-axis by default. In this exercise, you'll practice making line plots with specific columns on the x and y axes.\n",
    "\n",
    "You will work with a dataset consisting of monthly stock prices in 2015 for AAPL, GOOG, and IBM. The stock prices were obtained from [Yahoo Finance](https://finance.yahoo.com/). Your job is to plot the 'Month' column on the x-axis and the AAPL and IBM prices on the y-axis using a list of column names.\n",
    "\n",
    "All necessary modules have been imported for you, and the DataFrame is available in the workspace as df. Explore it using methods such as `.head()`, `.info()`, and `.describe()` to see the column names.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Create a list of y-axis column names called ***y_columns*** consisting of ***'AAPL'*** and ***'IBM'***.\n",
    "* Generate a line plot with ***x='Month'*** and ***y=y_columns*** as inputs.\n",
    "* Give the plot a title of ***'Monthly stock prices'***.\n",
    "* Specify the y-axis label.\n",
    "* Display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [['Jan', 117.160004, 534.5224450000002, 153.309998],\n",
    "          ['Feb', 128.46000700000002, 558.402511, 161.940002],\n",
    "          ['Mar', 124.43, 548.002468, 160.5],\n",
    "          ['Apr', 125.150002, 537.340027, 171.28999299999995],\n",
    "          ['May', 130.279999, 532.1099849999998, 169.649994],\n",
    "          ['Jun', 125.43, 520.51001, 162.660004],\n",
    "          ['Jul', 121.300003, 625.6099849999998, 161.990005],\n",
    "          ['Aug', 112.760002, 618.25, 147.889999],\n",
    "          ['Sep', 110.300003, 608.419983, 144.970001],\n",
    "          ['Oct', 119.5, 710.8099980000002, 140.080002],\n",
    "          ['Nov', 118.300003, 742.599976, 139.419998],\n",
    "          ['Dec', 105.260002, 758.880005, 137.619995]]\n",
    "\n",
    "values = np.array(values).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Month', 'AAPL', 'GOOG', 'IBM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zipped = list(zip(cols, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict(data_zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = dict(zip(data_dict.keys(), ['str', 'float64', 'float64', 'float64']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_dict).astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of y-axis column names: y_columns\n",
    "y_columns = ['AAPL', 'IBM']\n",
    "\n",
    "# Generate a line plot\n",
    "df.plot(x='Month', y=y_columns)\n",
    "\n",
    "# Add the title\n",
    "plt.title('Monthly stock prices')\n",
    "\n",
    "# Add the y-axis label\n",
    "plt.ylabel('Price ($US)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas scatter plots\n",
    "\n",
    "Pandas scatter plots are generated using the `kind='scatter'` keyword argument. Scatter plots require that the x and y columns be chosen by specifying the `x` and `y` parameters inside `.plot()`. Scatter plots also take an `s` keyword argument to provide the radius of each circle to plot in pixels.\n",
    "\n",
    "In this exercise, you're going to plot fuel efficiency (miles-per-gallon) versus horse-power for 392 automobiles manufactured from 1970 to 1982 from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Auto+MPG).\n",
    "\n",
    "The size of each circle is provided as a NumPy array called `sizes`. This array contains the normalized `'weight'` of each automobile in the dataset.\n",
    "\n",
    "All necessary modules have been imported and the DataFrame is available in the workspace as df.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Generate a scatter plot with ***'hp'*** on the x-axis and ***'mpg'*** on the y-axis. Specify ***s=sizes***.\n",
    "* Add a title to the plot.\n",
    "* Specify the x-axis and y-axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/auto-mpg.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.array([ 51.12044694,  56.78387977,  49.15557238,  49.06977358,\n",
    "        49.52823321,  78.4595872 ,  78.93021696,  77.41479205,\n",
    "        81.52541106,  61.71459825,  52.85646225,  54.23007578,\n",
    "        58.89427963,  39.65137852,  23.42587473,  33.41639502,\n",
    "        32.03903011,  27.8650165 ,  18.88972581,  14.0196956 ,\n",
    "        29.72619722,  24.58549713,  23.48516821,  20.77938954,\n",
    "        29.19459189,  88.67676838,  79.72987328,  79.94866084,\n",
    "        93.23005042,  18.88972581,  21.34122243,  20.6679223 ,\n",
    "        28.88670381,  49.24144612,  46.14174741,  45.39631334,\n",
    "        45.01218186,  73.76057586,  82.96880195,  71.84547684,\n",
    "        69.85320595, 102.22421043,  93.78252358, 110.        ,\n",
    "        36.52889673,  24.14234281,  44.84805372,  41.02504618,\n",
    "        20.51976563,  18.765772  ,  17.9095202 ,  17.75442285,\n",
    "        13.08832041,  10.83266174,  14.00441945,  15.91328975,\n",
    "        21.60597587,  18.8188451 ,  21.15311208,  24.14234281,\n",
    "        20.63083317,  76.05635059,  80.05816704,  71.18975117,\n",
    "        70.98330444,  56.13992036,  89.36985382,  84.38736544,\n",
    "        82.6716892 ,  81.4149056 ,  22.60363518,  63.06844313,\n",
    "        69.92143863,  76.76982089,  69.2066568 ,  35.81711267,\n",
    "        26.25184749,  36.94940537,  19.95069229,  23.88237331,\n",
    "        21.79608472,  26.1474042 ,  19.49759118,  18.36136808,\n",
    "        69.98970461,  56.13992036,  66.21810474,  68.02351436,\n",
    "        59.39644014, 102.10046481,  82.96880195,  79.25686195,\n",
    "        74.74521151,  93.34830013, 102.05923292,  60.7883734 ,\n",
    "        40.55589449,  44.7388015 ,  36.11079464,  37.9986264 ,\n",
    "        35.11233175,  15.83199594, 103.96451839, 100.21241654,\n",
    "        90.18186347,  84.27493641,  32.38645967,  21.62494928,\n",
    "        24.00218436,  23.56434276,  18.78345471,  22.21725537,\n",
    "        25.44271071,  21.36007926,  69.37650986,  76.19877818,\n",
    "        14.51292942,  19.38962134,  27.75740889,  34.24717407,\n",
    "        48.10262495,  29.459795  ,  32.80584831,  55.89556844,\n",
    "        40.06360581,  35.03982309,  46.33599903,  15.83199594,\n",
    "        25.01226779,  14.03498009,  26.90404245,  59.52231336,\n",
    "        54.92349014,  54.35035315,  71.39649768,  91.93424995,\n",
    "        82.70879915,  89.56285636,  75.45251972,  20.50128352,\n",
    "        16.04379287,  22.02531454,  11.32159874,  16.70430249,\n",
    "        18.80114574,  18.50153068,  21.00322336,  25.79385418,\n",
    "        23.80266582,  16.65430211,  44.35746794,  49.815853  ,\n",
    "        49.04119063,  41.52318884,  90.72524338,  82.07906251,\n",
    "        84.23747672,  90.29816462,  63.55551901,  63.23059357,\n",
    "        57.92740995,  59.64831981,  38.45278922,  43.19643409,\n",
    "        41.81296121,  19.62393488,  28.99647648,  35.35456858,\n",
    "        27.97283229,  30.39744886,  20.57526193,  26.96758278,\n",
    "        37.07354237,  15.62160631,  42.92863291,  30.21771564,\n",
    "        36.40567571,  36.11079464,  29.70395123,  13.41514444,\n",
    "        25.27829944,  20.51976563,  27.54281821,  21.17188565,\n",
    "        20.18836167,  73.97101962,  73.09614831,  65.35749368,\n",
    "        73.97101962,  43.51889468,  46.80945169,  37.77255674,\n",
    "        39.6256851 ,  17.24230306,  19.49759118,  15.62160631,\n",
    "        13.41514444,  55.49963323,  53.18333207,  55.31736854,\n",
    "        42.44868923,  13.86730874,  16.48817545,  19.33574884,\n",
    "        27.3931002 ,  41.31307817,  64.63368105,  44.52069676,\n",
    "        35.74387954,  60.75655952,  79.87569835,  68.46177648,\n",
    "        62.35745431,  58.70651902,  17.41217694,  19.33574884,\n",
    "        13.86730874,  22.02531454,  15.75091031,  62.68013142,\n",
    "        68.63071356,  71.36201911,  76.80558184,  51.58836621,\n",
    "        48.84134317,  54.86301837,  51.73502816,  74.14661842,\n",
    "        72.22648148,  77.88228247,  78.24284811,  15.67003285,\n",
    "        31.25845963,  21.36007926,  31.60164234,  17.51450098,\n",
    "        17.92679488,  16.40542438,  19.96892459,  32.99310928,\n",
    "        28.14577056,  30.80379718,  16.40542438,  13.48998471,\n",
    "        16.40542438,  17.84050478,  13.48998471,  47.1451025 ,\n",
    "        58.08281541,  53.06435374,  52.02897659,  41.44433489,\n",
    "        36.60292926,  30.80379718,  48.98404972,  42.90189859,\n",
    "        47.56635225,  39.24128299,  54.56115914,  48.41447259,\n",
    "        48.84134317,  49.41341845,  42.76835191,  69.30854366,\n",
    "        19.33574884,  27.28640858,  22.02531454,  20.70504474,\n",
    "        26.33555201,  31.37264569,  33.93740821,  24.08222494,\n",
    "        33.34566004,  41.05118927,  32.52595611,  48.41447259,\n",
    "        16.48817545,  18.97851406,  43.84255439,  37.22278157,\n",
    "        34.77459916,  44.38465193,  47.00510227,  61.39441929,\n",
    "        57.77221268,  65.12675249,  61.07507305,  79.14790534,\n",
    "        68.42801405,  54.10993164,  64.63368105,  15.42864956,\n",
    "        16.24054679,  15.26876826,  29.68171358,  51.88189829,\n",
    "        63.32798377,  42.36896092,  48.6988448 ,  20.15170555,\n",
    "        19.24612787,  16.98905358,  18.88972581,  29.68171358,\n",
    "        28.03762169,  30.35246559,  27.20120517,  19.13885751,\n",
    "        16.12562794,  18.71277385,  16.9722369 ,  29.85984799,\n",
    "        34.29495526,  37.54716158,  47.59450219,  19.93246832,\n",
    "        30.60028577,  26.90404245,  24.66650366,  21.36007926,\n",
    "        18.5366546 ,  32.64243213,  18.5366546 ,  18.09999962,\n",
    "        22.70075058,  36.23351603,  43.97776651,  14.24983724,\n",
    "        19.15671509,  14.17291518,  35.25757392,  24.38356372,\n",
    "        26.02234705,  21.83420642,  25.81458463,  28.90864169,\n",
    "        28.58044785,  30.91715052,  23.6833544 ,  12.82391671,\n",
    "        14.63757021,  12.89709155,  17.75442285,  16.24054679,\n",
    "        17.49742615,  16.40542438,  20.42743834,  17.41217694,\n",
    "        23.58415722,  19.96892459,  20.33531923,  22.99334585,\n",
    "        28.47146626,  28.90864169,  43.43816712,  41.57579979,\n",
    "        35.01567018,  35.74387954,  48.5565546 ,  57.77221268,\n",
    "        38.98605581,  49.98882458,  28.25412762,  29.01845599,\n",
    "        23.88237331,  27.60710798,  26.54539622,  31.14448175,\n",
    "        34.17556473,  16.3228815 ,  17.0732619 ,  16.15842026,\n",
    "        18.80114574,  18.80114574,  19.42557798,  20.2434083 ,\n",
    "        20.98452475,  16.07650192,  16.07650192,  16.57113469,\n",
    "        36.11079464,  37.84783835,  27.82194848,  33.46359332,\n",
    "        29.5706502 ,  23.38638738,  36.23351603,  32.40968826,\n",
    "        18.88972581,  21.92965639,  28.68963762,  30.80379718])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scatter plot\n",
    "df.plot(kind='scatter', x='hp', y='mpg', s=sizes)\n",
    "\n",
    "# Add the title\n",
    "plt.title('Fuel efficiency vs Horse-power')\n",
    "\n",
    "# Add the x-axis label\n",
    "plt.xlabel('Horse-power')\n",
    "\n",
    "# Add the y-axis label\n",
    "plt.ylabel('Fuel efficiency (mpg)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas box plots\n",
    "\n",
    "While pandas can plot multiple columns of data in a single figure, making plots that share the same x and y axes, there are cases where two columns cannot be plotted together because their units do not match. The `.plot()` method can generate subplots for each column being plotted. Here, each plot will be scaled independently.\n",
    "\n",
    "In this exercise your job is to generate box plots for ***fuel efficiency (mpg)*** and ***weight*** from the automobiles data set. To do this in a single figure, you'll specify `subplots=True` inside `.plot()` to generate two separate plots.\n",
    "\n",
    "All necessary modules have been imported and the automobiles dataset is available in the workspace as `df`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Make a list called ***cols*** of the column names to be plotted: ***'weight'*** and ***'mpg'***.\n",
    "* Call plot on ***df[cols]*** to generate a box plot of the two columns in a single figure. To do this, specify ***subplots=True***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the column names to be plotted: cols\n",
    "cols = ['weight', 'mpg']\n",
    "\n",
    "# Generate the box plots\n",
    "df[cols].plot(kind='box', subplots=True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas hist, pdf and cd\n",
    "\n",
    "Pandas relies on the `.hist()` method to not only generate histograms, but also plots of probability density functions (PDFs) and cumulative density functions (CDFs).\n",
    "\n",
    "In this exercise, you will work with a dataset consisting of restaurant bills that includes the amount customers tipped.\n",
    "\n",
    "The original dataset is provided by the [Seaborn package](https://github.com/mwaskom/seaborn-data/blob/master/tips.csv).\n",
    "\n",
    "Your job is to plot a PDF and CDF for the fraction column of the tips dataset. This column contains information about what `fraction` of the total bill is comprised of the tip.\n",
    "\n",
    "Remember, when plotting the PDF, you need to specify `normed=True` in your call to `.hist()`, and when plotting the CDF, you need to specify `cumulative=True` in addition to `normed=True`.\n",
    "\n",
    "All necessary modules have been imported and the tips dataset is available in the workspace as `df`. Also, some formatting code has been written so that the plots you generate will appear on separate rows.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Plot a PDF for the values in ***fraction*** with 30 ***bins*** between 0 and 30%. The range has been taken care of for you. ***ax=axes[0]*** means that this plot will appear in the first row.\n",
    "* Plot a CDF for the values in ***fraction*** with 30 ***bins*** between 0 and 30%. Again, the range has been specified for you. To make the CDF appear on the second row, you need to specify ***ax=axes[1]***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/tips.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This formats the plots such that they appear on separate rows\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "# Plot the PDF\n",
    "df.fraction.plot(ax=axes[0], kind='hist', bins=30, density=True, range=(0,.3))\n",
    "\n",
    "# Plot the CDF\n",
    "df.fraction.plot(ax=axes[1], kind='hist', bins=30, density=True, cumulative=True, range=(0,.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Statistical Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing with describe()\n",
    "\n",
    "***Describe***\n",
    "* count: number of entires\n",
    "* mean: average of entries\n",
    "* std: standard deviation\n",
    "* min: miniumum entry\n",
    "* 25%: first quartile\n",
    "* 50%: median or second quartile\n",
    "* 75%: third quartile\n",
    "* max: maximum entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.describe()  # summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['sepal length (cm)'].count()  # Applied to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['sepal width (cm)'].count()  # Applied to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[['petal length (cm)', 'petal width (cm)']].count()  # Applied to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris[['petal length (cm)', 'petal width (cm)']].count())  # Returns series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averages\n",
    "\n",
    "* measures the tendency to a central value of a measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['sepal length (cm)'].mean(numeric_only=True)  # Applied to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.mean(numeric_only=True)  # Applied to entire DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Deviations (std)\n",
    "\n",
    "* measures spread of a measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean and Standard Deviation on a Bell Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['sepal width (cm)'].plot(kind='hist', bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medians\n",
    "\n",
    "* middle number of the measurements\n",
    "* special example of a quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantile\n",
    "\n",
    "* If q is between 0 and 1, the qth quantile of a dataset is a numerical value that splits the data into two sets\n",
    "    * one with the fraction q of smaller observations\n",
    "    * one with the fraction q of larger observations\n",
    "* Quantiles are percentages\n",
    "* Median is the 0.5 quantile or the 50th percentile of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.5\n",
    "iris.quantile(q, numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inter-quartile range (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [0.25, 0.75]\n",
    "iris.quantile(q, numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range\n",
    "\n",
    "* interval between the smallest and largest observations\n",
    "* given by the min and max methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.min(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.max(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(kind='box')\n",
    "plt.ylabel('[cm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuel efficiency\n",
    "\n",
    "From the automobiles data set, which value corresponds to the median value of the `'mpg'` column? Your job is to select the `'mpg'` column and call the `.median(numeric_only=True)` method on it. The automobile DataFrame has been provided as `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/auto-mpg.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bachelor's degrees awarded to women\n",
    "In this exercise, you will investigate statistics of the percentage of Bachelor's degrees awarded to women from 1970 to 2011. Data is recorded every year for 17 different fields. This data set was obtained from the [Digest of Education Statistics](https://nces.ed.gov/programs/digest/2013menu_tables.asp).\n",
    "\n",
    "Your job is to compute the minimum and maximum values of the `'Engineering'` column and generate a line plot of the mean value of all 17 academic fields per year. To perform this step, you'll use the `.mean(numeric_only=True)` method with the keyword argument `axis='columns'`. This computes the mean across all columns per row.\n",
    "\n",
    "The DataFrame has been pre-loaded for you as `df` with the index set to `'Year'`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Print the minimum value of the ***'Engineering'*** column.\n",
    "* Print the maximum value of the ***'Engineering'*** column.\n",
    "* Construct the mean percentage per year with ***.mean(axis='columns')***. Assign the result to ***mean***.\n",
    "* Plot the average percentage per year. Since ***'Year'*** is the index of ***df***, it will appear on the x-axis of the plot. No keyword arguments are needed in your call to ***.plot()***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/percent-bachelors-degrees-women-usa.csv'\n",
    "df = pd.read_csv(data_file, index_col='Year')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the minimum value of the Engineering column\n",
    "df.Engineering.min(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the maximum value of the Engineering column\n",
    "df.Engineering.max(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the mean percentage per year: mean\n",
    "mean = df.mean(axis='columns')\n",
    "mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average percentage per year\n",
    "mean.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median vs mean\n",
    "\n",
    "In many data sets, there can be large differences in the mean and median value due to the presence of outliers.\n",
    "\n",
    "In this exercise, you'll investigate the mean, median, and max fare prices paid by passengers on the Titanic and generate a box plot of the fare prices. This data set was obtained from [Vanderbilt University](https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic.html).\n",
    "\n",
    "All necessary modules have been imported and the DataFrame is available in the workspace as `df`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Print summary statistics of the ***'fare'*** column of ***df*** with ***.describe()*** and ***print()***. Note: ***df.fare*** and ***df['fare']*** are equivalent.\n",
    "* Generate a box plot of the ***'fare'*** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/titanic.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fare.plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantiles\n",
    "\n",
    "In this exercise, you'll investigate the probabilities of life expectancy in countries around the world. This dataset contains life expectancy for persons born each year from 1800 to 2015. Since country names change or results are not reported, not every country has values. This dataset was obtained from [Gapminder](https://docs.google.com/a/continuum.io/spreadsheets/d/1dgOdlUEq6_V55OHZCxz5BG_0uoghJTeA6f83br5peNs/pub?range=A1:D70&gid=1&output=html#).\n",
    "\n",
    "First, you will determine the number of countries reported in 2015. There are a total of 260 unique countries in the entire dataset. Then, you will compute the 5th and 95th percentiles of life expectancy over the entire dataset. Finally, you will make a box plot of life expectancy every 50 years from 1800 to 2000. Notice the large change in the distributions over this period.\n",
    "\n",
    "The dataset has been pre-loaded into a DataFrame called `df`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Print the number of countries reported in 2015. To do this, use the ***.count()*** method on the ***'2015'*** column of ***df***.\n",
    "* Print the 5th and 95th percentiles of ***df***. To do this, use the ***.quantile()*** method with the list ***[0.05, 0.95]***.\n",
    "* Generate a box plot using the list of columns provided in ***years***. This has already been done for you, so click on 'Submit Answer' to view the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/life_expectancy_at_birth.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of countries reported in 2015\n",
    "df['2015'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 5th and 95th percentiles\n",
    "df.quantile([0.05, 0.95], numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a box plot\n",
    "years = ['1800','1850','1900','1950','2000']\n",
    "df[years].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard deviation of temperature\n",
    "\n",
    "Let's use the mean and standard deviation to explore differences in temperature distributions in Pittsburgh in 2013. The data has been obtained from [Weather Underground](https://www.wunderground.com/history/).\n",
    "\n",
    "In this exercise, you're going to compare the distribution of daily temperatures in January and March. You'll compute the mean and standard deviation for these two months. You will notice that while the mean values are similar, the standard deviations are quite different, meaning that one month had a larger fluctuation in temperature than the other.\n",
    "\n",
    "The DataFrames have been pre-loaded for you as `january`, which contains the January data, and `march`, which contains the March data.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Compute and print the means of the January and March data using the ***.mean(numeric_only=True)*** method.\n",
    "* Compute and print the standard deviations of the January and March data using the ***.std(numeric_only=True)*** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_values = np.array([['2013-01-01', 28],\n",
    "                       ['2013-01-02', 21],\n",
    "                       ['2013-01-03', 24],\n",
    "                       ['2013-01-04', 28],\n",
    "                       ['2013-01-05', 30],\n",
    "                       ['2013-01-06', 34],\n",
    "                       ['2013-01-07', 29],\n",
    "                       ['2013-01-08', 31],\n",
    "                       ['2013-01-09', 36],\n",
    "                       ['2013-01-10', 34],\n",
    "                       ['2013-01-11', 47],\n",
    "                       ['2013-01-12', 55],\n",
    "                       ['2013-01-13', 62],\n",
    "                       ['2013-01-14', 44],\n",
    "                       ['2013-01-15', 30],\n",
    "                       ['2013-01-16', 32],\n",
    "                       ['2013-01-17', 32],\n",
    "                       ['2013-01-18', 24],\n",
    "                       ['2013-01-19', 42],\n",
    "                       ['2013-01-20', 35],\n",
    "                       ['2013-01-21', 18],\n",
    "                       ['2013-01-22', 9],\n",
    "                       ['2013-01-23', 11],\n",
    "                       ['2013-01-24', 16],\n",
    "                       ['2013-01-25', 16],\n",
    "                       ['2013-01-26', 23],\n",
    "                       ['2013-01-27', 23],\n",
    "                       ['2013-01-28', 40],\n",
    "                       ['2013-01-29', 59],\n",
    "                       ['2013-01-30', 58],\n",
    "                       ['2013-01-31', 32]]).transpose()\n",
    "cols = ['Date', 'Temperature']\n",
    "jan_zip = list(zip(cols, jan_values))\n",
    "jan_dict = dict(jan_zip)\n",
    "january = pd.DataFrame.from_dict(jan_dict).astype({'Temperature': np.int64})\n",
    "january.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar_values = np.array([['2013-03-01', 28],\n",
    "                       ['2013-03-02', 26],\n",
    "                       ['2013-03-03', 24],\n",
    "                       ['2013-03-04', 28],\n",
    "                       ['2013-03-05', 32],\n",
    "                       ['2013-03-06', 34],\n",
    "                       ['2013-03-07', 36],\n",
    "                       ['2013-03-08', 32],\n",
    "                       ['2013-03-09', 40],\n",
    "                       ['2013-03-10', 55],\n",
    "                       ['2013-03-11', 55],\n",
    "                       ['2013-03-12', 40],\n",
    "                       ['2013-03-13', 32],\n",
    "                       ['2013-03-14', 30],\n",
    "                       ['2013-03-15', 38],\n",
    "                       ['2013-03-16', 36],\n",
    "                       ['2013-03-17', 32],\n",
    "                       ['2013-03-18', 34],\n",
    "                       ['2013-03-19', 36],\n",
    "                       ['2013-03-20', 32],\n",
    "                       ['2013-03-21', 22],\n",
    "                       ['2013-03-22', 28],\n",
    "                       ['2013-03-23', 34],\n",
    "                       ['2013-03-24', 34],\n",
    "                       ['2013-03-25', 32],\n",
    "                       ['2013-03-26', 34],\n",
    "                       ['2013-03-27', 34],\n",
    "                       ['2013-03-28', 37],\n",
    "                       ['2013-03-29', 43],\n",
    "                       ['2013-03-30', 43],\n",
    "                       ['2013-03-31', 44]]).transpose()\n",
    "mar_zip = list(zip(cols, mar_values))\n",
    "mar_dict = dict(mar_zip)\n",
    "march = pd.DataFrame.from_dict(mar_dict).astype({'Temperature': np.int64})\n",
    "march.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean of the January and March data\n",
    "january.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the standard deviation of the January and March data\n",
    "january.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Populations with Boolean Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe species column\n",
    "\n",
    "* contains categorical data\n",
    "* count: number of non-null entries\n",
    "* unique: number of distinct values\n",
    "* top: most frequent category\n",
    "* freq: number of occurrences of the top value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique and Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering by species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = iris['species'] == 'setosa'\n",
    "setosa = iris.loc[indices,:]  # extract new DataFrame\n",
    "\n",
    "indices = iris['species'] == 'versicolor'\n",
    "versicolor = iris.loc[indices,:]  # extract new DataFrame\n",
    "\n",
    "indices = iris['species'] == 'virginica'\n",
    "virginica = iris.loc[indices,:]  # extract new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versicolor['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginica['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versicolor.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginica.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual EDA: All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(kind='hist',\n",
    "          bins=50,\n",
    "          range=(0, 8),\n",
    "          alpha=0.3)\n",
    "plt.title('Entire Iris Dataset')\n",
    "plt.xlabel('[cm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual EDA: Individual Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa.plot(kind='hist',\n",
    "          bins=50,\n",
    "          range=(0, 8),\n",
    "          alpha=0.3)\n",
    "plt.title('Setosa Dataset')\n",
    "plt.xlabel('[cm]')\n",
    "\n",
    "versicolor.plot(kind='hist',\n",
    "          bins=50,\n",
    "          range=(0, 8),\n",
    "          alpha=0.3)\n",
    "plt.title('Versicolor Dataset')\n",
    "plt.xlabel('[cm]')\n",
    "\n",
    "virginica.plot(kind='hist',\n",
    "          bins=50,\n",
    "          range=(0, 8),\n",
    "          alpha=0.3)\n",
    "plt.title('Virginica Dataset')\n",
    "plt.xlabel('[cm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical EDA: describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_all = iris.describe()\n",
    "describe_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_setosa = setosa.describe()\n",
    "describe_setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_versicolor = versicolor.describe()\n",
    "describe_versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_virginica = virginica.describe()\n",
    "describe_virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Errors\n",
    "\n",
    "* This is the absolute difference of the correct statistics computed in its own group from the statistic computed with the whole population divided by the correct statistics\n",
    "* Elementwise arithmetic so no need for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_setosa = 100 * np.abs(describe_setosa - describe_all)\n",
    "error_setosa = error_setosa / describe_setosa\n",
    "error_setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_versicolor = 100 * np.abs(describe_versicolor - describe_all)\n",
    "error_versicolor = error_versicolor / describe_versicolor\n",
    "error_versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_virginica = 100 * np.abs(describe_virginica - describe_all)\n",
    "error_virginica = error_virginica / describe_virginica\n",
    "error_virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering and counting\n",
    "\n",
    "How many automobiles were manufactured in Asia in the automobile dataset? The DataFrame has been provided for you as `df`. Use filtering and the `.count()` member method to determine the number of rows where the `'origin'` column has the value `'Asia'`.\n",
    "\n",
    "As an example, you can extract the rows that contain `'US'` as the country of origin using `df[df['origin'] == 'US']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/auto-mpg.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['origin'] == 'Asia'].origin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate and summarize\n",
    "\n",
    "Let's use population filtering to determine how the automobiles in the US differ from the global average and standard deviation. How does the distribution of fuel efficiency (MPG) for the US differ from the global average and standard deviation?\n",
    "\n",
    "In this exercise, you'll compute the means and standard deviations of all columns in the full automobile dataset. Next, you'll compute the same quantities for just the US population and subtract the global values from the US values.\n",
    "\n",
    "All necessary modules have been imported and the DataFrame has been pre-loaded as `df`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Compute the global mean and global standard deviations of ***df*** using the ***.mean(numeric_only=True)*** and ***.std(numeric_only=True)*** methods. Assign the results to ***global_mean*** and ***global_std***.\n",
    "* Filter the ***'US'*** population from the ***'origin'*** column and assign the result to ***us***.\n",
    "* Compute the US mean and US standard deviations of ***us*** using the ***.mean(numeric_only=True)*** and ***.std(numeric_only=True)*** methods. Assign the results to ***us_mean*** and ***us_std***.\n",
    "* Print the differences between ***us_mean*** and ***global_mean*** and ***us_std*** and ***global_std***. This has already been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute the global mean and global standard deviation: global_mean, global_std\n",
    "global_mean = df.mean(numeric_only=True)\n",
    "global_std = df.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the US population from the origin column: us\n",
    "us = df[df['origin'] == 'US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the US mean and US standard deviation: us_mean, us_std\n",
    "us_mean = us.mean(numeric_only=True)\n",
    "us_std = us.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the differences\n",
    "print(us_mean - global_mean)\n",
    "print(us_std - global_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate and plot\n",
    "\n",
    "Population filtering can be used alongside plotting to quickly determine differences in distributions between the sub-populations. You'll work with the Titanic dataset.\n",
    "\n",
    "There were three passenger classes on the Titanic, and passengers in each class paid a different fare price. In this exercise, you'll investigate the differences in these fare prices.\n",
    "\n",
    "Your job is to use Boolean filtering and generate box plots of the fare prices for each of the three passenger classes. The fare prices are contained in the `'fare'` column and passenger class information is contained in the `'pclass'` column.\n",
    "\n",
    "When you're done, notice the portions of the box plots that differ and those that are similar.\n",
    "\n",
    "The DataFrame has been pre-loaded for you as `titanic`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Inside ***plt.subplots()***, specify the ***nrows*** and ***ncols*** parameters so that there are 3 rows and 1 column.\n",
    "* Filter the rows where the ***'pclass'*** column has the values ***1*** and generate a box plot of the ***'fare'*** column.\n",
    "* Filter the rows where the ***'pclass'*** column has the values ***2*** and generate a box plot of the ***'fare'*** column.\n",
    "* Filter the rows where the ***'pclass'*** column has the values ***3*** and generate a box plot of the ***'fare'*** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/titanic.csv'\n",
    "titanic = pd.read_csv(data_file)\n",
    "titanic.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the box plots on 3 separate rows and 1 column\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "# Generate a box plot of the fare prices for the First passenger class\n",
    "titanic.loc[titanic['pclass'] == 1].plot(ax=axes[0], y='fare', kind='box')\n",
    "\n",
    "# Generate a box plot of the fare prices for the Second passenger class\n",
    "titanic.loc[titanic['pclass'] == 2].plot(ax=axes[1], y='fare', kind='box')\n",
    "\n",
    "# Generate a box plot of the fare prices for the Third passenger class\n",
    "titanic.loc[titanic['pclass'] == 3].plot(ax=axes[2], y='fare', kind='box')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Time Series in pandas\n",
    "\n",
    "In this chapter, you will learn how to manipulate and visualize time series data using Pandas. You will become familiar with concepts such as upsampling, downsampling, and interpolation. You will practice using Pandas' method chaining to efficiently filter your data and perform time series analyses. From stock prices to flight timings, time series data are found in a wide variety of domains and being able to effectively work with such data can be an invaluable skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing pandas time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pandas to read datetime objects\n",
    "\n",
    "* read_csv() function\n",
    "    * Can read strings into datetime objects\n",
    "    * Need to specify ***parse_dates=True***\n",
    "* ISO 8601 format\n",
    "    * ***yyyy-mm-dd hh:mm:ss***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Product Sales CSV - Parse dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/sales_data/sales-feb-2015.csv',\n",
    "                    parse_dates=True,\n",
    "                    index_col='Date')\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting single datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc['2015-02-19 10:59:00', 'Company']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting whole day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc['2015-02-05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial datetime string selection\n",
    "\n",
    "* Alternative formats:\n",
    "    * ***sales.loc['February 5, 2015']***\n",
    "    * ***sales.loc['2015-Feb-5']***\n",
    "* Whole month: ***sales.loc['2015-02']***\n",
    "* Whole year: ***sales.loc['2015']***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting whole month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc['2015-02'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing using dates/times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc['2015-2-16':'2015-2-20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert strings to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evening_2_11 = pd.to_datetime(['2015-2-11 20:03',\n",
    "                               '2015-2-11 21:00',\n",
    "                               '2015-2-11 22:50',\n",
    "                               '2015-2-11 23:00'])\n",
    "evening_2_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.reindex(evening_2_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.reindex(evening_2_11, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.reindex(evening_2_11, method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "#### Reading and slicing times\n",
    "\n",
    "For this exercise, we have read in the same data file using three different approaches:\n",
    "\n",
    "```python\n",
    "df1 = pd.read_csv(filename)\n",
    "df2 = pd.read_csv(filename, parse_dates=['Date'])\n",
    "df3 = pd.read_csv(filename, index_col='Date', parse_dates=True)\n",
    "```\n",
    "\n",
    "Use the `.head()` and `.info()` methods in the IPython Shell to inspect the DataFrames. Then, try to index each DataFrame with a datetime string. Which of the resulting DataFrames allows you to easily index and slice data by dates using, for example, `df1.loc['2010-Aug-01']`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/weather_data_austin_2010.csv'\n",
    "df1 = pd.read_csv(data_file)\n",
    "df2 = pd.read_csv(data_file, parse_dates=['Date'])\n",
    "df3 = pd.read_csv(data_file, index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***datatime slicing allowed when index is datetime***\n",
    "\n",
    "* doesn't work with\n",
    "```python\n",
    "df1.loc['2010-Aug-01']\n",
    "df2.loc['2010-Aug-01']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc['2010-Aug-01'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and using a DatetimeIndex\n",
    "\n",
    "The pandas Index is a powerful way to handle time series data, so it is valuable to know how to build one yourself. Pandas provides the `pd.to_datetime()` function for just this task. For example, if passed the list of strings `['2015-01-01 091234','2015-01-01 091234']` and a `format` specification variable, such as `format='%Y-%m-%d %H%M%S`, pandas will parse the string into the proper datetime elements and build the datetime objects.\n",
    "\n",
    "In this exercise, a list of temperature data and a list of date strings has been pre-loaded for you as `temperature_list` and `date_list` respectively. Your job is to use the `.to_datetime()` method to build a DatetimeIndex out of the list of date strings, and to then use it along with the list of temperature data to build a pandas Series.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Prepare a format string, ***time_format***, using ***'%Y-%m-%d %H:%M'*** as the desired format.\n",
    "* Convert ***date_list*** into a ***datetime*** object by using the ***pd.to_datetime()*** function. Specify the format string you defined above and assign the result to ***my_datetimes***.\n",
    "* Construct a pandas Series called ***time_series*** using ***pd.Series()*** with ***temperature_list*** and ***my_datetimes***. Set the ***index*** of the Series to be ***my_datetimes***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_file = 'data/date_list.csv'\n",
    "date_df = pd.read_csv(date_file, header=None)\n",
    "\n",
    "date_df[0] = date_df[0].map(lambda x: x.lstrip(\" '\").rstrip(\"',\"))\n",
    "\n",
    "date_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = list(date_df[0])\n",
    "date_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = np.random.uniform(low=41.8, high=95.3, size=8759)\n",
    "temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a format string: time_format\n",
    "time_format = '%Y%m%d %H:%M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date_list into a datetime object: my_datetimes\n",
    "my_datetimes = pd.to_datetime(date_list, format=time_format) \n",
    "my_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a pandas Series using temperature_list and my_datetimes: time_series\n",
    "time_series = pd.Series(temp_list, index=my_datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial string indexing and slicing\n",
    "\n",
    "Pandas time series support \"partial string\" indexing. What this means is that even when passed only a portion of the datetime, such as the date but not the time, pandas is remarkably good at doing what one would expect. Pandas datetime indexing also supports a wide variety of commonly used datetime string formats, even when mixed.\n",
    "\n",
    "In this exercise, a time series that contains hourly weather data has been pre-loaded for you. This data was read using the `parse_dates=True` option in `read_csv()` with `index_col=\"Dates\"` so that the Index is indeed a `DatetimeIndex`.\n",
    "\n",
    "All data from the `'Temperature'` column has been extracted into the variable `ts0`. Your job is to use a variety of natural date strings to extract one or more values from `ts0`.\n",
    "\n",
    "After you are done, you will have three new variables - `ts1`, `ts2`, and `ts3`. You can slice these further to extract only the first and last entries of each. Try doing this after your submission for more practice.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Extract data from ***ts0*** for a single hour - the hour from 9pm to 10pm on ***2010-10-11***. Assign it to ***ts1***.\n",
    "* Extract data from ***ts0*** for a single day - ***July 4th, 2010*** - and assign it to ***ts2***.\n",
    "* Extract data from ***ts0*** for the second half of December 2010 - ***12/15/2010*** to ***12/31/2010***. Assign it to ***ts3***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hour from 9pm to 10pm on '2010-10-11': ts1\n",
    "ts1 = time_series.loc['2010-10-11 21:00:00':'2010-10-11 22:00:00']\n",
    "ts1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract '2010-07-04' from ts0: ts2\n",
    "ts2 = time_series.loc['2010-07-04']\n",
    "ts2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from '2010-12-15' to '2010-12-31': ts3\n",
    "ts3 = time_series.loc['2010-12-15':'2010-12-31']\n",
    "ts3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing the Index\n",
    "\n",
    "Reindexing is useful in preparation for adding or otherwise combining two time series data sets. To reindex the data, we provide a new index and ask pandas to try and match the old data to the new index. If data is unavailable for one of the new index dates or times, you must tell pandas how to fill it in. Otherwise, pandas will fill with `NaN` by default.\n",
    "\n",
    "In this exercise, two time series data sets containing daily data have been pre-loaded for you, each indexed by dates. The first, `ts1`, includes weekends, but the second, `ts2`, does not. The goal is to combine the two data sets in a sensible way. Your job is to reindex the second data set so that it has weekends as well, and then add it to the first. When you are done, it would be informative to inspect your results.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Create a new time series ***ts3*** by reindexing ***ts2*** with the index of ***ts1***. To do this, call ***.reindex()*** on ***ts2*** and pass in the index of ***ts1*** (***ts1.index***).\n",
    "* Create another new time series, ***ts4***, by calling the same ***.reindex()*** as above, but also specifiying a fill method, using the keyword argument ***method=\"ffill\"*** to forward-fill values.\n",
    "* Add ***ts1 + ts2***. Assign the result to ***sum12***.\n",
    "* Add ***ts1 + ts3***. Assign the result to ***sum13***.\n",
    "* Add ***ts1 + ts4***. Assign the result to ***sum14***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1_index = pd.DatetimeIndex(['2016-07-01', '2016-07-02', '2016-07-03', '2016-07-04',\n",
    "                              '2016-07-05', '2016-07-06', '2016-07-07', '2016-07-08',\n",
    "                              '2016-07-09', '2016-07-10', '2016-07-11', '2016-07-12',\n",
    "                              '2016-07-13', '2016-07-14', '2016-07-15', '2016-07-16',\n",
    "                              '2016-07-17'])\n",
    "ts1_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1_values = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
    "ts1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = pd.Series(ts1_values, index=ts1_index)\n",
    "ts1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2_index = pd.DatetimeIndex(['2016-07-01', '2016-07-04', '2016-07-05', '2016-07-06',\n",
    "                              '2016-07-07', '2016-07-08', '2016-07-11', '2016-07-12',\n",
    "                              '2016-07-13', '2016-07-14', '2016-07-15'])\n",
    "ts2_values = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
    "ts2 = pd.Series(ts2_values, index=ts2_index)\n",
    "ts2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex without fill method: ts3\n",
    "ts3 = ts2.reindex(ts1.index)\n",
    "ts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex with fill method, using forward fill: ts4\n",
    "ts4 = ts2.reindex(ts1.index, method='ffill')\n",
    "ts4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ts1 + ts2: sum12\n",
    "sum12 = ts1 + ts2\n",
    "sum12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ts1 + ts3: sum13\n",
    "sum13 = ts1 + ts3\n",
    "sum13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ts1 + ts4: sum14\n",
    "sum14 = ts1 + ts4\n",
    "sum14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling pandas time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/sales_data/sales-feb-2015.csv',\n",
    "                    parse_dates=True,\n",
    "                    index_col='Date')\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling\n",
    "\n",
    "* Statistical methods over different time intervals\n",
    "```python\n",
    "mean()\n",
    "sum()\n",
    "count()\n",
    "# etc.\n",
    "```\n",
    "* Down-sampling\n",
    "    * reduce datetime rows to slower frequency\n",
    "* Up-sampling\n",
    "    * increase datetime rows to faster frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mean = sales.resample('D').mean(numeric_only=True)\n",
    "daily_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mean.loc['2015-2-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc['2015-2-2', 'Units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc['2015-2-2', 'Units'].mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.resample('D').sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.resample('D').sum().max(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.resample('W').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Input      | Description  |\n",
    "|------------|--------------|\n",
    "| 'min', 'T' | minute       |\n",
    "| 'H'        | hour         |\n",
    "| 'D'        | day          |\n",
    "| 'B'        | business day |\n",
    "| 'W'        | week         |\n",
    "| 'M'        | month        |\n",
    "| 'Q'        | quarter      |\n",
    "| 'A'        | year         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplying frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc[:, 'Units'].resample('2W').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_days = sales.loc['2015-2-4':'2015-2-5', 'Units']\n",
    "two_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling and filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_days.resample('4H').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling and frequency\n",
    "\n",
    "Pandas provides methods for resampling time series data. When downsampling or upsampling, the syntax is similar, but the methods called are different. Both use the concept of 'method chaining' - `df.method1().method2().method3()` - to direct the output from one method call to the input of the next, and so on, as a sequence of operations, one feeding into the next.\n",
    "\n",
    "For example, if you have hourly data, and just need daily data, pandas will not guess how to throw out the 23 of 24 points. You must specify this in the method. One approach, for instance, could be to take the mean, as in `df.resample('D').mean(numeric_only=True)`.\n",
    "\n",
    "In this exercise, a data set containing hourly temperature data has been pre-loaded for you. Your job is to resample the data using a variety of aggregation methods to answer a few questions.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Downsample the ***'Temperature'*** column of ***df*** to 6 hour data using ***.resample('6h')*** and ***.mean(numeric_only=True)***. Assign the result to ***df1***.\n",
    "* Downsample the ***'Temperature'*** column of ***df*** to daily data using ***.resample('D')*** and then count the number of data points in each day with ***.count()***. Assign the result ***df2***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DataCamp-master/11-pandas-foundations/_datasets/weather_data_austin_2010.csv',\n",
    "                 parse_dates=True,\n",
    "                 index_col='Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to 6 hour data and aggregate by mean: df1\n",
    "df1 = df.Temperature.resample('6H').mean(numeric_only=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to daily data and count the number of data points: df2\n",
    "df2 = df.Temperature.resample('D').count()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating and resampling\n",
    "\n",
    "With pandas, you can resample in different ways on different subsets of your data. For example, resampling different months of data with different aggregations. In this exercise, the data set containing hourly temperature data from the last exercise has been pre-loaded.\n",
    "\n",
    "Your job is to resample the data using a variety of aggregation methods. The DataFrame is available in the workspace as `df`. You will be working with the `'Temperature'` column.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Use partial string indexing to extract temperature data for August 2010 into ***august***.\n",
    "* Use the temperature data for August and downsample to find the daily maximum temperatures. Store the result in ***august_highs***.\n",
    "* Use partial string indexing to extract temperature data for February 2010 into ***february***.\n",
    "* Use the temperature data for February and downsample to find the daily minimum temperatures. Store the result in ***february_lows***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temperature data for August: august\n",
    "august = df.loc['2010-08', 'Temperature']\n",
    "august.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to obtain only the daily highest temperatures in August: august_highs\n",
    "august_highs = august.resample('D').max(numeric_only=True)\n",
    "august_highs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temperature data for February: february\n",
    "february = august = df.loc['2010-02', 'Temperature']\n",
    "february.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to obtain the daily lowest temperatures in February: february_lows\n",
    "february_lows = february.resample('D').min(numeric_only=True)\n",
    "february_lows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling mean and frequency\n",
    "In this exercise, some hourly weather data is pre-loaded for you. You will continue to practice resampling, this time using rolling means.\n",
    "\n",
    "Rolling means (or moving averages) are generally used to smooth out short-term fluctuations in time series data and highlight long-term trends. You can read more about them here.\n",
    "\n",
    "To use the `.rolling()` method, you must always use method chaining, first calling `.rolling()` and then chaining an aggregation method after it. For example, with a Series `hourly_data`, `hourly_data.rolling(window=24).mean(numeric_only=True)` would compute new values for each hourly point, based on a 24-hour window stretching out behind each point. The frequency of the output data is the same: it is still hourly. Such an operation is useful for smoothing time series data.\n",
    "\n",
    "Your job is to resample the data using the combination of `.rolling()` and `.mean(numeric_only=True)`. You will work with the same DataFrame `df` from the previous exercise.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Use partial string indexing to extract temperature data from August 1 2010 to August 15 2010. Assign to ***unsmoothed***.\n",
    "* Use ***.rolling()*** with a 24 hour window to smooth the mean temperature data. Assign the result to ***smoothed***.\n",
    "* Use a dictionary to create a new DataFrame ***august*** with the time series ***smoothed*** and ***unsmoothed*** as columns.\n",
    "* Plot both the columns of ***august*** as line plots using the ***.plot()*** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from 2010-Aug-01 to 2010-Aug-15: unsmoothed\n",
    "unsmoothed = df['Temperature']['2010-Aug-01':'2010-Aug-15']\n",
    "unsmoothed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a rolling mean with a 24 hour window: smoothed\n",
    "smoothed = df['Temperature']['2010-Aug-01':'2010-Aug-15'].rolling(window=24).mean(numeric_only=True)\n",
    "smoothed.iloc[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with columns smoothed and unsmoothed: august\n",
    "august = pd.DataFrame({'smoothed':smoothed, 'unsmoothed':unsmoothed})\n",
    "august.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both smoothed and unsmoothed data using august.plot().\n",
    "august.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample and roll with it\n",
    "\n",
    "As of pandas version 0.18.0, the interface for applying rolling transformations to time series has become more consistent and flexible, and feels somewhat like a `groupby` (If you do not know what a `groupby` is, don't worry, you will learn about it in the next course!).\n",
    "\n",
    "You can now flexibly chain together resampling and rolling operations. In this exercise, the same weather data from the previous exercises has been pre-loaded for you. Your job is to extract one month of data, resample to find the daily high temperatures, and then use a rolling and aggregation operation to smooth the data.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Use partial string indexing to extract August 2010 temperature data, and assign to ***august***.\n",
    "* Resample to daily frequency, saving the maximum daily temperatures, and assign the result to ***daily_highs***.\n",
    "* As part of one long method chain, repeat the above resampling (or you can re-use ***daily_highs***) and then combine it with ***.rolling()*** to apply a 7 day ***.mean(numeric_only=True)*** (with ***window=7*** inside ***.rolling()***) so as to smooth the daily highs. Assign the result to ***daily_highs_smoothed*** and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the August 2010 data: august\n",
    "august = df['Temperature']['2010-08']\n",
    "august.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to daily data, aggregating by max: daily_highs\n",
    "daily_highs = august.resample('D').max(numeric_only=True)\n",
    "daily_highs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a rolling 7-day window with method chaining to smooth the daily high temperatures in August\n",
    "daily_highs_smoothed = daily_highs.rolling(window=7).mean(numeric_only=True)\n",
    "daily_highs_smoothed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating pandas time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/sales_data/sales-feb-2015.csv',\n",
    "                    parse_dates=['Date'])\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Company'].str.upper().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substring matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Product'].str.contains('ware').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(True + False)\n",
    "print(True + True)\n",
    "print(False + False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Product'].str.contains('ware').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Date'].dt.hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central = sales['Date'].dt.tz_localize('US/Central')\n",
    "central.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central.dt.tz_convert('US/Eastern').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Date'].dt.tz_localize('US/Central').dt.tz_convert('US/Eastern').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('DataCamp-master/11-pandas-foundations/_datasets/world_population.csv',\n",
    "                         parse_dates=True,\n",
    "                         index_col= 'Date')\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsample population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.resample('A').first().head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolate missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.resample('A').first().interpolate('linear').head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method chaining and filtering\n",
    "\n",
    "We've seen that pandas supports method chaining. This technique can be very powerful when cleaning and filtering data.\n",
    "\n",
    "In this exercise, a DataFrame containing flight departure data for a single airline and a single airport for the month of July 2015 has been pre-loaded. Your job is to use `.str()` filtering and method chaining to generate summary statistics on flight delays each day to Dallas.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Use ***.str.strip()*** to strip extra whitespace from ***df.columns***. Assign the result back to ***df.columns***.\n",
    "* In the ***'Destination Airport'*** column, extract all entries where Dallas (***'DAL'***) is the destination airport. Use ***.str.contains('DAL')*** for this and store the result in ***dallas***.\n",
    "* Resample ***dallas*** such that you get the total number of departures each day. Store the result in ***daily_departures***.\n",
    "* Generate summary statistics for daily Dallas departures using ***.describe()***. Store the result in ***stats***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DataCamp-master/11-pandas-foundations/_datasets/austin_airport_departure_data_2015_july.csv',\n",
    "                 skiprows=15,\n",
    "                 parse_dates=True,\n",
    "                 index_col='Date (MM/DD/YYYY)')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip extra whitespace from the column names: df.columns\n",
    "print(f'Before: \\n {df.columns}')\n",
    "df.columns = df.columns.str.strip()\n",
    "print(f'After: \\n {df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for which the destination airport is Dallas: dallas\n",
    "dallas = df['Destination Airport'].str.contains('DAL')\n",
    "dallas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total number of Dallas departures each day: daily_departures\n",
    "daily_departures = dallas.resample('D').sum()\n",
    "daily_departures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the summary statistics for daily Dallas departures: stats\n",
    "stats = daily_departures.describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values and interpolation\n",
    "\n",
    "One common application of interpolation in data analysis is to fill in missing data.\n",
    "\n",
    "In this exercise, noisy measured data that has some dropped or otherwise missing values has been loaded. The goal is to compare two time series, and then look at summary statistics of the differences. The problem is that one of the data sets is missing data at some of the times. The pre-loaded data `ts1` has value for all times, yet the data set `ts2` does not: it is missing data for the weekends.\n",
    "\n",
    "Your job is to first interpolate to fill in the data for all days. Then, compute the differences between the two data sets, now that they both have full support for all times. Finally, generate the summary statistics that describe the distribution of differences.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Replace the index of ***ts2*** with that of ***ts1***, and then fill in the missing values of ***ts2*** by using ***.interpolate(how='linear')***. Save the result as ***ts2_interp***.\n",
    "* Compute the difference between ***ts1*** and ***ts2_interp***. Take the absolute value of the difference with ***np.abs()***, and assign the result to ***differences***.\n",
    "* Generate and print summary statistics of the ***differences*** with ***.describe()*** and ***print()***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1_index = pd.DatetimeIndex(['2016-07-01', '2016-07-02', '2016-07-03', '2016-07-04',\n",
    "                              '2016-07-05', '2016-07-06', '2016-07-07', '2016-07-08',\n",
    "                              '2016-07-09', '2016-07-10', '2016-07-11', '2016-07-12',\n",
    "                              '2016-07-13', '2016-07-14', '2016-07-15', '2016-07-16',\n",
    "                              '2016-07-17'])\n",
    "ts1_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1_values = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
    "ts1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = pd.Series(ts1_values, index=ts1_index)\n",
    "ts1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2_index = pd.DatetimeIndex(['2016-07-01', '2016-07-04', '2016-07-05', '2016-07-06',\n",
    "                              '2016-07-07', '2016-07-08', '2016-07-11', '2016-07-12',\n",
    "                              '2016-07-13', '2016-07-14', '2016-07-15'])\n",
    "ts2_values = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
    "ts2 = pd.Series(ts2_values, index=ts2_index)\n",
    "ts2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of ts2 to ts1, and then use linear interpolation to fill in the NaNs: ts2_interp\n",
    "ts2_interp = ts2.reindex(ts1.index).interpolate(how='linear')\n",
    "ts2_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the absolute difference of ts1 and ts2_interp: differences \n",
    "differences = np.abs(ts1 - ts2_interp)\n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print summary statistics of the differences\n",
    "differences.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time zones and conversion\n",
    "\n",
    "Time zone handling with pandas typically assumes that you are handling the Index of the Series. In this exercise, you will learn how to handle timezones that are associated with datetimes in the column data, and not just the Index.\n",
    "\n",
    "You will work with the flight departure dataset again, and this time you will select Los Angeles (`'LAX'`) as the destination airport.\n",
    "\n",
    "Here we will use a mask to ensure that we only compute on data we actually want. To learn more about Boolean masks, click [here](https://docs.scipy.org/doc/numpy/reference/maskedarray.generic.html)!\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Create a Boolean mask, ***mask***, such that if the ***'Destination Airport'*** column of df equals ***'LAX'***, the result is ***True***, and otherwise, it is ***False***.\n",
    "* Use the mask to extract only the ***LAX*** rows. Assign the result to ***la***.\n",
    "* Concatenate the two columns ***la['Date (MM/DD/YYYY)']*** and ***la['Wheels-off Time']*** with a ***' '*** space in between. Pass this ***to pd.to_datetime()*** to create a datetime array of all the times the LAX-bound flights left the ground.\n",
    "* Use ***Series.dt.tz_localize()*** to localize the time to ***'US/Central'***.\n",
    "* Use the ***.dt.tz_convert()*** method to convert datetimes from ***'US/Central'*** to ***'US/Pacific'***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DataCamp-master/11-pandas-foundations/_datasets/austin_airport_departure_data_2015_july.csv',\n",
    "                 skiprows=15,\n",
    "                 parse_dates=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Boolean mask to filter out all the 'LAX' departure flights: mask\n",
    "mask = df['Destination Airport'] == 'LAX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mask to subset the data: la\n",
    "la = df[mask]\n",
    "la.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two columns of data to create a datetime series: times_tz_none \n",
    "times_tz_none = pd.to_datetime(la['Date (MM/DD/YYYY)'] + ' ' + la['Wheels-off Time'])\n",
    "times_tz_none.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Localize the time to US/Central: times_tz_central\n",
    "times_tz_central = times_tz_none.dt.tz_localize('US/Central')\n",
    "times_tz_central.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datetimes from US/Central to US/Pacific\n",
    "times_tz_pacific = times_tz_central.dt.tz_convert('US/Pacific')\n",
    "times_tz_pacific.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing pandas time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Topics***\n",
    "* Line types\n",
    "* Plot types\n",
    "* Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pd.read_csv('data/sp500_2010-01-01_-_2015-12-31.csv',\n",
    "                    parse_dates=True,\n",
    "                    index_col= 'Date')\n",
    "sp500.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500['Close'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500['Close'].plot(title='S&P 500')\n",
    "plt.ylabel('Closing Price (US Dollars)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.loc['2012-4-1':'2012-4-7', 'Close'].plot(title='S&P 500')\n",
    "In [11]: plt.ylabel('Closing Price (US Dollars)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.loc['2012-4', 'Close'].plot(style='k.-', title='S&P500')\n",
    "plt.ylabel('Closing Price (US Dollars)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More plot styles\n",
    "\n",
    "* Style format string\n",
    "    * color (k: black)\n",
    "    * marker (.: dot)\n",
    "    * line type (-: solid)\n",
    "    \n",
    "|   Color  |   Marker  |    Line   |\n",
    "|:--------:|:---------:|:---------:|\n",
    "| b: blue  | o: circle | : dotted  |\n",
    "| g: green | *: star   | -: dashed |\n",
    "| r: red   | s: square |           |\n",
    "| c: cyan  | +: plus   |           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500['Close'].plot(kind='area', title='S&P 500')\n",
    "plt.ylabel('Closing Price (US Dollars)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.loc['2012', ['Close','Volume']].plot(title='S&P 500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.loc['2012', ['Close','Volume']].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting time series, datetime indexing\n",
    "\n",
    "Pandas handles datetimes not only in your data, but also in your plotting.\n",
    "\n",
    "In this exercise, some time series data has been pre-loaded. However, we have not parsed the date-like columns nor set the index, as we have done for you in the past!\n",
    "\n",
    "The plot displayed is how pandas renders data with the default integer/positional index. Your job is to convert the `'Date'` column from a collection of strings into a collection of datetime objects. Then, you will use this converted `'Date'` column as your new index, and re-plot the data, noting the improved datetime awareness. After you are done, you can cycle between the two plots you generated by clicking on the 'Previous Plot' and 'Next Plot' buttons.\n",
    "\n",
    "Before proceeding, look at the plot shown and observe how pandas handles data with the default integer index. Then, inspect the DataFrame `df` using the `.head()` method in the IPython Shell to get a feel for its structure.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Use ***pd.to_datetime()*** to convert the ***'Date'*** column to a collection of datetime objects, and assign back to ***df.Date***.\n",
    "* Set the index to this updated ***'Date'*** column, using ***df.set_index()*** with the optional keyword argument ***inplace=True***, so that you don't have to assign the result back to ***df***.\n",
    "* Re-plot the DataFrame to see that the axis is now datetime aware. This code has been written for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DataCamp-master/11-pandas-foundations/_datasets/weather_data_austin_2010.csv',\n",
    "                 usecols=[0, 3])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw data before setting the datetime index\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column into a collection of datetime objects: df.Date\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "df.Date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to be the converted 'Date' column\n",
    "df.set_index('Date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-plot the DataFrame to see that the axis is now datetime aware!\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting date ranges, partial indexing\n",
    "\n",
    "Now that you have set the DatetimeIndex in your DataFrame, you have a much more powerful and flexible set of tools to use when plotting your time series data. Of these, one of the most convenient is partial string indexing and slicing. In this exercise, we've pre-loaded a full year of Austin 2010 weather data, with the index set to be the datetime parsed `'Date'` column as shown in the previous exercise.\n",
    "\n",
    "Your job is to use partial string indexing of the dates, in a variety of datetime string formats, to plot all the summer data and just one week of data together. After you are done, you can cycle between the two plots by clicking on the 'Previous Plot' and 'Next Plot' buttons.\n",
    "\n",
    "First, remind yourself how to extract one month of temperature data using `'May 2010'` as a key into `df.Temperature[]`, and call `head()` to inspect the result: `df.Temperature['May 2010'].head()`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Plot the summer temperatures using method chaining. The summer ranges from the months ***'2010-Jun'*** to ***'2010-Aug'***.\n",
    "* Plot the temperatures for one week in June using the same method chaining, but this time indexing with ***'2010-06-10':'2010-06-17'*** before you follow up with ***.plot()***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DataCamp-master/11-pandas-foundations/_datasets/weather_data_austin_2010.csv',\n",
    "                 parse_dates=True,\n",
    "                 index_col='Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summer data\n",
    "df.Temperature['2010-Jun':'2010-Aug'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the one week data\n",
    "df.Temperature['2010-06-10':'2010-06-17'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Case Study - Sunlight in Austin\n",
    "\n",
    "Working with real-world weather and climate data, in this chapter you will bring together and apply all of the skills you have acquired in this course. You will use Pandas to manipulate the data into a form usable for analysis, and then systematically explore it using the techniques you learned in the prior chapters. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study\n",
    "\n",
    "* Comparing observed weather data from two sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Climate normals of Austin, TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate = pd.read_csv('DataCamp-master/11-pandas-foundations/_datasets/weather_data_austin_2010.csv',\n",
    "                         parse_dates=True,\n",
    "                         index_col='Date')\n",
    "df_climate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather data of Austin, TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DataCamp-master/11-pandas-foundations/_datasets/NOAA_QCLCD_2011_hourly_13904.txt',\n",
    "                 header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: read_csv()\n",
    "\n",
    "* Useful keyword options\n",
    "    * names: assigning column labels\n",
    "    * index_col: assigning index\n",
    "    * parse_dates: parsing datetimes\n",
    "    * na_values: parsing NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in a data file\n",
    "\n",
    "Now that you have identified the method to use to read the data, let's try to read one file. The problem with real data such as this is that the files are almost never formatted in a convenient way. In this exercise, there are several problems to overcome in reading the file. First, there is no header, and thus the columns don't have labels. There is also no obvious index column, since none of the data columns contain a full date or time.\n",
    "\n",
    "Your job is to read the file into a DataFrame using the default arguments. After inspecting it, you will re-read the file specifying that there are no headers supplied.\n",
    "\n",
    "The CSV file has been provided for you as the variable `data_file`.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Import ***pandas*** as ***pd***.\n",
    "* Read the file ***data_file*** into a DataFrame called ***df***.\n",
    "* Print the output of ***df.head()***. This has been done for you. Notice the formatting problems in ***df***.\n",
    "* Re-read the data using specifying the keyword argument ***header=None*** and assign it to ***df_headers***.\n",
    "* Print the output of ***df_headers.head()***. This has already been done for you. Hit 'Submit Answer' and see how this resolves the formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataCamp-master/11-pandas-foundations/_datasets/NOAA_QCLCD_2011_hourly_13904.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data file: df\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data file with header=None: df_headers\n",
    "df_headers = pd.read_csv(data_file,\n",
    "                         header=None)\n",
    "df_headers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-assigning column names\n",
    "\n",
    "After the initial step of reading in the data, the next step is to clean and tidy it so that it is easier to work with.\n",
    "\n",
    "In this exercise, you will begin this cleaning process by re-assigning column names and dropping unnecessary columns.\n",
    "\n",
    "pandas has been imported in the workspace as `pd`, and the file `NOAA_QCLCD_2011_hourly_13904.txt` has been parsed and loaded into a DataFrame `df`. The comma separated string of column names, `column_labels`, and list of columns to drop, `list_to_drop`, have also been loaded for you.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Convert the comma separated string ***column_labels*** to a list of strings using ***.split(',')***. Assign the result to ***column_labels_list***.\n",
    "* Reassign ***df.columns*** using the list of strings ***column_labels_list***.\n",
    "* Call ***df.drop()*** with ***list_to_drop*** and ***axis='columns'***. Assign the result to ***df_dropped***.\n",
    "* Print ***df_dropped.head()*** to examine the result. This has already been done for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = 'Wban,date,Time,StationType,sky_condition,sky_conditionFlag,visibility,visibilityFlag,wx_and_obst_to_vision,wx_and_obst_to_visionFlag,dry_bulb_faren,dry_bulb_farenFlag,dry_bulb_cel,dry_bulb_celFlag,wet_bulb_faren,wet_bulb_farenFlag,wet_bulb_cel,wet_bulb_celFlag,dew_point_faren,dew_point_farenFlag,dew_point_cel,dew_point_celFlag,relative_humidity,relative_humidityFlag,wind_speed,wind_speedFlag,wind_direction,wind_directionFlag,value_for_wind_character,value_for_wind_characterFlag,station_pressure,station_pressureFlag,pressure_tendency,pressure_tendencyFlag,presschange,presschangeFlag,sea_level_pressure,sea_level_pressureFlag,record_type,hourly_precip,hourly_precipFlag,altimeter,altimeterFlag,junk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_drop = ['sky_conditionFlag',\n",
    "                'visibilityFlag',\n",
    "                'wx_and_obst_to_vision',\n",
    "                'wx_and_obst_to_visionFlag',\n",
    "                'dry_bulb_farenFlag',\n",
    "                'dry_bulb_celFlag',\n",
    "                'wet_bulb_farenFlag',\n",
    "                'wet_bulb_celFlag',\n",
    "                'dew_point_farenFlag',\n",
    "                'dew_point_celFlag',\n",
    "                'relative_humidityFlag',\n",
    "                'wind_speedFlag',\n",
    "                'wind_directionFlag',\n",
    "                'value_for_wind_character',\n",
    "                'value_for_wind_characterFlag',\n",
    "                'station_pressureFlag',\n",
    "                'pressure_tendencyFlag',\n",
    "                'pressure_tendency',\n",
    "                'presschange',\n",
    "                'presschangeFlag',\n",
    "                'sea_level_pressureFlag',\n",
    "                'hourly_precip',\n",
    "                'hourly_precipFlag',\n",
    "                'altimeter',\n",
    "                'record_type',\n",
    "                'altimeterFlag',\n",
    "                'junk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the comma to create a list: column_labels_list\n",
    "column_labels_list = column_labels.split(',')\n",
    "column_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the new column labels to the DataFrame: df.columns\n",
    "df.columns = column_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the appropriate columns: df_dropped\n",
    "df_dropped = df.drop(list_to_drop, axis='columns')\n",
    "df_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and tidying datetime data\n",
    "\n",
    "In order to use the full power of pandas time series, you must construct a `DatetimeIndex`. To do so, it is necessary to clean and transform the date and time columns.\n",
    "\n",
    "The DataFrame `df_dropped` you created in the last exercise is provided for you and pandas has been imported as `pd`.\n",
    "\n",
    "Your job is to clean up the `date` and `Time` columns and combine them into a datetime collection to be used as the Index.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Convert the ***'date'*** column to a string with ***.astype(str)*** and assign to ***df_dropped['date']***.\n",
    "* Add leading zeros to the ***'Time'*** column. This has been done for you.\n",
    "* Concatenate the new ***'date'*** and ***'Time'*** columns together. Assign to ***date_string***.\n",
    "* Convert the ***date_string*** Series to datetime values with ***pd.to_datetime()***. Specify the ***format*** parameter.\n",
    "* Set the index of the ***df_dropped*** DataFrame to be ***date_times***. Assign the result to ***df_clean***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to string: df_dropped['date']\n",
    "df_dropped['date'] = df_dropped.date.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad leading zeros to the Time column: df_dropped['Time']\n",
    "df_dropped['Time'] = df_dropped['Time'].apply(lambda x:'{:0>4}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the new date and Time columns: date_string\n",
    "date_string = df_dropped['date'] + df_dropped['Time']\n",
    "date_string.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date_string Series to datetime: date_times\n",
    "date_times = pd.to_datetime(date_string, format='%Y%m%d%H%M')\n",
    "date_times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to be the new date_times container: df_clean\n",
    "df_clean = df_dropped.set_index(date_times)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the numeric columns\n",
    "\n",
    "The numeric columns contain missing values labeled as 'M'. In this exercise, your job is to transform these columns such that they contain only numeric values and interpret missing data as NaN.\n",
    "\n",
    "The pandas function pd.to_numeric() is ideal for this purpose: It converts a Series of values to floating-point values. Furthermore, by specifying the keyword argument errors='coerce', you can force strings like 'M' to be interpreted as NaN.\n",
    "\n",
    "A DataFrame df_clean is provided for you at the start of the exercise, and as usual, pandas has been imported as pd.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Print the ***'dry_bulb_faren'*** temperature between 8 AM and 9 AM on June 20, 2011.\n",
    "* Convert the ***'dry_bulb_faren'*** column to numeric values with ***pd.to_numeric()***. Specify ***errors='coerce'***.\n",
    "* Print the transformed ***dry_bulb_faren*** temperature between 8 AM and 9 AM on June 20, 2011.\n",
    "* Convert the ***'wind_speed***' and ***'dew_point_faren'*** columns to numeric values with ***pd.to_numeric()***. Again, specify ***errors='coerce'***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\n",
    "df_clean.loc['2011-6-20 08:00:00':'2011-6-20 09:00:00', 'dry_bulb_faren']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dry_bulb_faren column to numeric values: df_clean['dry_bulb_faren']\n",
    "df_clean['dry_bulb_faren'] = pd.to_numeric(df_clean['dry_bulb_faren'], errors='coerce')\n",
    "df_clean.dry_bulb_faren.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\n",
    "df_clean.loc['2011-6-20 08:00:00':'2011-6-20 09:00:00', 'dry_bulb_faren']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the wind_speed and dew_point_faren columns to numeric values\n",
    "df_clean['wind_speed'] = pd.to_numeric(df_clean['wind_speed'], errors='coerce')\n",
    "df_clean['dew_point_faren'] = pd.to_numeric(df_clean['dew_point_faren'], errors='coerce')\n",
    "\n",
    "df_clean[['wind_speed', 'dew_point_faren']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: time series\n",
    "\n",
    "* Index selection by date time\n",
    "* Partial datetime selection\n",
    "* Slicing ranges of datetimes\n",
    "\n",
    "```python\n",
    "climate2010['2010-05-31 22:00:00'] # datetime\n",
    "climate2010['2010-06-01'] # Entire day\n",
    "climate2010['2010-04'] # Entire month\n",
    "climate2010['2010-09':'2010-10'] # 2 months\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: statistics methods\n",
    "\n",
    "* Methods for computing statistics:\n",
    "    * describe(): summary\n",
    "    * mean(): average\n",
    "    * count(): counting entries\n",
    "    * median(): median\n",
    "    * std(): standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal min, max, median\n",
    "\n",
    "Now that you have the data read and cleaned, you can begin with statistical EDA. First, you will analyze the 2011 Austin weather data.\n",
    "\n",
    "Your job in this exercise is to analyze the 'dry_bulb_faren' column and print the median temperatures for specific time ranges. You can do this using partial datetime string selection.\n",
    "\n",
    "The cleaned dataframe is provided in the workspace as df_clean.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Select the ***'dry_bulb_faren'*** column and print the output of ***.median(numeric_only=True)***.\n",
    "* Use ***.loc[]*** to select the range ***'2011-Apr':'2011-Jun'*** from ***'dry_bulb_faren'*** and print the output of ***.median(numeric_only=True)***.\n",
    "* Use ***.loc[]*** to select the month ***'2011-Jan'*** from ***'dry_bulb_faren'*** and print the output of ***.median(numeric_only=True)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the median of the dry_bulb_faren column\n",
    "df_clean.dry_bulb_faren.median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the median of the dry_bulb_faren column for the time range '2011-Apr':'2011-Jun'\n",
    "df_clean.loc['2011-Apr':'2011-Jun', 'dry_bulb_faren'].median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the median of the dry_bulb_faren column for the month of January\n",
    "df_clean.loc['2011-Jan', 'dry_bulb_faren'].median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal variance\n",
    "\n",
    "You're now ready to compare the 2011 weather data with the 30-year normals reported in 2010. You can ask questions such as, on average, how much hotter was every day in 2011 than expected from the 30-year average?\n",
    "\n",
    "The DataFrames `df_clean` and `df_climate` from previous exercises are available in the workspace.\n",
    "\n",
    "Your job is to first resample `df_clean` and `df_climate` by day and aggregate the mean temperatures. You will then extract the temperature related columns from each - `'dry_bulb_faren'` in `df_clean`, and `'Temperature'` in `df_climate` - as NumPy arrays and compute the difference.\n",
    "\n",
    "Notice that the indexes of `df_clean` and `df_climate` are not aligned - `df_clean` has dates in 2011, while `df_climate` has dates in 2010. This is why you extract the temperature columns as NumPy arrays. An alternative approach is to use the pandas `.reset_index()` method to make sure the Series align properly. You will practice this approach as well.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Downsample ***df_clean*** with daily frequency and aggregate by the mean. Store the result as ***daily_mean_2011***.\n",
    "* Extract the ***'dry_bulb_faren'*** column from ***daily_mean_2011*** as a NumPy array using ***.values***. Store the result as ***daily_temp_2011***. Note: ***.values*** is an attribute, not a method, so you don't have to use ***()***.\n",
    "* Downsample ***df_climate*** with daily frequency and aggregate by the mean. Store the result as ***daily_climate***.\n",
    "* Extract the ***'Temperature'*** column from ***daily_climate*** using the ***.reset_index()*** method. To do this, first reset the index of ***daily_climate***, and then use bracket slicing to access ***'Temperature'***. Store the result as ***daily_temp_climate***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample df_clean by day and aggregate by mean: daily_mean_2011\n",
    "daily_mean_2011 = df_clean.resample('D').mean(numeric_only=True)\n",
    "daily_mean_2011.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dry_bulb_faren column from daily_mean_2011 using .values: daily_temp_2011\n",
    "daily_temp_2011 = daily_mean_2011.dry_bulb_faren.values\n",
    "daily_temp_2011[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample df_climate by day and aggregate by mean: daily_climate\n",
    "daily_climate = df_climate.resample('D').mean(numeric_only=True)\n",
    "daily_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate\n",
    "daily_temp_climate = daily_climate.reset_index()['Temperature']\n",
    "daily_temp_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference between the two arrays and print the mean difference\n",
    "difference = daily_temp_2011 - daily_temp_climate\n",
    "difference.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sunny or cloudy\n",
    "\n",
    "On average, how much hotter is it when the sun is shining? In this exercise, you will compare temperatures on sunny days against temperatures on overcast days.\n",
    "\n",
    "Your job is to use Boolean selection to filter out sunny and overcast days, and then compute the difference of the mean daily maximum temperatures between each type of day.\n",
    "\n",
    "The DataFrame `df_clean` from previous exercises has been provided for you. The column `'sky_condition'` provides information about whether the day was sunny (`'CLR'`) or overcast (`'OVC'`).\n",
    "\n",
    "***Instructions 1/3***\n",
    "\n",
    "* Get the cases in ***df_clean*** where the sky is clear. That is, when ***'sky_condition'*** equals ***'CLR'***, assigning to ***is_sky_clear***.\n",
    "* Use ***.loc[]*** to filter ***df_clean*** by ***is_sky_clear***, assigning to ***sunny***.\n",
    "* Resample ***sunny*** by day (***'D'***), and take the max to find the maximum daily temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df_clean, when is sky_condition 'CLR'?\n",
    "is_sky_clear = df_clean['sky_condition']=='CLR'\n",
    "is_sky_clear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_clean using is_sky_clear\n",
    "sunny = df_clean[is_sky_clear].copy()\n",
    "sunny = sunny.apply(lambda col: pd.to_numeric(col, errors='coerce')).dropna(how='all', axis=1)\n",
    "sunny.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample sunny by day then calculate the max\n",
    "# Additional cleaning was done in the previous cell, to remove all non-numeric values\n",
    "# Resample doesn't work if the column has strings\n",
    "sunny_daily_max = sunny.resample('1D').max(numeric_only=True)\n",
    "sunny_daily_max.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions 2/3***\n",
    "\n",
    "* Get the cases in ***df_clean*** where the sky is overcast. Using ***.str.contains()***, find when ***'sky_condition'*** contains ***'OVC'***, assigning to ***is_sky_overcast***.\n",
    "* Use ***.loc[]*** to filter ***df_clean*** by ***is_sky_overcast***, assigning to ***overcast***.\n",
    "* Resample ***overcast*** by day (***'D'***), and take the max to find the maximum daily temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df_clean, when does sky_condition contain 'OVC'?\n",
    "is_sky_overcast = df_clean['sky_condition'].str.contains('OVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_clean using is_sky_overcast\n",
    "overcast = df_clean[is_sky_overcast].copy()\n",
    "overcast = overcast.apply(lambda col: pd.to_numeric(col, errors='coerce')).dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample overcast by day then calculate the max\n",
    "overcast_daily_max = overcast.resample('D').max(numeric_only=True)\n",
    "overcast_daily_max.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions 3/3***\n",
    "\n",
    "* Calculate the mean of ***sunny_daily_max***, assigning to ***sunny_daily_max_mean***.\n",
    "* Calculate the mean of ***overcast_daily_max***, assigning to ***overcast_daily_max_mean***.\n",
    "* Print ***sunny_daily_max_mean*** minus ***overcast_daily_max_mean***. How much hotter are sunny days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of sunny_daily_max\n",
    "sunny_daily_max_mean = sunny_daily_max.mean(numeric_only=True)\n",
    "sunny_daily_max_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of overcast_daily_max\n",
    "overcast_daily_max_mean = overcast_daily_max.mean(numeric_only=True)\n",
    "overcast_daily_max_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the difference (sunny minus overcast)\n",
    "sunny_daily_max_mean - overcast_daily_max_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The average daily maximum dry bulb temperature was 6.5 degrees Fahrenheit higher on sunny days compared to overcast days.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line plots in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate.Temperature.loc['2010-07'].plot(title='Temperature (July 2010)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate.DewPoint.plot(kind='hist', bins=30,\n",
    "                         title='Dew Point Distribution (2010)',\n",
    "                         ec='black',\n",
    "                         color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box plots in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate.DewPoint.plot(kind='box',\n",
    "                         title='Dew Point Distribution (2010)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subplots in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate.plot(kind='hist',\n",
    "                density=True,\n",
    "                subplots=True,\n",
    "                ec='Black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekly average temperature and visibility\n",
    "\n",
    "Is there a correlation between temperature and visibility? Let's find out.\n",
    "\n",
    "In this exercise, your job is to plot the weekly average temperature and visibility as subplots. To do this, you need to first select the appropriate columns and then resample by week, aggregating the mean.\n",
    "\n",
    "In addition to creating the subplots, you will compute the Pearson correlation coefficient using `.corr()`. The Pearson correlation coefficient, known also as Pearson's r, ranges from -1 (indicating total negative linear correlation) to 1 (indicating total positive linear correlation). A value close to 1 here would indicate that there is a strong correlation between temperature and visibility.\n",
    "\n",
    "The DataFrame `df_clean` has been pre-loaded for you.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Import ***matplotlib.pyplot as plt***.\n",
    "* Select the ***'visibility'*** and ***'dry_bulb_faren'*** columns and resample them by week, aggregating the mean. Assign the result to ***weekly_mean***.\n",
    "* Print the output of ***weekly_mean.corr()***.\n",
    "* Plot the ***weekly_mean*** dataframe with ***.plot()***, specifying ***subplots=True***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.visibility = pd.to_numeric(df_clean.visibility, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the visibility and dry_bulb_faren columns and resample them: weekly_mean\n",
    "weekly_mean = df_clean[['visibility', 'dry_bulb_faren']].resample('W').mean(numeric_only=True)\n",
    "weekly_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output of weekly_mean.corr()\n",
    "weekly_mean.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weekly_mean with subplots=True\n",
    "weekly_mean.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily hours of clear sky\n",
    "\n",
    "In a previous exercise, you analyzed the `'sky_condition'` column to explore the difference in temperature on sunny days compared to overcast days. Recall that a `'sky_condition'` of `'CLR'` represents a sunny day. In this exercise, you will explore sunny days in greater detail. Specifically, you will use a box plot to visualize the fraction of days that are sunny.\n",
    "\n",
    "The `'sky_condition'` column is recorded hourly. Your job is to resample this column appropriately such that you can extract the number of sunny hours in a day and the number of total hours. Then, you can divide the number of sunny hours by the number of total hours, and generate a box plot of the resulting fraction.\n",
    "\n",
    "As before, `df_clean` is available for you in the workspace.\n",
    "\n",
    "***Instructions 1/3***\n",
    "\n",
    "* Get the cases in ***df_clean*** where the sky is clear. That is, when ***'sky_condition'*** equals ***'CLR'***, assigning to ***is_sky_clear***.\n",
    "* Resample ***is_sky_clear*** by day, assigning to ***resampled***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df_clean, when is sky_condition 'CLR'?\n",
    "is_sky_clear = df_clean.sky_condition == 'CLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample is_sky_clear by day\n",
    "resampled = is_sky_clear.resample('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions 2/3***\n",
    "\n",
    "* Calculate the number of measured sunny hours per day as the sum of ***resampled***, assigning to ***sunny_hours***.\n",
    "* Calculate the total number of measured hours per day as the count of ***resampled***, assigning to ***total_hours***.\n",
    "* Calculate the fraction of hours per day that were sunny as the ratio of sunny hours to total hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of sunny hours per day\n",
    "sunny_hours = resampled.sum()\n",
    "sunny_hours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of measured hours per day\n",
    "total_hours = resampled.count()\n",
    "total_hours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of hours per day that were sunny\n",
    "sunny_fraction = sunny_hours/total_hours\n",
    "sunny_fraction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions 3/3***\n",
    "\n",
    "* Draw a box plot of ***sunny_fraction*** using ***.plot()*** with kind set to ***'box'***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunny_fraction.plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The weather in the dataset is typically sunny less than 40% of the time.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heat or humidity\n",
    "Dew point is a measure of relative humidity based on pressure and temperature. A dew point above 65 is considered uncomfortable while a temperature above 90 is also considered uncomfortable.\n",
    "\n",
    "In this exercise, you will explore the maximum temperature and dew point of each month. The columns of interest are `'dew_point_faren'` and `'dry_bulb_faren'`. After resampling them appropriately to get the maximum temperature and dew point in each month, generate a histogram of these values as subplots. Uncomfortably, you will notice that the maximum dew point is above 65 every month!\n",
    "\n",
    "`df_clean` has been pre-loaded for you.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* Select the ***'dew_point_faren'*** and ***'dry_bulb_faren'*** columns (in that order). Resample by month and aggregate the maximum monthly temperatures. Assign the result to ***monthly_max***.\n",
    "* Plot a histogram of the resampled data with ***bins=8***, ***alpha=0.5***, and ***subplots=True***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample dew_point_faren and dry_bulb_faren by Month, aggregating the maximum values: monthly_max\n",
    "monthly_max = df_clean[['dew_point_faren', 'dry_bulb_faren']].resample('M').max(numeric_only=True)\n",
    "monthly_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram with bins=8, alpha=0.5, subplots=True\n",
    "monthly_max.plot(kind='hist',\n",
    "                 ec='black',\n",
    "                 bins=8,\n",
    "                 alpha=0.5,\n",
    "                 subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of high temperatures\n",
    "\n",
    "We already know that 2011 was hotter than the climate normals for the previous thirty years. In this final exercise, you will compare the maximum temperature in August 2011 against that of the August 2010 climate normals. More specifically, you will use a CDF plot to determine the probability of the 2011 daily maximum temperature in August being above the 2010 climate normal value. To do this, you will leverage the data manipulation, filtering, resampling, and visualization skills you have acquired throughout this course.\n",
    "\n",
    "The two DataFrames `df_clean` and `df_climate` are available in the workspace. Your job is to select the maximum temperature in August in `df_climate`, and then maximum daily temperatures in August 2011. You will then filter out the days in August 2011 that were above the August 2010 maximum, and use this to construct a CDF plot.\n",
    "\n",
    "Once you've generated the CDF, notice how it shows that there was a 50% probability of the 2011 daily maximum temperature in August being 5 degrees above the 2010 climate normal value!\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "* From ***df_climate***, extract the maximum temperature observed in August 2010. The relevant column here is ***'Temperature'***. You can select the rows corresponding to August 2010 in multiple ways. For example, ***df_climate.loc['2011-Feb']*** selects all rows corresponding to February 2011, while ***df_climate.loc['2009-09', 'Pressure']*** selects the rows corresponding to September 2009 from the ***'Pressure'*** column.\n",
    "* From ***df_clean***, select the August 2011 temperature data from the ***'dry_bulb_faren'***. Resample this data by day and aggregate the maximum value. Store the result in ***august_2011***.\n",
    "* Filter rows of ***august_2011*** to keep days where the value exceeded ***august_max***. Store the result in ***august_2011_high***.\n",
    "* Construct a CDF of ***august_2011_high*** using 25 bins. Remember to specify the ***kind***, ***density***, and ***cumulative*** parameters in addition to ***bins***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the maximum temperature in August 2010 from df_climate: august_max\n",
    "august_max = df_climate.Temperature.loc['2010-08'].max(numeric_only=True)\n",
    "august_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample August 2011 temps in df_clean by day & aggregate the max value: august_2011\n",
    "august_2011 = df_clean.dry_bulb_faren.loc['2011-08'].resample('D').max(numeric_only=True)\n",
    "august_2011.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for days in august_2011 where the value exceeds august_max: august_2011_high\n",
    "august_2011_high = august_2011.loc[august_2011 > august_max]\n",
    "august_2011_high.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a CDF of august_2011_high\n",
    "august_2011_high.plot(kind='hist', bins=25, density=True, cumulative=True, ec='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "***You can now...***\n",
    "* Import many types of datasets and deal with import issues\n",
    "* Export data to facilitate collaborative data science\n",
    "* Perform statistical and visual EDA natively in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certificate\n",
    "\n",
    "![](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/2019-02-04_pandas_foundations/2019-02-04_pandas_foundations_certificate.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
